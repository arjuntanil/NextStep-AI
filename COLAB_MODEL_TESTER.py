# ============================================================================
# ğŸ¯ COLAB MODEL TESTER - Check Your Trained Career Advisor LLM
# ============================================================================
# PASTE THIS IN A NEW CELL IN COLAB AFTER TRAINING COMPLETES
# ============================================================================

import torch

print("\n" + "="*70)
print("ğŸ¯ CAREER ADVISOR MODEL - QUALITY CHECKER")
print("="*70)

# Verify model is loaded
try:
    device
    model
    tokenizer
    print("âœ… Model is ready to test!")
    print(f"âœ… Device: {device}")
    print(f"âœ… Model: GPT2-Medium (fine-tuned)")
except:
    print("âŒ ERROR: Model not found!")
    print("   Please run the training code first in a previous cell.")
    raise Exception("Model not loaded. Run training first!")

print("\n" + "="*70)
print("ğŸ§ª STARTING QUALITY TESTS")
print("="*70)

# ============================================================================
# TEST FUNCTION
# ============================================================================

def test_model(question, show_details=False):
    """
    Test the model with a question and return quality metrics
    """
    # Generate response
    input_text = f"<|startoftext|>### Question: {question}\n\n### Answer:"
    inputs = tokenizer(input_text, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=350,
            temperature=0.7,
            top_p=0.92,
            top_k=50,
            do_sample=True,
            repetition_penalty=1.3,
            no_repeat_ngram_size=3,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    advice = response.split("### Answer:")[1].strip() if "### Answer:" in response else response
    
    # Quality checks
    advice_lower = advice.lower()
    word_count = len(advice.split())
    words = advice_lower.split()
    unique_ratio = len(set(words)) / len(words) if words else 0
    
    # Check 1: Skills/Technologies
    has_skills = any(w in advice_lower for w in ['skill', 'skills', 'learn', 'technology', 'technologies', 'knowledge'])
    
    # Check 2: Interview Preparation
    has_interview = any(w in advice_lower for w in ['question', 'interview', 'ask', 'prepare'])
    
    # Check 3: Well-structured
    has_structure = any(marker in advice for marker in ['###', '*', 'â€¢', '1.', '2.', '\n-'])
    
    # Check 4: No technical errors
    error_patterns = [
        ('spring boot', 'django'),
        ('â‚¹3â€“5 lpa', '$0.40'),
        ('kite database', ''),
        ('oracle 9-core', ''),
        ('postman for backend', '')
    ]
    
    has_errors = False
    detected_errors = []
    for pattern1, pattern2 in error_patterns:
        if pattern1 in advice_lower:
            if pattern2 and pattern2 in advice_lower:
                has_errors = True
                detected_errors.append(f"{pattern1}+{pattern2}")
            elif not pattern2:
                has_errors = True
                detected_errors.append(pattern1)
    
    # Check 5: Good length
    is_good_length = 50 < word_count < 400
    
    # Check 6: Coherent
    is_coherent = unique_ratio > 0.4
    
    # Calculate score
    checks = [has_skills, has_interview, has_structure, not has_errors, is_good_length, is_coherent]
    score = sum(checks)
    
    # Grade
    if score >= 5:
        grade = "EXCELLENT âœ…"
    elif score >= 4:
        grade = "GOOD âœ…"
    elif score >= 3:
        grade = "FAIR âš ï¸"
    else:
        grade = "POOR âŒ"
    
    # Display results
    print(f"\n{'='*70}")
    print(f"â“ Question: {question}")
    print(f"{'â”€'*70}")
    print(f"\nğŸ’¡ Generated Advice:\n")
    print(advice[:300] + "..." if len(advice) > 300 else advice)
    
    if show_details:
        print(f"\n{'â”€'*70}")
        print("ğŸ“Š QUALITY ANALYSIS:")
        print(f"{'â”€'*70}")
        print(f"   {'âœ…' if has_skills else 'âŒ'} Contains Skills/Technologies")
        print(f"   {'âœ…' if has_interview else 'âŒ'} Contains Interview Prep")
        print(f"   {'âœ…' if has_structure else 'âŒ'} Well-Structured Format")
        print(f"   {'âœ…' if not has_errors else 'âŒ'} No Technical Errors" + (f" (Found: {', '.join(detected_errors)})" if has_errors else ""))
        print(f"   {'âœ…' if is_good_length else 'âŒ'} Good Length ({word_count} words)")
        print(f"   {'âœ…' if is_coherent else 'âŒ'} Coherent ({unique_ratio:.1%} unique)")
    
    print(f"\n{'â”€'*70}")
    print(f"ğŸ¯ SCORE: {score}/6 - {grade}")
    print(f"{'='*70}")
    
    return {
        'advice': advice,
        'score': score,
        'grade': grade,
        'word_count': word_count,
        'has_errors': has_errors,
        'errors': detected_errors
    }

# ============================================================================
# AUTOMATIC TESTS - 5 Key Questions
# ============================================================================

print("\n\n" + "="*70)
print("ğŸ§ª AUTOMATIC QUALITY TESTS (5 Questions)")
print("="*70)

test_questions = [
    "I love to become a DevOps engineer",
    "I love to become a Data Scientist",
    "What is software development?",
    "Tell me about cloud engineering",
    "I want to learn networking"
]

results = []
for i, question in enumerate(test_questions, 1):
    print(f"\n{'#'*70}")
    print(f"TEST {i}/5")
    print(f"{'#'*70}")
    result = test_model(question, show_details=True)
    results.append(result)

# ============================================================================
# SUMMARY REPORT
# ============================================================================

print("\n\n" + "="*70)
print("ğŸ“Š OVERALL TEST SUMMARY")
print("="*70)

total_score = sum(r['score'] for r in results)
max_score = len(results) * 6
percentage = (total_score / max_score) * 100

excellent_count = sum(1 for r in results if r['score'] >= 5)
good_count = sum(1 for r in results if r['score'] == 4)
fair_count = sum(1 for r in results if r['score'] == 3)
poor_count = sum(1 for r in results if r['score'] < 3)

errors_found = sum(1 for r in results if r['has_errors'])

print(f"\nğŸ“ˆ Overall Score: {total_score}/{max_score} ({percentage:.1f}%)")
print(f"\nğŸ“Š Test Results:")
print(f"   âœ… EXCELLENT (5-6/6): {excellent_count} tests")
print(f"   âœ… GOOD (4/6): {good_count} tests")
print(f"   âš ï¸  FAIR (3/6): {fair_count} tests")
print(f"   âŒ POOR (<3/6): {poor_count} tests")
print(f"\nâš ï¸  Technical Errors Found: {errors_found} tests")

if errors_found > 0:
    print("\n   Errors detected in:")
    for i, r in enumerate(results, 1):
        if r['has_errors']:
            print(f"   - Test {i}: {', '.join(r['errors'])}")

# ============================================================================
# FINAL VERDICT
# ============================================================================

print("\n" + "="*70)
print("ğŸ¯ FINAL VERDICT")
print("="*70)

if percentage >= 85 and errors_found == 0:
    print("\nğŸŒŸ EXCELLENT - Model is PRODUCTION-READY!")
    print("   âœ… High accuracy across all tests")
    print("   âœ… No technical errors detected")
    print("   âœ… Safe to deploy immediately")
    print("\nğŸ“‹ Next Steps:")
    print("   1. âœ… Save to Google Drive (Option B)")
    print("   2. âœ… Or download to local PC (Option C)")
    print("   3. âœ… Deploy to production")
    
elif percentage >= 70 and errors_found <= 1:
    print("\nâœ… GOOD - Model is usable with minor monitoring")
    print("   âœ… Good accuracy on most tests")
    print("   âš ï¸  Minor issues detected (acceptable)")
    print("   âœ… Can deploy but monitor responses")
    print("\nğŸ“‹ Next Steps:")
    print("   1. âœ… Save to Google Drive or download")
    print("   2. âš ï¸  Deploy with monitoring")
    print("   3. ğŸ’¡ Consider retraining with 18 epochs if issues persist")
    
elif percentage >= 50:
    print("\nâš ï¸  FAIR - Model needs improvement")
    print("   âš ï¸  Moderate accuracy issues")
    print("   âš ï¸  Some technical errors detected")
    print("   âŒ NOT recommended for production")
    print("\nğŸ“‹ Recommended Actions:")
    print("   1. âŒ DO NOT save this model")
    print("   2. ğŸ”„ RETRAIN with these settings:")
    print("      â€¢ epochs = 20")
    print("      â€¢ learning_rate = 1e-5")
    print("      â€¢ batch_size = 2")
    print("   3. ğŸ§ª Run this test again after retraining")
    
else:
    print("\nâŒ POOR - Model needs significant retraining")
    print("   âŒ Low accuracy across tests")
    print("   âŒ Multiple technical errors")
    print("   âŒ DO NOT deploy to production")
    print("\nğŸ“‹ Critical Actions Required:")
    print("   1. âŒ DO NOT save/download this model")
    print("   2. ğŸ”„ RETRAIN with improved settings:")
    print("      â€¢ epochs = 20-25")
    print("      â€¢ learning_rate = 5e-6 (even lower)")
    print("      â€¢ Check training data quality")
    print("   3. ğŸ§ª Run comprehensive validation")

print("\n" + "="*70)

# ============================================================================
# INTERACTIVE MODE - Ask Your Own Questions!
# ============================================================================

print("\n\n" + "="*70)
print("ğŸ’¬ INTERACTIVE MODE - Test Your Own Questions!")
print("="*70)
print("\nğŸ“ Instructions:")
print("   â€¢ Type any career question to test")
print("   â€¢ Type 'exit' or 'quit' to stop")
print("   â€¢ Type 'examples' for sample questions")
print("="*70)

examples = [
    "What skills do I need for Full Stack Development?",
    "How do I prepare for cybersecurity interviews?",
    "Tell me about AI and Machine Learning careers",
    "What is blockchain development?",
    "I'm interested in mobile app development"
]

while True:
    print("\n" + "â”€"*70)
    user_input = input("\nâ“ Your Question (or 'exit' to stop): ").strip()
    
    if user_input.lower() in ['exit', 'quit', 'stop']:
        print("\nğŸ‘‹ Testing complete! Good luck with your Career Advisor!")
        break
    
    if user_input.lower() == 'examples':
        print("\nğŸ“š Example Questions:")
        for i, ex in enumerate(examples, 1):
            print(f"   {i}. {ex}")
        continue
    
    if not user_input:
        print("âš ï¸  Please enter a question!")
        continue
    
    print("\nâ³ Generating response...")
    result = test_model(user_input, show_details=True)
    
    feedback = input("\nğŸ’­ Was this response good? (y/n): ").strip().lower()
    if feedback == 'n':
        print("ğŸ’¡ Tip: Try rephrasing or being more specific!")

print("\n" + "="*70)
print("ğŸ‰ MODEL TESTING COMPLETE!")
print("="*70)
print("\nâœ… Your Career Advisor LLM has been thoroughly tested!")
print("âœ… Review the summary above to decide next steps")
print("âœ… Save the model if quality is good (â‰¥70% score)")
print("\nğŸ“š For detailed deployment guide, see HOW_TO_TEST_MODEL.md")
