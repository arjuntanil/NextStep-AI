# NextStepAI: AI-Powered Career Navigator ğŸš€

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2+-61dafb.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Why NextStepAI?](#why-nextstepai)
- [Core Features](#core-features)
- [Technology Stack](#technology-stack)
- [AI Models & Parameters](#ai-models--parameters)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
- [API Reference](#api-reference)
- [Project Structure](#project-structure)
- [Deployment](#deployment)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## Overview

**NextStepAI** is a production-ready AI career coaching platform that combines **Machine Learning**, **Fine-tuned LLMs**, and **Retrieval-Augmented Generation (RAG)** to deliver personalized career guidance. The platform features a modern **React frontend with Aurora WebGL animation** and a high-performance **FastAPI backend**.

### What Problem Does This Solve?

**The Challenge:**
- 90%+ of resumes are filtered by ATS before reaching recruiters
- Job seekers struggle to identify skill gaps for career advancement
- Generic career advice lacks personalization and data-driven insights
- Information overload without intelligent matching

**The Solution:**
- ğŸ” **AI Resume Analysis** - Extract skills, predict jobs, identify gaps
- ğŸ¤– **Fine-Tuned Career Advisor** - GPT-2 trained on 749 career examples
- ğŸ“š **RAG Coach** - Upload PDFs for personalized Q&A
- ğŸ¯ **Live Job Matching** - Real-time LinkedIn job scraping
- ğŸ¨ **Modern UI** - React with Aurora WebGL effects

---

## Why NextStepAI?

### Key Innovations

| Feature | Technology | Impact |
|---------|-----------|--------|
| **CV Analysis** | Multinomial Naive Bayes + TF-IDF | 85% job prediction accuracy |
| **Resume-JD Matcher** | Cosine similarity + Gemini API | Precise skill gap identification |
| **Career Advisor** | Fine-tuned GPT-2 (355M params) | Context-aware career guidance |
| **RAG Coach** | TinyLLama 1.1B + FAISS | Privacy-first document Q&A |
| **Admin Dashboard** | Recharts + Material-UI | Comprehensive analytics |
| **Job Scraping** | BeautifulSoup + LinkedIn | Real-time opportunities |
| **Wozber UI** | React + OGL (WebGL) | Modern minimalist design |

### Why This Matters

âœ… **Personalized** - AI analyzes YOUR resume against YOUR target jobs  
âœ… **Data-Driven** - 8000+ job mappings, 749 career examples, live job data  
âœ… **Privacy-First** - TinyLLama runs locally via Ollama (no external API calls for RAG)  
âœ… **Production-Ready** - JWT auth, SQLite database, role-based access control, admin dashboard  
âœ… **Modern UI** - Wozber-style light theme, premium formatting, Aurora WebGL background  
âœ… **Comprehensive Analytics** - User tracking, retention metrics, engagement monitoring  

---

## Core Features

### 1. ğŸ“„ CV Analyzer

**Upload your resume and receive comprehensive AI-powered analysis**

**Workflow:**
```
PDF/DOCX Upload â†’ Skill Extraction â†’ ML Job Prediction â†’ 
Gap Analysis â†’ ATS Feedback â†’ LinkedIn Job Scraping â†’ YouTube Tutorials
```

**Output:**
- âœ… Recommended job title with 85% accuracy
- âœ… Match percentage (your skills vs. required)
- âœ… Skills to learn with YouTube tutorials
- âœ… Live LinkedIn job postings
- âœ… ATS optimization feedback
- âœ… History tracking (logged-in users)

**Technologies:**
- **Skill Extraction:** Google Gemini Pro (with 100+ RegEx fallback patterns)
- **Job Classification:** TF-IDF + Multinomial Naive Bayes (8000+ training examples)
- **Job Scraping:** BeautifulSoup4 (LinkedIn India)
- **File Parsing:** pdfplumber, python-docx
- **Validation:** skills_db.json (10,000+ valid skills)

---

### 2. ğŸ¤– AI Career Advisor

**Chat with a fine-tuned GPT-2 model trained on career counseling**

**How It Works:**
1. **Primary:** Fine-tuned GPT-2-Medium generates personalized advice
2. **Fallback:** RAG system over curated career guides
3. **Enhancement:** Live job postings for recommended roles

**Model Training Details:**
```
Base Model: GPT-2-Medium (355M parameters)
Training Data: 749 career examples (train/val: 80/20)
Epochs: 15
Learning Rate: 1e-5
Batch Size: 2 (gradient accumulation: 8)
Max Length: 512 tokens
Training Time: 15-20 min (GPU) / 6+ hours (CPU)
Final Loss: 0.87
```

**Example Output:**
```
Question: "Tell me about DevOps careers"

Answer: 
### Key Skills:
â€¢ Docker, Kubernetes, CI/CD
â€¢ AWS/Azure/GCP cloud platforms
â€¢ Linux, Bash scripting

### Top Certifications:
â€¢ AWS Certified DevOps Engineer
â€¢ Certified Kubernetes Administrator (CKA)

### Salary Range: â‚¹8-20 LPA (India)
[+ Live job postings]
```

---

### 3. ğŸ¯ Resume Analyzer with Job Description

**Upload both resume and job description for precise matching**

**Workflow:**
```
Resume + JD Upload â†’ Dual Skill Extraction â†’ Semantic Matching â†’ 
ATS Score Calculation â†’ Gap Analysis â†’ YouTube Learning Paths
```

**Output:**
- âœ… ATS compatibility score (0-100%)
- âœ… Matching skills (green indicators)
- âœ… Missing critical skills (red indicators with YouTube links)
- âœ… ATS optimization tips (blue card)
- âœ… All detected skills (purple card)

**Technologies:**
- **Skill Extraction:** Dual Gemini API calls (resume + JD)
- **Matching:** Cosine similarity + set operations
- **Learning Paths:** YouTube API integration
- **File Support:** PDF, DOCX for both documents

---

### 4. ğŸ“š RAG Coach

**Upload resume + job description PDFs for personalized guidance**

**Workflow:**
```
Upload PDFs â†’ Auto Document Detection â†’ FAISS Indexing â†’ 
Auto Analysis â†’ Interactive Q&A
```

**RAG System Parameters:**
```python
LLM: TinyLLama 1.1B (Q4 quantized)
  - Memory: ~4GB RAM
  - Inference: ~50 tokens/sec (CPU)
  - Privacy: 100% local execution

Embeddings: all-MiniLM-L6-v2
  - Dimensions: 384
  - Model Size: 90MB

Vector Store: FAISS
  - Index Type: IndexFlatL2
  - Distance Metric: L2 Euclidean
  - Top-K Retrieval: 4 chunks

Chunking Strategy:
  - Chunk Size: 500 characters
  - Overlap: 50 characters
  - Splitter: RecursiveCharacterTextSplitter
```

**Auto-Generated Analysis:**
- âœ… Skills to add (with synonym normalization)
- âœ… Resume enhancement bullet points
- âœ… ATS-friendly keywords
- âœ… Interactive Q&A with source attribution

**Key Innovation:**
- **Skill Normalization:** 50+ synonym mappings (React.js â†’ react, PostgreSQL â†’ postgres)
- **Document Detection:** Content-based classification (resume vs JD)
- **Query Intent:** Filters context based on question type

---

### 5. ğŸ“Š Admin Dashboard (New!)

**Comprehensive analytics and user management for administrators**

**Features:**
- **KPI Cards:** Total users, active users (7d/30d), analyses, average match score
- **Retention Metrics:** Overall, 7-day, and 30-day retention rates
- **User Growth Chart:** 30-day time-series visualization
- **Match Score Distribution:** Pie chart categorizing ATS scores
- **Top Jobs & Skills:** Bar charts showing most recommended jobs and missing skills
- **Recent Activity Feed:** Real-time user action tracking
- **User Management:** Suspend, activate, delete users

**Technologies:**
- **Charts:** Recharts (Area, Bar, Pie charts)
- **Data Visualization:** Material-UI Grid with responsive layout
- **Backend:** Comprehensive `/admin/stats` endpoint with 15+ metrics
- **Design:** Wozber-style minimalist cards with gradient accents

**Access:**
- Admin-only route: `/admin`
- Role-based authentication with JWT
- Create admin user: `python create_admin.py`

---

## Technology Stack

### Frontend

**React 18 + Material-UI v5 + Aurora WebGL**

```javascript
Core Libraries:
  - React 18.2.0 (hooks, context API)
  - Material-UI v5.14.19 (components, theming)
  - React Router v6.20.1 (client-side routing)
  - Axios 1.6.2 (HTTP client with JWT)
  - Recharts 2.10.3 (data visualization)
  - OGL 1.0.11 (WebGL library for Aurora)

Aurora Effect Implementation:
  - Technology: WebGL shaders (vertex + fragment)
  - Library: OGL (7KB, GPU-accelerated)
  - Color Scheme: Purple (#8b5cf6) â†’ Blue (#3b82f6) â†’ Green (#10b981)
  - Performance: 60 FPS at 1080p
  - Configuration:
    * Amplitude: 1.5
    * Blend: 0.8
    * Speed: 0.4
    * Noise: Simplex noise algorithm

UI Features:
  - Wozber-Style Design: Clean white cards with subtle shadows
  - Light Theme: Light gray (#F9FAFB) background with indigo/cyan accents
  - Protected Routes: JWT-based authentication guards
  - Responsive Design: Mobile-first with MUI breakpoints
  - Custom Fonts: Poppins (primary), Inter, Space Grotesk (headings)
  - Premium Formatting: Markdown-style AI responses with typography hierarchy
```

**Aurora Technical Details:**
```javascript
// Vertex Shader (GLSL 3.0)
#version 300 es
in vec2 position;
void main() {
  gl_Position = vec4(position, 0.0, 1.0);
}

// Fragment Shader (Simplex Noise)
- Generates animated gradient using GPU
- Color interpolation between 3 stops
- Time-based animation (uTime uniform)
- Resolution-aware (uResolution uniform)
```

---

### Backend

**FastAPI + Python 3.10 + Async Architecture**

```python
Core Framework:
  - FastAPI 0.116.1 (async ASGI)
  - Uvicorn 0.35.0 (ASGI server)
  - Pydantic 2.11.7 (data validation)

Authentication:
  - python-jose 3.5.0 (JWT tokens)
  - passlib 1.7.4 (bcrypt hashing)
  - Google OAuth 2.0 (SSO)

Database:
  - SQLAlchemy 2.0.43 (ORM)
  - SQLite 3 (file-based, zero configuration)

API Features:
  - RESTful endpoints (/analyze_resume/, /rag-coach/*, /admin/*)
  - Admin dashboard with comprehensive analytics
  - Role-based access control (user/admin)
  - CORS enabled (localhost:3000, localhost:8501)
  - Lazy model loading (background threads)
  - Comprehensive logging

Additional Features:
  - Admin Dashboard: User analytics, retention metrics, engagement tracking
  - History Tracking: Resume analyses, career queries, RAG interactions
  - User Management: Suspend, activate, delete users (admin only)
```

---

### AI & Machine Learning

**LLMs:**
```
1. Google Gemini Pro (API)
   - Purpose: Skill extraction, ATS feedback, RAG fallback
   - Model: gemini-1.5-pro
   - Context Window: 32k tokens
   - Temperature: 0.3 (consistent outputs)
   - Fallback: Regex-based extraction (100+ patterns)

2. GPT-2 Medium (Fine-tuned)
   - Purpose: Career advice generation
   - Parameters: 355 million
   - Size: 1.5GB
   - Training: 749 examples, 15 epochs
   - Format: PyTorch (HuggingFace)
   - Fallback: RAG over career guides

3. TinyLLama 1.1B (RAG)
   - Purpose: Document Q&A
   - Parameters: 1.1 billion
   - Quantization: Q4_K_M (4-bit)
   - Memory: ~4GB RAM
   - Inference: CPU-friendly (~50 tokens/sec)
   - Privacy: 100% local execution via Ollama
```

**ML Models:**
```python
Job Classification:
  - Algorithm: Multinomial Naive Bayes
  - Feature Engineering: TF-IDF Vectorization
  - Training Data: 8000+ job-skill mappings (jobs_cleaned.csv)
  - Accuracy: ~85% on test set
  - Inference Time: <10ms
  - Model Size: 450KB

  Categories (12 groups):
    â€¢ Data Professional
    â€¢ Software Developer
    â€¢ IT Operations & Infrastructure
    â€¢ Project / Product Manager
    â€¢ QA / Test Engineer
    â€¢ Human Resources
    â€¢ Sales & Business Development
    â€¢ Marketing
    â€¢ UI/UX & Design
    â€¢ Finance & Accounting
    â€¢ Customer Support
    â€¢ Other

  Hyperparameters (GridSearchCV):
    â€¢ Vectorizer: TfidfVectorizer(ngram_range=(1,2))
    â€¢ Classifier: MultinomialNB(alpha=0.1-1.0) OR LogisticRegression
    â€¢ Train/Test Split: 80/20
    â€¢ CV Folds: 5
    â€¢ Scoring: F1 Weighted
```

**RAG System:**
```python
Vector Database: FAISS
  - Index Type: IndexFlatL2 (exact search)
  - Distance Metric: L2 Euclidean
  - Persistent Storage: ./rag_data/faiss_index

Embeddings: HuggingFace Sentence Transformers
  - Model: all-MiniLM-L6-v2
  - Dimensions: 384
  - Normalization: L2 norm
  - Batch Size: 32

Document Processing:
  - Loader: PyPDFLoader (LangChain)
  - Chunking: RecursiveCharacterTextSplitter
    * chunk_size: 500
    * chunk_overlap: 50
    * separators: ["\n\n", "\n", ". ", " "]
  - Metadata: source, doc_type, page, doc_index

Retrieval:
  - Strategy: Similarity search
  - Top-K: 4 chunks per query
  - Filtering: Document type (resume vs JD)
  - Context Window: 2048 tokens (TinyLLama)
```

**Document Parsing:**
```
PDF: pdfplumber 0.10.3 (complex layouts)
DOCX: python-docx 1.1.0 (paragraph extraction)
Text Splitting: langchain 0.0.335
```

---

## AI Models & Parameters

### Fine-Tuned LLM (Career Advisor)

**Training Configuration:**
```python
# Base Model
model_name = "gpt2-medium"
num_parameters = 355_000_000
architecture = "Transformer decoder (24 layers, 16 attention heads)"

# Training Data
train_dataset = "career_advice_dataset.jsonl + career_advice_ultra_clear_dataset.jsonl"
total_examples = 749
train_val_split = "80/20 (599 train, 150 val)"

# Training Hyperparameters
training_args = {
    "num_train_epochs": 15,
    "learning_rate": 1e-5,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,  # Effective batch = 16
    "max_seq_length": 512,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "fp16": True,  # Mixed precision (GPU only)
    "logging_steps": 50,
    "save_strategy": "epoch",
    "evaluation_strategy": "epoch"
}

# Training Results
total_steps = 1500  # ~100 steps/epoch
training_time_gpu = "15-20 minutes (RTX 2050)"
training_time_cpu = "6+ hours (not recommended)"
final_loss = 0.87
validation_perplexity = 2.39

# Generation Config
generation_config = {
    "max_length": 200,
    "temperature": 0.7,  # 0.5-0.9 for coherence
    "top_p": 0.9,        # Nucleus sampling
    "top_k": 50,         # Top-k sampling
    "repetition_penalty": 1.2,
    "do_sample": True
}
```

**Dataset Structure:**
```json
{
  "prompt": "What skills are required for a Data Scientist in India?",
  "completion": "### Key Skills:\n* Python, R, SQL\n* ML: Regression, Trees, Deep Learning\n\n### Certifications:\n* Google Cloud Professional Data Engineer\n\n### Salary: â‚¹8-25 LPA"
}
```

---

### RAG System (Document Q&A)

**TinyLLama Configuration:**
```python
# Model Setup (Ollama)
model_name = "tinyllama"
full_name = "TinyLlama-1.1B-Chat-v1.0"
parameters = 1_100_000_000
quantization = "Q4_K_M (4-bit)"
model_size = "637MB download"
memory_usage = "~4GB RAM"

# Inference Performance
tokens_per_second_cpu = 50
tokens_per_second_gpu = 150
context_window = 2048
temperature = 0.7
max_tokens = 512
top_p = 0.9
repeat_penalty = 1.1

# Privacy
execution = "100% local (no API calls)"
data_retention = "Zero (PDFs can be deleted post-indexing)"
```

**FAISS Vector Store:**
```python
# Index Configuration
index_type = "IndexFlatL2"  # Exact search, no compression
distance_metric = "L2 Euclidean distance"
dimension = 384  # all-MiniLM-L6-v2 embedding size

# Storage
index_path = "./rag_data/faiss_index"
persistence = "Disk (loaded on startup)"
index_size = "~50MB for 1000 chunks"

# Retrieval Parameters
search_type = "similarity"
k = 4  # Top-4 chunks
score_threshold = 0.7  # Minimum similarity
return_metadata = True  # source, doc_type, page
```

**Embedding Model:**
```python
# HuggingFace Sentence Transformers
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_size = "90MB"
embedding_dimension = 384
max_sequence_length = 256
normalization = "L2 norm"
device = "cpu"  # Lightweight, no GPU needed

# Performance
encoding_speed = "~1000 sentences/sec (CPU)"
batch_size = 32
```

---

### ML Model Training (Job Classification)

**Model Training Script:** `model_training.py`

```python
# Data Loading
dataset = pd.read_csv("jobs_cleaned.csv")
total_records = 8000+
columns = ["Job Title", "Skills", "Grouped_Title"]

# Preprocessing
skill_validation = "skills_db.json (10,000+ valid skills)"
job_consolidation = "54 unique titles â†’ 10 groups"
normalization = "lowercase, strip whitespace"
train_test_split = "80/20"

# Feature Engineering
vectorizer = TfidfVectorizer(
    max_features=500,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)

# Model Selection (GridSearchCV)
algorithms_tested = [
    "MultinomialNB",
    "LogisticRegression"
]
cv_folds = 5
best_algorithm = "MultinomialNB(alpha=1.0)"

# Training
fit_time = "~2 minutes"
accuracy = 0.85
precision = 0.83
recall = 0.82
f1_score = 0.82

# Output Artifacts
saved_models = [
    "job_recommender_pipeline.joblib",     # TF-IDF + NB
    "job_title_encoder.joblib",            # LabelEncoder
    "prioritized_skills.joblib",           # Job â†’ skills mapping
    "master_skill_vocab.joblib"            # Complete vocabulary
]
```

---

## How It Works

### 1. CV Analyzer - Complete Pipeline

**Stage-by-Stage Breakdown:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: File Upload & Text Extraction             â”‚
â”‚ â”œâ”€ PDF: pdfplumber (handles complex layouts)       â”‚
â”‚ â””â”€ DOCX: python-docx (paragraph extraction)        â”‚
â”‚ Output: Plain text (500-5000 chars)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: AI Skill Extraction                       â”‚
â”‚ â”œâ”€ Primary: Gemini LLM (contextual NER)            â”‚
â”‚ â”‚   - Prompt: Extract technical skills, tools      â”‚
â”‚ â”‚   - Temperature: 0.3 (consistency)               â”‚
â”‚ â”‚   - Time: 2-4 seconds                            â”‚
â”‚ â””â”€ Fallback: 9 RegEx patterns (100+ technologies)  â”‚
â”‚ Output: ["python", "django", "mysql", "git"]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: ML Job Prediction                         â”‚
â”‚ â”œâ”€ TF-IDF Vectorization (skills â†’ numeric vector)  â”‚
â”‚ â”œâ”€ Naive Bayes Classification (P(Job|Skills))      â”‚
â”‚ â””â”€ LabelEncoder (decode to job title)              â”‚
â”‚ Output: "Software Developer" (85% accuracy)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: Skill Gap Analysis                        â”‚
â”‚ â”œâ”€ Retrieve required skills from database          â”‚
â”‚ â”œâ”€ Set operations (required - user_skills)         â”‚
â”‚ â””â”€ Calculate match percentage                      â”‚
â”‚ Output: Match 42%, Missing ["docker", "k8s"]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 5: AI Layout Feedback                        â”‚
â”‚ â”œâ”€ Primary: Gemini LLM (ATS optimization)          â”‚
â”‚ â””â”€ Fallback: 7 rule-based checks                   â”‚
â”‚ Output: "âœ… Add professional summary"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 6: Live Job Scraping                         â”‚
â”‚ â”œâ”€ LinkedIn job search (India)                     â”‚
â”‚ â”œâ”€ BeautifulSoup parsing (3 fallback selectors)    â”‚
â”‚ â””â”€ Browser emulation headers                       â”‚
â”‚ Output: Top 5 jobs with links                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 7: YouTube Tutorial Mapping                  â”‚
â”‚ â”œâ”€ JSON lookup (youtube_links.json)                â”‚
â”‚ â””â”€ Map each missing skill to tutorial              â”‚
â”‚ Output: Skill â†’ YouTube link                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 8: Database Storage (if logged in)           â”‚
â”‚ â”œâ”€ SQLAlchemy ORM                                  â”‚
â”‚ â””â”€ Save to ResumeAnalysis table                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 9: Return JSON Response                      â”‚
â”‚ Total Time: 8-12 seconds end-to-end                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Metrics:**

| Stage | Time | Notes |
|-------|------|-------|
| Text Extraction | <1s | Typical 2-page resume |
| Skill Extraction | 2-4s | Gemini API latency |
| Job Prediction | <0.1s | Pre-trained ML model |
| Gap Analysis | <0.01s | Python set operations |
| Layout Feedback | 2-3s | Or instant (fallback) |
| Job Scraping | 2-4s | Network-dependent |
| YouTube Mapping | <0.01s | In-memory lookup |
| **TOTAL** | **8-12s** | **Full pipeline** |

---

### 2. AI Career Advisor - Dual System

**Workflow:**

```
User Query: "Tell me about DevOps"
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Check Model  â”‚
    â”‚   Status     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Model Loaded?   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       /           \
    YES             NO
     â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Fine-tunedâ”‚   â”‚   RAG    â”‚
â”‚  GPT-2   â”‚   â”‚  System  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“               â†“
   Generate      Retrieve
   Response      Context
     â†“               â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Enhance with    â”‚
    â”‚ Live Jobs       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Return JSON     â”‚
    â”‚ Response        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fine-Tuned Model Path:**
```python
# Generation Process
input_text = "### Question: Tell me about DevOps\n\n### Answer:"
tokenized = tokenizer(input_text, return_tensors="pt")
output = model.generate(
    tokenized.input_ids,
    max_length=200,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.2
)
response = tokenizer.decode(output[0], skip_special_tokens=True)
# Returns: Structured advice with skills, certs, salary
```

**RAG Fallback Path:**
```python
# Retrieval Process
query_embedding = embeddings.embed_query(user_question)
relevant_docs = faiss_index.similarity_search(query_embedding, k=4)
context = "\n\n".join([doc.page_content for doc in relevant_docs])

# Generation
prompt = f"Context: {context}\n\nQuestion: {user_question}\n\nAnswer:"
response = gemini_llm.invoke(prompt)
# Returns: Context-aware answer from career guides
```

---

### 3. RAG Coach - Document Intelligence

**Complete Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Upload & Detection            â”‚
â”‚ â”œâ”€ User uploads resume.pdf + jd.pdf    â”‚
â”‚ â”œâ”€ Content analysis (keywords)          â”‚
â”‚ â”‚   Resume: "experience", "education"   â”‚
â”‚ â”‚   JD: "requirements", "qualifications"â”‚
â”‚ â””â”€ Tag metadata: doc_type, source      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Chunking & Embedding          â”‚
â”‚ â”œâ”€ RecursiveCharacterTextSplitter      â”‚
â”‚ â”‚   - chunk_size: 500                  â”‚
â”‚ â”‚   - overlap: 50                      â”‚
â”‚ â”œâ”€ all-MiniLM-L6-v2 embeddings         â”‚
â”‚ â””â”€ 384-dim vectors                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: FAISS Indexing                â”‚
â”‚ â”œâ”€ IndexFlatL2 (exact search)          â”‚
â”‚ â”œâ”€ Store metadata with each chunk      â”‚
â”‚ â””â”€ Persistent storage (disk)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: Auto Skill Analysis           â”‚
â”‚ â”œâ”€ Extract skills from both docs       â”‚
â”‚ â”œâ”€ Normalize (React.js â†’ react)        â”‚
â”‚ â”œâ”€ Set operations (JD - Resume)        â”‚
â”‚ â””â”€ Generate bullet points              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: Interactive Q&A               â”‚
â”‚ â”œâ”€ User query embedding                 â”‚
â”‚ â”œâ”€ FAISS similarity search (top-4)     â”‚
â”‚ â”œâ”€ Filter by doc_type (if needed)      â”‚
â”‚ â”œâ”€ TinyLLama generation                â”‚
â”‚ â””â”€ Return answer + sources             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Skill Normalization (Key Innovation):**
```python
# 50+ synonym mappings
synonym_map = {
    'react.js': 'react',
    'reactjs': 'react',
    'node.js': 'nodejs',
    'postgresql': 'postgres',
    'sqlite3': 'sqlite',
    'restful api': 'rest api',
    'ci/cd': 'cicd',
    'oop': 'object-oriented programming',
    # ... 40+ more
}

# Before: 24 "missing" skills (false positives)
# After: 4 actual missing skills
# Accuracy improvement: 83% reduction in false positives
```

**Query Intent Detection:**
```python
# Filter context by question type
if "job description" in query.lower():
    filter_docs(doc_type="JOB_DESCRIPTION")
elif "my resume" in query.lower():
    filter_docs(doc_type="RESUME")
else:
    use_all_docs()
```

---

## Installation

### Quick Start (5 Minutes)

```bash
# 1. Clone repository
git clone https://github.com/arjuntanil/NextStep-AI.git
cd NextStep-AI

# 2. Create virtual environment
python -m venv career_coach
career_coach\Scripts\activate  # Windows
# source career_coach/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env: Add GOOGLE_API_KEY, JWT_SECRET_KEY

# 5. Train models
python model_training.py

# 6. Build RAG indexes
python ingest_guides.py

# 7. Install Ollama (for RAG Coach)
# Download from https://ollama.ai
ollama pull tinyllama

# 8. Create admin user (optional)
python create_admin.py
# Default credentials: admin@gmail.com / admin

# 9. Start backend
python -m uvicorn backend_api:app --reload

# 10. Start frontend (new terminal)
# Option A: React (modern UI with Aurora)
cd frontend && npm install && npm start

# Option B: Streamlit (simple UI - legacy)
streamlit run app.py
```

### React Frontend Setup

```bash
cd frontend

# Install dependencies (~1-2 minutes, 1406 packages)
npm install

# Start development server
npm run dev

# Access at: http://localhost:3000
```

**React Frontend Features:**
- â˜€ï¸ Wozber-style light theme with clean white cards
- âš¡ Aurora WebGL background (GPU-accelerated, purple/blue/green)
- ğŸ” JWT authentication with protected routes
- ğŸ“± Mobile-responsive Material-UI components
- ğŸ¨ Indigo/Cyan color scheme with Poppins font
- ğŸ“Š Admin dashboard with comprehensive visualizations
- âœ¨ Premium markdown-style AI response formatting

---

### Environment Variables

**.env Configuration:**
```env
# Required
GOOGLE_API_KEY=AIzaSy...  # Get from https://makersuite.google.com/app/apikey
JWT_SECRET_KEY=<64-char-hex>  # python -c "import secrets; print(secrets.token_hex(32))"

# Optional (for Google OAuth login)
GOOGLE_CLIENT_ID=your_google_oauth_client_id
GOOGLE_CLIENT_SECRET=your_google_oauth_client_secret

# Frontend URL
STREAMLIT_FRONTEND_URL=http://localhost:8501
```

---

### Fine-Tune Career Advisor (Optional)

**Option 1: Local Training (GPU Recommended)**
```bash
python production_finetuning_optimized.py
# Time: 15-20 min (GPU) / 6+ hours (CPU)
# Output: career-advisor-final/ directory (1.5GB)
```

**Option 2: Skip Training (Use RAG Only)**
The system will automatically fall back to the RAG system if the fine-tuned model is not found. This is perfectly functional for most use cases.

---

## Usage

### Access URLs

| Service | URL | Description |
|---------|-----|-------------|
| **React Frontend** | http://localhost:3000 | Modern UI with Aurora |
| **Streamlit Frontend** | http://localhost:8501 | Data-centric UI |
| **Backend API** | http://localhost:8000 | FastAPI server |
| **API Docs** | http://localhost:8000/docs | Swagger UI |

### Using CV Analyzer

1. Navigate to "CV Analyzer" page
2. Upload resume (PDF/DOCX)
3. Wait 8-12 seconds for analysis
4. Review:
   - Recommended job title
   - Match percentage
   - Skills to learn (with YouTube links)
   - Live LinkedIn jobs
   - ATS feedback

### Using Resume Analyzer with JD

1. Navigate to "Resume Analyzer (with Job Description)" page
2. Upload both resume and job description (PDF/DOCX)
3. Wait 10-15 seconds for analysis
4. Review:
   - ATS compatibility score with gradient bar
   - Matching skills (green cards)
   - Missing skills with YouTube links (red cards)
   - ATS optimization tips (blue card)
   - All detected skills (purple card)

### Using AI Career Advisor

1. Navigate to "AI Career Advisor" page
2. Ask question (e.g., "Tell me about Data Science")
3. Adjust temperature (0.1-1.0) and length (50-120)
4. Click "Get AI Advice"
5. Review premium-formatted response with:
   - Headers (###, ####)
   - Bullet points with custom markers
   - Bold/italic text
   - Purple accent colors

### Using RAG Coach

1. Navigate to "RAG Coach" page
2. Upload resume.pdf + job_description.pdf
3. Wait for auto-analysis (~5-10 seconds)
4. Review:
   - Skills to add (normalized)
   - Resume bullet points
   - ATS keywords
5. Ask follow-up questions with context-aware answers

### Using Admin Dashboard (Admin Only)

1. Login with admin credentials (admin@gmail.com / admin)
2. Click "Features" â†’ "Admin Panel"
3. View comprehensive analytics:
   - KPI cards (users, analyses, retention)
   - User growth chart (30 days)
   - Top jobs and missing skills bar charts
   - Match score distribution pie chart
   - Recent activity feed
   - Retention metrics display
4. Manage users (view, suspend, activate, delete)

---

## API Reference

### Authentication

```bash
# Initiate Google OAuth
GET /auth/login

# OAuth callback
GET /auth/callback

# Get current user
GET /users/me
Authorization: Bearer <JWT_TOKEN>
```

### Resume Analysis

```bash
POST /analyze_resume/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume/ \
  -F "file=@resume.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "recommended_job_title": "Full Stack Developer",
  "required_skills": ["python", "django", "react", "docker"],
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "match_percentage": 75.0,
  "live_jobs": [{"title": "...", "company": "...", "link": "..."}],
  "layout_feedback": "âœ… Add professional summary..."
}
```

### AI Career Advisor

```bash
POST /query-career-path/
Content-Type: application/json

{
  "text": "Tell me about DevOps",
  "max_length": 200,
  "temperature": 0.7
}
```

### RAG Coach

```bash
# Upload PDFs
POST /rag-coach/upload
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/rag-coach/upload \
  -F "files=@resume.pdf" \
  -F "files=@job_description.pdf" \
  -F "process_resume_job=true"

# Query
POST /rag-coach/query
Content-Type: application/json

{
  "question": "What skills should I add?",
  "show_context": true
}
```

**Response:**
```json
{
  "answer": "Based on the job description, you should focus on...",
  "context_chunks": [
    {"content": "...", "source": "jd.pdf", "doc_type": "JOB_DESCRIPTION"}
  ],
  "sources": ["resume.pdf", "job_description.pdf"]
}
```

### Resume + JD Analysis

```bash
POST /analyze_resume_with_jd/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume_with_jd/ \
  -F "resume=@resume.pdf" \
  -F "job_description=@jd.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "jd_skills": ["python", "django", "react", "docker", "kubernetes"],
  "matching_skills": ["python", "django", "react"],
  "missing_skills": ["docker", "kubernetes"],
  "ats_score": 60.0,
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "layout_feedback": "âœ… Skills section is ATS-friendly..."
}
```

### History

```bash
# Resume analyses
GET /history/analyses
Authorization: Bearer TOKEN

# Career queries
GET /history/queries
Authorization: Bearer TOKEN

# RAG interactions
GET /history/rag-queries
Authorization: Bearer TOKEN
```

### Admin Dashboard

```bash
# Get comprehensive statistics (admin only)
GET /admin/stats
Authorization: Bearer ADMIN_TOKEN

# Get all users with pagination
GET /admin/users?skip=0&limit=50
Authorization: Bearer ADMIN_TOKEN

# Suspend user
PUT /admin/user/{user_id}/suspend
Authorization: Bearer ADMIN_TOKEN

# Activate user
PUT /admin/user/{user_id}/activate
Authorization: Bearer ADMIN_TOKEN

# Delete user
DELETE /admin/user/{user_id}
Authorization: Bearer ADMIN_TOKEN
```

**Admin Stats Response:**
```json
{
  "total_users": 150,
  "active_users_30days": 85,
  "active_users_7days": 42,
  "new_users_7days": 12,
  "total_analyses": 450,
  "analyses_7days": 67,
  "avg_match_percentage": 72.3,
  "retention_rate": 0.567,
  "retention_7days": 0.714,
  "retention_30days": 0.589,
  "user_growth": [{"date": "2024-11-01", "count": 138}, ...],
  "top_jobs": [{"job": "Software Developer", "count": 89}, ...],
  "top_missing_skills": [{"skill": "docker", "count": 45}, ...],
  "match_distribution": [65.2, 78.5, 82.1, ...],
  "recent_activity": [{"type": "analysis", "user": "user@email.com", ...}],
  "activity_heatmap": [{"day": "Monday", "hour": 14, "count": 23}, ...]
}
```

---

## Deployment

### Local Production Deployment

**Running on Windows Server/VPS:**
```bash
# 1. Install Python 3.10+ and Git
# 2. Clone and setup (same as installation steps)
# 3. Create Windows Service or use Task Scheduler for auto-start

# Run backend as background process
start /B python -m uvicorn backend_api:app --host 0.0.0.0 --port 8000

# Or use a process manager like PM2
npm install -g pm2
pm2 start "uvicorn backend_api:app --host 0.0.0.0 --port 8000" --name nextstepai-backend
```

**Linux/Mac Deployment with systemd:**
```bash
# Create service file: /etc/systemd/system/nextstepai.service
[Unit]
Description=NextStepAI Backend
After=network.target

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/NextStepAI
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/uvicorn backend_api:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target

# Enable and start
sudo systemctl enable nextstepai
sudo systemctl start nextstepai
```

### Database Management

**SQLite Database Location:**
```
nextstepai.db (created automatically in project root)
```

**Backup SQLite Database:**
```bash
# Create backup
copy nextstepai.db nextstepai_backup_2024-10-29.db

# Or use SQLite command
sqlite3 nextstepai.db ".backup nextstepai_backup.db"
```

**Database Schema:**
```python
# Tables created by SQLAlchemy (models.py):
- users
  * id, email, full_name, password_hash
  * role (user/admin), is_active, created_at, last_active
  
- resume_analyses
  * id, owner_id, recommended_job_title, match_percentage
  * skills_to_add, resume_filename, total_skills_count, created_at
  
- career_queries
  * id, owner_id, user_query_text, matched_job_group
  * model_used (finetuned/rag), response_time_seconds, created_at
  
- rag_coach_queries
  * id, owner_id, question, answer, sources
  * query_length, answer_length, created_at
```

### Production Checklist

- âœ… Set strong JWT_SECRET_KEY (64 chars)
- âœ… Backup SQLite database regularly
- âœ… Enable HTTPS with reverse proxy (nginx/Apache)
- âœ… Configure CORS for production domain
- âœ… Set up rate limiting (10 req/min)
- âœ… Enable structured logging
- âœ… Use environment variables (no hardcoded secrets)
- âœ… Monitor disk space (SQLite database growth)
- âœ… Set up automatic backups (daily/weekly)
- âœ… Use process manager (PM2/systemd) for auto-restart

---

## Troubleshooting

### Backend Issues

**Port 8000 in use:**
```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8000 | xargs kill -9
```

**Model loading hangs:**
```bash
# Wait 2-5 minutes or disable fine-tuned model
DISABLE_FINETUNED_MODEL_LOAD=1
```

### RAG Coach Issues

**Ollama model not found:**
```bash
ollama pull tinyllama
ollama list  # Verify
```

**No documents in vector store:**
```bash
curl -X POST http://localhost:8000/rag-coach/build-index
```

### Authentication Issues

**OAuth redirect mismatch:**
- Update Google Console redirect URI
- Must match: `http://localhost:8000/auth/callback`

**JWT token expired:**
- Logout and login again

### Performance Issues

**Slow responses (20+ seconds):**
- Reduce temperature (0.7 â†’ 0.3)
- Reduce max_length (200 â†’ 80)
- Use GPU instead of CPU

**Out of memory:**
- Close other applications
- Reduce chunk size in RAG
- Use quantized models

---

## Project Structure

```
NextStepAI/
â”œâ”€â”€ backend_api.py              # Main FastAPI application (2897 lines)
â”œâ”€â”€ models.py                   # SQLAlchemy database models
â”œâ”€â”€ model_training.py           # ML model training script (Naive Bayes)
â”œâ”€â”€ production_finetuning_optimized.py  # GPT-2 fine-tuning script
â”œâ”€â”€ create_admin.py             # Admin user creation utility
â”œâ”€â”€ requirements.txt            # Python dependencies (191 packages)
â”œâ”€â”€ nextstepai.db              # SQLite database (auto-created)
â”‚
â”œâ”€â”€ Data Files/
â”‚   â”œâ”€â”€ jobs_cleaned.csv           # 8000+ job-skill mappings
â”‚   â”œâ”€â”€ skills_db.json             # 10,000+ validated skills
â”‚   â”œâ”€â”€ career_advice_dataset.jsonl     # 749 career Q&A pairs
â”‚   â”œâ”€â”€ youtube_links.json         # Skill â†’ YouTube tutorial mapping
â”‚   â”œâ”€â”€ job_postings_new.json      # Scraped job postings
â”‚   â””â”€â”€ career_guides.json         # Career advice guides for RAG
â”‚
â”œâ”€â”€ Model Artifacts/
â”‚   â”œâ”€â”€ job_recommender_pipeline.joblib   # TF-IDF + Naive Bayes
â”‚   â”œâ”€â”€ job_title_encoder.joblib          # Label encoder
â”‚   â”œâ”€â”€ prioritized_skills.joblib         # Job â†’ top skills mapping
â”‚   â”œâ”€â”€ master_skill_vocab.joblib         # Complete skill vocabulary
â”‚   â””â”€â”€ career-advisor-final/             # Fine-tuned GPT-2 (1.5GB)
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ tokenizer files
â”‚
â”œâ”€â”€ Vector Stores/
â”‚   â”œâ”€â”€ rag_coach_index/           # FAISS index for uploaded docs
â”‚   â”œâ”€â”€ jobs_index/                # Job postings embeddings
â”‚   â””â”€â”€ guides_index/              # Career guides embeddings
â”‚
â”œâ”€â”€ frontend/                      # React 18 application
â”‚   â”œâ”€â”€ package.json               # 13 dependencies
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                 # Router configuration
â”‚   â”‚   â”œâ”€â”€ index.css              # Global styles (Wozber theme)
â”‚   â”‚   â”œâ”€â”€ theme.js               # Material-UI theme
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Layout.js          # Header + navigation
â”‚   â”‚   â”‚   â”œâ”€â”€ Aurora.js          # WebGL background
â”‚   â”‚   â”‚   â””â”€â”€ ProtectedRoute.js  # Auth guard
â”‚   â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â”‚   â””â”€â”€ AuthContext.js     # JWT state management
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.js       # Home page
â”‚   â”‚   â”‚   â”œâ”€â”€ CVAnalyzer.js      # Resume analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ ResumeAnalyzer.js  # Resume + JD matching
â”‚   â”‚   â”‚   â”œâ”€â”€ CareerAdvisor.js   # AI chat
â”‚   â”‚   â”‚   â”œâ”€â”€ RAGCoach.js        # Document Q&A
â”‚   â”‚   â”‚   â”œâ”€â”€ AdminDashboard.js  # Analytics (600+ lines)
â”‚   â”‚   â”‚   â”œâ”€â”€ History.js         # User history
â”‚   â”‚   â”‚   â”œâ”€â”€ Login.js
â”‚   â”‚   â”‚   â””â”€â”€ Register.js
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ api.js             # Axios API client
â”‚   â””â”€â”€ build/                     # Production build
â”‚
â””â”€â”€ uploads/
    â””â”€â”€ processed/                 # Temporary uploaded files

Key Files Overview:
- backend_api.py: 22 endpoints (auth, CV analysis, RAG, admin)
- models.py: 4 database tables with relationships
- model_training.py: GridSearchCV, confusion matrix, artifact generation
- AdminDashboard.js: 8+ visualizations with Recharts
```

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

**Code Style:**
- Follow PEP 8 for Python
- Use type hints
- Add docstrings
- Keep lines <100 characters

---

## License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file.

---

## Acknowledgments

**Technologies:**
- [HuggingFace Transformers](https://huggingface.co/) - LLM infrastructure
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [React](https://reactjs.org/) - Frontend library
- [Material-UI](https://mui.com/) - UI components
- [OGL](https://github.com/oframe/ogl) - WebGL library (Aurora)
- [LangChain](https://python.langchain.com/) - RAG orchestration
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [Ollama](https://ollama.ai/) - Local LLM inference

**Data:**
- Google Gemini - Skill extraction
- all-MiniLM-L6-v2 - Sentence embeddings
- GPT-2-Medium - Base model
- Scikit-learn - ML utilities

---

## Contact

**Author:** Arjun T Anil  
**GitHub:** [@arjuntanil](https://github.com/arjuntanil)  
**Repository:** [NextStep-AI](https://github.com/arjuntanil/NextStep-AI)  

**Support:**
- ğŸ› [Report Bugs](https://github.com/arjuntanil/NextStep-AI/issues)
- ğŸ’¡ [Request Features](https://github.com/arjuntanil/NextStep-AI/discussions)

---

<div align="center">

**â­ Star this project if you find it helpful! â­**

Made with â¤ï¸ by [Arjun T Anil](https://github.com/arjuntanil)

</div>
