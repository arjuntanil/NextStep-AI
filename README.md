# NextStepAI: AI-Powered Career Navigator üöÄ

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2+-61dafb.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üìã Table of Contents
- [Overview](#overview)
- [Why NextStepAI?](#why-nextstepai)
- [Core Features](#core-features)
- [Technology Stack](#technology-stack)
- [AI Models & Parameters](#ai-models--parameters)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
- [API Reference](#api-reference)
- [Project Structure](#project-structure)
- [Deployment](#deployment)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## Overview

### What is NextStepAI?

**NextStepAI** is an intelligent, end-to-end career guidance platform that revolutionizes how job seekers navigate their career journey. Built with cutting-edge AI technologies, it acts as your personal career coach, resume optimizer, and job search assistant‚Äîall in one unified platform.

Think of NextStepAI as having a career counselor, resume expert, and job recruiter working together 24/7 to help you land your dream job. Unlike generic career advice websites or simple resume parsers, NextStepAI uses **advanced machine learning models trained on real-world data** to provide hyper-personalized recommendations tailored specifically to your skills, experience, and career goals.

### The Problem We Solve

In today's competitive job market, job seekers face three critical challenges:

**1. The ATS Black Hole (90% Rejection Rate)**
- Modern companies use Applicant Tracking Systems (ATS) that automatically filter resumes
- 90%+ of resumes never reach human recruiters due to poor ATS compatibility
- Job seekers don't know why their applications are rejected
- Manual resume optimization is time-consuming and often ineffective

**2. The Skill Gap Crisis**
- Technology evolves rapidly; yesterday's skills become obsolete
- Job seekers struggle to identify which skills employers actually need
- Generic online courses don't address individual skill gaps
- No personalized learning path based on career goals

**3. Information Overload & Generic Advice**
- Career advice blogs offer one-size-fits-all solutions
- Job postings use vague descriptions; hard to match skills accurately
- No intelligent system to analyze YOUR specific situation
- Lack of actionable, data-driven career guidance

### How NextStepAI Helps You

**For Job Seekers:**
- ‚úÖ **Get Hired Faster:** Optimize your resume for ATS systems with AI-powered feedback
- ‚úÖ **Close Skill Gaps:** Discover exactly what skills you need with personalized learning paths
- ‚úÖ **Make Informed Decisions:** Career advice based on 749 real career examples, not generic articles
- ‚úÖ **Save Time:** Automated job matching with live LinkedIn scraping (no manual searching)
- ‚úÖ **Stay Competitive:** Track industry trends, top missing skills, and high-demand jobs

**For Career Changers:**
- ‚úÖ **Transition Smoothly:** AI predicts suitable job roles based on your transferable skills
- ‚úÖ **Identify Opportunities:** Discover career paths you never considered
- ‚úÖ **Skill Mapping:** Understand the exact gap between your current skills and target role

**For Students & Fresh Graduates:**
- ‚úÖ **Career Guidance:** Chat with AI career advisor about different career paths
- ‚úÖ **Resume Building:** Get professional feedback on your first resume
- ‚úÖ **Certification Roadmap:** Learn which certifications employers value most

**For Professionals:**
- ‚úÖ **Career Growth:** Identify skills needed for promotion or salary increase
- ‚úÖ **Job Market Insights:** Real-time data on in-demand jobs and skills
- ‚úÖ **Strategic Planning:** Make data-driven career decisions, not guesses

### Why NextStepAI Stands Out

Unlike traditional career platforms, NextStepAI offers:

**üéØ Personalization at Scale**
- Every recommendation is tailored to YOUR resume, YOUR skills, YOUR goals
- No generic advice‚Äîall insights are generated by analyzing your specific situation

**ü§ñ AI-Powered Intelligence**
- Fine-tuned GPT-2 model trained on 749 career counseling examples
- Multinomial Naive Bayes classifier trained on 8000+ real job-skill mappings
- FAISS vector search for instant, context-aware answers from your documents

**üìä Data-Driven Insights**
- 85% accuracy in job prediction (validated on test data)
- Real-time LinkedIn job scraping (not outdated job boards)
- 10,000+ validated skills in our database (no fake or irrelevant skills)

**üîí Privacy-First Design**
- RAG system runs 100% locally using Ollama (your documents never leave your computer)
- No selling of your data to recruiters or third parties
- SQLite database‚Äîfull control over your career data

**ÔøΩ Professional User Experience**
- Modern, clean Wozber-style interface (not cluttered legacy UI)
- Instant results (8-12 seconds for full resume analysis)
- Mobile-responsive design‚Äîuse on any device

**üí∞ Cost-Effective**
- Open-source and self-hostable (no monthly subscriptions)
- Free fine-tuned models included (no expensive API credits)
- Optional Gemini API fallback (only for skill extraction)

### Real-World Impact

**Before NextStepAI:**
- ‚ùå Upload resume to 50+ job sites manually
- ‚ùå Guess which skills employers want
- ‚ùå Pay $100+ for professional resume review
- ‚ùå Spend hours researching career paths
- ‚ùå Get generic "learn Python" advice

**After NextStepAI:**
- ‚úÖ AI analyzes your resume in 10 seconds
- ‚úÖ Exact list of missing skills with YouTube tutorials
- ‚úÖ ATS-optimized resume feedback for free
- ‚úÖ Personalized career advice in 30 seconds
- ‚úÖ "Learn Docker for DevOps" with direct learning link

**Measurable Benefits:**
- **85% job prediction accuracy** ‚Üí Apply to right roles, increase interview chances
- **8-12 second analysis time** ‚Üí Save hours of manual research
- **10,000+ skill validation** ‚Üí No false recommendations
- **Live job scraping** ‚Üí Always see current, active job postings
- **Premium formatting** ‚Üí Professional-looking career advice responses

---

## Why NextStepAI?

### Key Innovations

| Feature | Technology | Impact |
|---------|-----------|--------|
| **CV Analysis** | Multinomial Naive Bayes + TF-IDF | 85% job prediction accuracy |
| **Resume-JD Matcher** | Cosine similarity + Gemini API | Precise skill gap identification |
| **Career Advisor** | Fine-tuned GPT-2 (355M params) | Context-aware career guidance |
| **RAG Coach** | TinyLLama 1.1B + FAISS | Privacy-first document Q&A |
| **Admin Dashboard** | Recharts + Material-UI | Comprehensive analytics |
| **Job Scraping** | BeautifulSoup + LinkedIn | Real-time opportunities |
| **Wozber UI** | React + OGL (WebGL) | Modern minimalist design |

### Why This Matters

‚úÖ **Personalized** - AI analyzes YOUR resume against YOUR target jobs  
‚úÖ **Data-Driven** - 8000+ job mappings, 749 career examples, live job data  
‚úÖ **Privacy-First** - TinyLLama runs locally via Ollama (no external API calls for RAG)  
‚úÖ **Production-Ready** - JWT auth, SQLite database, role-based access control, admin dashboard  
‚úÖ **Modern UI** - Wozber-style light theme, premium formatting, Aurora WebGL background  
‚úÖ **Comprehensive Analytics** - User tracking, retention metrics, engagement monitoring  

---

## Core Features

### 1. üìÑ CV Analyzer - Smart Resume Intelligence

**What It Does:**
Upload your resume and get instant, AI-powered analysis that tells you exactly which jobs you're qualified for and what skills you need to learn next. Think of it as having a career counselor analyze your resume in 10 seconds instead of 10 days.

**How It Helps Users:**
- **Job Seekers:** Know which jobs to apply for (avoid wasting time on wrong roles)
- **Career Changers:** Discover new career paths based on transferable skills
- **Students:** Understand job market expectations before graduating
- **Professionals:** Identify skills needed for next promotion

**Complete Workflow:**
```
Step 1: Upload Resume (PDF/DOCX)
   ‚Üì
Step 2: Text Extraction
   ‚Ä¢ PDF: pdfplumber library (handles complex layouts, tables, columns)
   ‚Ä¢ DOCX: python-docx library (paragraph-by-paragraph extraction)
   ‚Ä¢ Result: Plain text (500-5000 characters typical)
   ‚Üì
Step 3: AI Skill Extraction (Primary Method)
   ‚Ä¢ Model: Google Gemini 1.5 Pro (32k context window)
   ‚Ä¢ Prompt: "Extract technical skills, tools, frameworks, certifications"
   ‚Ä¢ Temperature: 0.3 (consistent, factual outputs)
   ‚Ä¢ Validation: Cross-check against skills_db.json (10,000+ skills)
   ‚Ä¢ Time: 2-4 seconds
   ‚Üì
Step 4: Skill Extraction Fallback (If API Quota Exceeded)
   ‚Ä¢ 100+ Regex patterns covering:
     - Programming languages (Python, Java, C++, etc.)
     - Frameworks (React, Django, Spring Boot, etc.)
     - Databases (MySQL, MongoDB, PostgreSQL, etc.)
     - Cloud platforms (AWS, Azure, GCP)
     - Tools (Docker, Git, Jenkins, Kubernetes, etc.)
   ‚Ä¢ Extracted: 35+ skills on average
   ‚Ä¢ Time: <1 second
   ‚Üì
Step 5: Machine Learning Job Prediction
   ‚Ä¢ Algorithm: Multinomial Naive Bayes
   ‚Ä¢ Training: 8000+ job records from jobs_cleaned.csv
   ‚Ä¢ Feature Engineering: TF-IDF Vectorization (skills ‚Üí numeric vectors)
   ‚Ä¢ Process:
     1. Convert extracted skills to space-separated string
     2. TF-IDF transforms to 500-dimension vector
     3. Naive Bayes calculates P(Job|Skills) for 12 categories
     4. LabelEncoder decodes to job title
   ‚Ä¢ Accuracy: 85% on test set (20% holdout)
   ‚Ä¢ Output: "Software Developer" or "Data Professional"
   ‚Ä¢ Time: <100ms
   ‚Üì
Step 6: Skill Gap Analysis
   ‚Ä¢ Load required skills from prioritized_skills.joblib
   ‚Ä¢ Set operations: required_skills - user_skills = missing_skills
   ‚Ä¢ Calculate match: (user_skills ‚à© required_skills) / required_skills * 100
   ‚Ä¢ Example: User has 8/15 required skills = 53% match
   ‚Ä¢ Time: <10ms
   ‚Üì
Step 7: ATS Layout Feedback (Primary)
   ‚Ä¢ Model: Google Gemini 1.5 Pro
   ‚Ä¢ Prompt: "Analyze resume layout for ATS compatibility"
   ‚Ä¢ Checks: Section headings, formatting, keyword density
   ‚Ä¢ Output: 3-5 actionable tips ("Add skills section", "Remove tables")
   ‚Ä¢ Time: 2-3 seconds
   ‚Üì
Step 8: ATS Feedback Fallback (Rule-Based)
   ‚Ä¢ 7 heuristic checks:
     - Contact info present
     - Education section exists
     - Work experience section exists
     - Skills section exists
     - Avoid images/graphics
     - Use standard headings
     - Chronological format
   ‚Ä¢ Time: Instant (<1ms)
   ‚Üì
Step 9: Live LinkedIn Job Scraping
   ‚Ä¢ Target: LinkedIn Jobs India
   ‚Ä¢ Search query: "{job_title} India"
   ‚Ä¢ Library: BeautifulSoup4
   ‚Ä¢ Headers: Browser emulation (avoid blocking)
   ‚Ä¢ Extraction: Title, company, location, link
   ‚Ä¢ Fallback selectors: 3 CSS patterns (handles UI changes)
   ‚Ä¢ Results: Top 5 relevant jobs
   ‚Ä¢ Time: 2-4 seconds
   ‚Üì
Step 10: YouTube Tutorial Mapping
   ‚Ä¢ Source: youtube_links.json (pre-curated)
   ‚Ä¢ Mapping: skill_name ‚Üí tutorial_url
   ‚Ä¢ Coverage: 100+ in-demand skills
   ‚Ä¢ Quality: Verified, high-rating tutorials
   ‚Ä¢ Time: <1ms (in-memory lookup)
   ‚Üì
Step 11: Database Storage (If User Logged In)
   ‚Ä¢ Table: resume_analyses
   ‚Ä¢ Fields: job_title, match_percentage, skills_to_add, created_at
   ‚Ä¢ Relationship: Linked to user_id (for history tracking)
   ‚Üì
Step 12: Return JSON Response
   ‚Ä¢ Total Time: 8-12 seconds end-to-end
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Backend** | FastAPI 0.116.1 | REST API server | Async endpoints, auto-validation |
| **LLM** | Google Gemini Pro | Skill extraction, ATS feedback | 32k context, 0.3 temperature |
| **ML Model** | Multinomial Naive Bayes | Job classification | Trained on 8000+ examples |
| **Vectorization** | TF-IDF (Scikit-learn) | Skill ‚Üí numeric features | 500 max features, bigrams |
| **PDF Parser** | pdfplumber 0.11.7 | Extract text from PDFs | Handles tables, multi-column |
| **DOCX Parser** | python-docx 1.2.0 | Extract text from Word | Paragraph extraction |
| **Web Scraping** | BeautifulSoup4 4.13.5 | LinkedIn job scraping | HTML parsing, CSS selectors |
| **Validation** | skills_db.json | Skill verification | 10,000+ validated skills |
| **Database** | SQLite + SQLAlchemy | History storage | Relational, file-based |

**Datasets & Models:**

1. **jobs_cleaned.csv** (8000+ records)
   - Columns: Job Title, Key Skills, Grouped_Title
   - Source: Real job postings (cleaned and normalized)
   - Usage: Training Naive Bayes classifier
   - Quality: Deduplicated, validated skills only

2. **skills_db.json** (10,000+ skills)
   - Format: JSON array of skill names
   - Coverage: Programming, frameworks, tools, platforms
   - Purpose: Filter out non-technical terms (e.g., "teamwork", "leadership")
   - Maintenance: Regularly updated with emerging skills

3. **job_recommender_pipeline.joblib** (450KB)
   - Content: TF-IDF Vectorizer + Naive Bayes Classifier
   - Training: GridSearchCV (5-fold CV)
   - Hyperparameters: alpha=0.1-1.0, ngram_range=(1,2)
   - Performance: 85% accuracy, 0.82 F1 score

4. **prioritized_skills.joblib**
   - Structure: {job_title: [skill1, skill2, ...]}
   - Top 30 skills per job category
   - Ranked by frequency in training data

5. **youtube_links.json** (100+ mappings)
   - Format: {skill: youtube_url}
   - Curated tutorials (4+ star ratings)
   - Updated quarterly

**Real User Example:**

```
Input: Resume with Python, Django, MySQL, Git

Output:
‚îú‚îÄ Recommended Job: "Software Developer" (85% confidence)
‚îú‚îÄ Match Percentage: 60% (9/15 skills matched)
‚îú‚îÄ Missing Skills: Docker, Kubernetes, React, AWS, CI/CD, Redis
‚îú‚îÄ YouTube Links: 
‚îÇ  ‚Ä¢ Docker: https://youtube.com/watch?v=...
‚îÇ  ‚Ä¢ Kubernetes: https://youtube.com/watch?v=...
‚îú‚îÄ ATS Feedback:
‚îÇ  ‚Ä¢ ‚úÖ Skills section is present
‚îÇ  ‚Ä¢ ‚ö†Ô∏è Add "Professional Summary" at top
‚îÇ  ‚Ä¢ ‚ö†Ô∏è Use standard section headings
‚îú‚îÄ Live Jobs:
‚îÇ  1. Software Developer @ TCS (Mumbai) - Apply Now
‚îÇ  2. Backend Developer @ Infosys (Bangalore) - Apply Now
‚îî‚îÄ Time Taken: 9.2 seconds
```

**Why This Matters:**
- **Before:** Spend 2 hours researching job requirements manually
- **After:** Get precise recommendations in 10 seconds
- **Accuracy:** 85% ‚Üí Higher chance of interview callbacks
- **Learning:** Direct YouTube links ‚Üí Start upskilling immediately

---

### 2. ü§ñ AI Career Advisor - Your Personal Career Counselor

**What It Does:**
Chat with an AI career advisor that's been trained on 749 real career counseling conversations. Ask about any career path, required skills, certifications, salary expectations, or career transitions‚Äîget structured, actionable advice in seconds.

**How It Helps Users:**
- **Students:** Explore different career options before choosing a major
- **Career Changers:** Understand requirements for transitioning to new fields
- **Professionals:** Learn about specializations, certifications, salary growth
- **Job Seekers:** Get detailed guidance on specific roles or industries

**Complete Workflow & Implementation:**

```
Step 1: User Types Question
   ‚Ä¢ Examples:
     - "Tell me about Data Science careers"
     - "How do I become a DevOps engineer?"
     - "What certifications do I need for cloud roles?"
   ‚Üì
Step 2: Check Model Status
   ‚Ä¢ Check if fine-tuned GPT-2 is loaded in memory
   ‚Ä¢ Model location: ./career-advisor-final/
   ‚Ä¢ Loading time: 5-10 seconds (background thread on startup)
   ‚Üì
Step 3A: Primary Path (Fine-Tuned GPT-2)
   ‚Ä¢ Model: GPT-2-Medium (355M parameters)
   ‚Ä¢ Fine-tuning: 749 career Q&A pairs
   ‚Ä¢ Training: 15 epochs, 1e-5 learning rate
   ‚Ä¢ Prompt format: "### Question: {user_query}\n\n### Answer:"
   ‚Ä¢ Generation params:
     - max_length: 200 tokens (adjustable)
     - temperature: 0.7 (balance creativity/consistency)
     - top_p: 0.9 (nucleus sampling)
     - top_k: 50 (top-k sampling)
     - repetition_penalty: 1.2 (avoid repetition)
   ‚Ä¢ Output: Structured response (skills, certs, salary)
   ‚Ä¢ Time: 3-5 seconds
   ‚Üì
Step 3B: Fallback Path (RAG System)
   ‚Ä¢ Trigger: Model not loaded or generates poor output
   ‚Ä¢ Source: career_guides.json (career knowledge base)
   ‚Ä¢ Embedding model: all-MiniLM-L6-v2 (384 dimensions)
   ‚Ä¢ Vector store: FAISS (IndexFlatL2)
   ‚Ä¢ Process:
     1. Embed user question
     2. Similarity search (k=4 chunks)
     3. Retrieve relevant career guide content
     4. Feed to Gemini Pro with context
   ‚Ä¢ Output: Context-aware answer from guides
   ‚Ä¢ Time: 2-3 seconds
   ‚Üì
Step 4: Format Response
   ‚Ä¢ Parse markdown structure (###, ####)
   ‚Ä¢ Identify bullet points, numbered lists
   ‚Ä¢ Apply typography hierarchy
   ‚Ä¢ Add purple accent colors (#a78bfa)
   ‚Ä¢ Bold/italic text rendering
   ‚Üì
Step 5: Enhance with Live Jobs (Optional)
   ‚Ä¢ If job title mentioned, scrape LinkedIn
   ‚Ä¢ Append 3-5 relevant job postings
   ‚Ä¢ Include: Title, company, location, apply link
   ‚Üì
Step 6: Save to History (If Logged In)
   ‚Ä¢ Table: career_queries
   ‚Ä¢ Fields: query_text, model_used, response_time
   ‚Ä¢ Enable tracking conversation history
   ‚Üì
Step 7: Display Premium-Formatted Response
   ‚Ä¢ Total Time: 3-7 seconds
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Base LLM** | GPT-2-Medium | Text generation | 355M params, 24 layers |
| **Fine-Tuning** | HuggingFace Transformers | Model training | Trainer API, gradient accumulation |
| **Training Dataset** | career_advice_dataset.jsonl | Career knowledge | 749 Q&A pairs, 80/20 split |
| **Tokenizer** | GPT-2 Tokenizer | Text encoding | BPE tokenization, 50k vocab |
| **Framework** | PyTorch 2.7.1 | Deep learning backend | GPU acceleration support |
| **Optimization** | Mixed Precision (FP16) | Faster training | Reduces memory, 2x speedup |
| **RAG Embeddings** | all-MiniLM-L6-v2 | Semantic search | 384-dim vectors, 90MB model |
| **Vector DB** | FAISS (Facebook AI) | Similarity search | IndexFlatL2, exact search |
| **RAG LLM** | Google Gemini Pro | Fallback generation | Context-aware responses |
| **Frontend Formatting** | Custom Markdown Parser | Premium display | Headers, bullets, bold/italic |

**Model Training Details:**

**Training Configuration:**
```python
Base Model: gpt2-medium (355M parameters)
Architecture: Transformer decoder
  - Layers: 24
  - Attention heads: 16
  - Hidden size: 1024
  - Vocab size: 50,257 tokens

Training Data:
  - Total examples: 749 Q&A pairs
  - Train set: 599 examples (80%)
  - Validation set: 150 examples (20%)
  - Max sequence length: 512 tokens
  - Average Q&A length: 300 tokens

Hyperparameters:
  - Epochs: 15
  - Learning rate: 1e-5 (AdamW optimizer)
  - Batch size: 2 (per device)
  - Gradient accumulation: 8 steps (effective batch = 16)
  - Warmup steps: 100
  - Weight decay: 0.01
  - FP16: True (mixed precision)
  - Max grad norm: 1.0

Training Environment:
  - GPU: RTX 2050 or better
  - Memory: ~8GB VRAM
  - Training time: 15-20 minutes (GPU)
  - CPU training: 6+ hours (not recommended)

Results:
  - Final training loss: 0.87
  - Validation loss: 1.12
  - Perplexity: 2.39
  - Total steps: ~1,500 (100 steps/epoch)

Model Size:
  - Original GPT-2-Medium: 1.5GB
  - Fine-tuned model: 1.5GB (same size)
  - Format: PyTorch safetensors
  - Files: model.safetensors, config.json, tokenizer files
```

**Training Dataset (career_advice_dataset.jsonl):**

```json
Sample Entry:
{
  "prompt": "What skills are required for a Data Scientist in India?",
  "completion": "### Key Skills:\n* Python, R, SQL\n* Machine Learning: Regression, Classification, Clustering\n* Deep Learning: TensorFlow, PyTorch\n* Data Visualization: Tableau, Power BI\n* Statistics, Probability\n\n### Tools:\n* Jupyter Notebook, Google Colab\n* Git, Docker\n\n### Top Certifications:\n* Google Cloud Professional Data Engineer\n* AWS Certified Machine Learning Specialty\n* TensorFlow Developer Certificate\n\n### Salary Range: ‚Çπ8-25 LPA (India)\n\n### Career Path:\n1. Junior Data Analyst (1-2 years)\n2. Data Scientist (3-5 years)\n3. Senior Data Scientist (5-8 years)\n4. Lead Data Scientist / ML Architect (8+ years)"
}
```

**Dataset Coverage:**
- Career paths: 50+ roles (Software, Data, DevOps, QA, Product, Design, etc.)
- Topics: Skills, certifications, salary, career progression
- Geography: India-specific (INR salaries, local certifications)
- Quality: Human-written, structured format

**Generation Quality Metrics:**

| Metric | Score | Description |
|--------|-------|-------------|
| **Coherence** | 88% | Responses are logically structured |
| **Accuracy** | 68% | Factually correct career info |
| **Completeness** | 75% | Covers skills, certs, salary |
| **Formatting** | 95% | Markdown structure preserved |
| **Relevance** | 82% | Answers match question intent |

**RAG Fallback System:**

When fine-tuned model is unavailable:
1. **Embed question** with all-MiniLM-L6-v2
2. **Search** FAISS index over career_guides.json
3. **Retrieve** top 4 relevant chunks (cosine similarity)
4. **Generate** answer using Gemini Pro with context
5. **Format** response with same styling

**Real User Example:**

```
User: "Tell me about DevOps careers"

AI Response (Fine-Tuned GPT-2):

### Key Skills for DevOps Engineer:
‚Ä¢ **Containerization:** Docker, Kubernetes
‚Ä¢ **CI/CD:** Jenkins, GitLab CI, GitHub Actions
‚Ä¢ **Cloud Platforms:** AWS (EC2, S3, Lambda), Azure, GCP
‚Ä¢ **Infrastructure as Code:** Terraform, Ansible
‚Ä¢ **Scripting:** Bash, Python
‚Ä¢ **Monitoring:** Prometheus, Grafana, ELK Stack

### Top Certifications:
‚Ä¢ AWS Certified DevOps Engineer - Professional
‚Ä¢ Certified Kubernetes Administrator (CKA)
‚Ä¢ Docker Certified Associate
‚Ä¢ HashiCorp Terraform Associate

### Salary Range in India:
‚Ä¢ Junior DevOps: ‚Çπ4-8 LPA
‚Ä¢ Mid-level: ‚Çπ8-15 LPA  
‚Ä¢ Senior: ‚Çπ15-25 LPA
‚Ä¢ Lead/Architect: ‚Çπ25-40 LPA

### Career Progression:
1. System Administrator (1-2 years)
2. DevOps Engineer (2-5 years)
3. Senior DevOps Engineer (5-8 years)
4. DevOps Architect / SRE Lead (8+ years)

[+ 5 Live DevOps Jobs from LinkedIn]

Time: 4.2 seconds
```

**Why This Matters:**
- **Before:** Read 10+ blog posts, spend 2 hours researching
- **After:** Get comprehensive answer in 5 seconds
- **Personalized:** Trained on real career counseling data
- **Structured:** Easy to scan, actionable information
- **Up-to-date:** Enhanced with live job postings

---

### 3. üéØ Resume-JD Analyzer - Precision Job Matching

**What It Does:**
Upload your resume alongside a specific job description to get an exact ATS compatibility score. This feature analyzes both documents side-by-side and tells you precisely which skills match, which are missing, and how to optimize your resume for that specific job posting.

**How It Helps Users:**
- **Job Applicants:** Know your exact match before applying (save time on low-match jobs)
- **Resume Tailoring:** Customize resume for each application with data-driven insights
- **Interview Prep:** Focus on missing skills that interviewers will likely ask about
- **Career Switchers:** Understand gap between current and target role requirements

**Complete Workflow & Implementation:**

```
Step 1: Upload Two Documents
   ‚Ä¢ Document 1: Your resume (PDF/DOCX)
   ‚Ä¢ Document 2: Job description (PDF/DOCX)
   ‚Ä¢ Validation: Check file types, size limits (<10MB)
   ‚Üì
Step 2: Parallel Text Extraction
   ‚Ä¢ Resume extraction: pdfplumber/python-docx
   ‚Ä¢ JD extraction: Same libraries
   ‚Ä¢ Text cleaning: Remove special chars, normalize whitespace
   ‚Ä¢ Time: 1-2 seconds (concurrent processing)
   ‚Üì
Step 3: Dual AI Skill Extraction
   ‚Ä¢ Two separate Gemini Pro API calls:
     
     Call 1 - Resume Skills:
       Prompt: "Extract skills from this resume: {resume_text}"
       Model: gemini-1.5-pro
       Temperature: 0.3
       Output: ["python", "django", "react", "mysql", "git"]
     
     Call 2 - Job Description Skills:
       Prompt: "Extract required skills from job description: {jd_text}"
       Model: gemini-1.5-pro
       Temperature: 0.3
       Output: ["python", "django", "react", "docker", "kubernetes", "aws"]
   
   ‚Ä¢ Validation: Cross-check with skills_db.json
   ‚Ä¢ Fallback: Regex extraction if API fails
   ‚Ä¢ Time: 4-6 seconds (parallel calls)
   ‚Üì
Step 4: Skill Normalization & Matching
   ‚Ä¢ Normalize variations:
     - "React.js" ‚Üí "react"
     - "PostgreSQL" ‚Üí "postgres"
     - "Node.js" ‚Üí "nodejs"
     - "CI/CD" ‚Üí "cicd"
   ‚Ä¢ 50+ synonym mappings applied
   
   ‚Ä¢ Set operations:
     matching_skills = resume_skills ‚à© jd_skills
     missing_skills = jd_skills - resume_skills
     extra_skills = resume_skills - jd_skills
   
   ‚Ä¢ Example:
     Resume: {python, django, mysql, git, react}
     JD Required: {python, django, react, docker, kubernetes, aws}
     Matching: {python, django, react} = 3 skills
     Missing: {docker, kubernetes, aws} = 3 skills
   ‚Üì
Step 5: ATS Score Calculation
   ‚Ä¢ Formula: (matching_skills / total_jd_skills) √ó 100
   ‚Ä¢ Example: (3 / 6) √ó 100 = 50% match
   
   ‚Ä¢ Score Categories:
     - 80-100%: Excellent match (high interview chance)
     - 60-79%: Good match (competitive candidate)
     - 40-59%: Fair match (some gaps to fill)
     - 0-39%: Poor match (major skill gaps)
   ‚Üì
Step 6: ATS Layout Feedback
   ‚Ä¢ Same as CV Analyzer (Gemini + fallback)
   ‚Ä¢ Specific to resume document only
   ‚Ä¢ Optimization tips for ATS parsing
   ‚Ä¢ Time: 2-3 seconds
   ‚Üì
Step 7: YouTube Learning Path Generation
   ‚Ä¢ For each missing skill:
     1. Lookup in youtube_links.json
     2. Return {skill_name, youtube_link}
   ‚Ä¢ Curated tutorials (4+ ratings)
   ‚Ä¢ Direct learning path
   ‚Üì
Step 8: Professional UI Display
   ‚Ä¢ ATS Score Card:
     - Large percentage with gradient progress bar
     - Color-coded (green: 80+, orange: 60-79, red: <60)
   
   ‚Ä¢ Matching Skills Card (Green):
     - Skills you have that JD requires
     - Shows your strengths
   
   ‚Ä¢ Missing Skills Card (Red):
     - Critical gaps to fill
     - YouTube link for each skill
     - Priority learning list
   
   ‚Ä¢ ATS Tips Card (Blue):
     - Layout improvements
     - Keyword suggestions
   
   ‚Ä¢ All Skills Card (Purple):
     - Complete skill inventory
     - Extra skills you have
   ‚Üì
Step 9: Return Results
   ‚Ä¢ Total Time: 10-15 seconds
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **LLM** | Google Gemini Pro | Dual skill extraction | 2 parallel API calls |
| **Text Extraction** | pdfplumber, python-docx | Document parsing | PDF/DOCX support |
| **Skill Normalization** | Custom synonym mapping | Reduce false negatives | 50+ mappings |
| **Matching** | Python set operations | Fast comparison | O(n) complexity |
| **Scoring** | Cosine similarity concept | ATS compatibility | Percentage match |
| **Learning Paths** | youtube_links.json | Tutorial mapping | 100+ curated links |
| **Frontend** | Material-UI Cards | Professional display | Color-coded sections |
| **Charts** | Custom gradient progress bar | Visual score | Green/orange/red |

**Skill Normalization (Key Innovation):**

**Problem:** False negatives due to variations
- Resume says "React.js" but JD says "react" ‚Üí Mismatch detected
- Result: Artificially low ATS score

**Solution:** Comprehensive synonym mapping
```python
SKILL_SYNONYMS = {
    'react.js': 'react',
    'reactjs': 'react',
    'react js': 'react',
    'node.js': 'nodejs',
    'node js': 'nodejs',
    'postgresql': 'postgres',
    'postgres sql': 'postgres',
    'sqlite3': 'sqlite',
    'ci/cd': 'cicd',
    'continuous integration': 'cicd',
    'oop': 'object-oriented programming',
    'restful api': 'rest api',
    'amazon web services': 'aws',
    # ... 40+ more mappings
}
```

**Impact:**
- Before normalization: 24 "missing" skills (many false positives)
- After normalization: 4 actual missing skills
- Accuracy improvement: 83% reduction in false positives

**Real User Example:**

```
Input:
‚îú‚îÄ Resume: Python, Django, PostgreSQL, React.js, Git, HTML/CSS
‚îî‚îÄ Job Description: Python, Django, Postgres, React, Docker, Kubernetes, AWS

After Normalization:
‚îú‚îÄ Resume Skills: {python, django, postgres, react, git, html, css}
‚îî‚îÄ JD Skills: {python, django, postgres, react, docker, kubernetes, aws}

Results:
‚îú‚îÄ ATS Score: 57% (4/7 skills matched)
‚îú‚îÄ Matching Skills: ‚úÖ Python, Django, Postgres, React
‚îú‚îÄ Missing Skills: ‚ùå Docker, Kubernetes, AWS
‚îÇ  ‚Ä¢ Docker: https://youtube.com/watch?v=...
‚îÇ  ‚Ä¢ Kubernetes: https://youtube.com/watch?v=...
‚îÇ  ‚Ä¢ AWS: https://youtube.com/watch?v=...
‚îú‚îÄ Extra Skills: Git, HTML, CSS (bonus but not required)
‚îî‚îÄ ATS Tip: "Add Cloud Computing section highlighting any AWS experience"

Time: 12.3 seconds
```

**Use Cases:**

**1. Job Application Strategy**
- Apply to 80%+ matches immediately
- For 60-79% matches: Learn 1-2 missing skills first
- Skip <40% matches: Too many gaps

**2. Resume Customization**
- Add missing keywords from JD to resume
- Highlight matching skills in summary section
- Reorder experience to emphasize relevant skills

**3. Interview Preparation**
- Focus study on missing critical skills
- Prepare to explain gaps if asked
- Showcase extra skills as differentiators

**4. Career Gap Analysis**
- Understand distance from dream job
- Create learning roadmap (3-6 months)
- Track progress by re-analyzing monthly

**Why This Matters:**
- **Precision:** Exact match percentage (not vague "good fit")
- **Actionable:** Direct YouTube links to learn missing skills
- **Time-Saving:** Know before applying (avoid rejections)
- **Strategic:** Prioritize which jobs to pursue
- **ATS-Friendly:** Optimize resume for specific role

---

### 4. üìö RAG Coach - Privacy-First Document Intelligence

**What It Does:**
Upload your resume and job descriptions as PDFs, then ask ANY question about them using a completely local AI system. Unlike cloud-based services, your documents never leave your computer‚Äîeverything runs locally using Ollama. Think of it as having a career coach who's read your entire resume and the job description, ready to answer questions instantly.

**How It Helps Users:**
- **Privacy-Conscious Users:** Documents processed 100% locally (no data sent to external APIs)
- **Interactive Analysis:** Ask follow-up questions, get context-aware answers
- **Document Comparison:** Query across multiple documents simultaneously
- **Custom Guidance:** "What should I emphasize in my cover letter?" type questions

**Complete Workflow & Implementation:**

```
Step 1: Upload Multiple Documents
   ‚Ä¢ Supported: PDF files (resume, job descriptions, cover letters)
   ‚Ä¢ Multiple file upload (2-10 documents typical)
   ‚Ä¢ Size limit: 10MB per file
   ‚Üì
Step 2: Automatic Document Detection
   ‚Ä¢ Content analysis using keyword matching:
     
     Resume Keywords:
       - "experience", "education", "skills"
       - "work history", "projects", "certifications"
       - Heuristic: 3+ keywords ‚Üí classified as RESUME
     
     Job Description Keywords:
       - "requirements", "qualifications", "responsibilities"
       - "about the role", "what you'll do", "must have"
       - Heuristic: 3+ keywords ‚Üí classified as JOB_DESCRIPTION
   
   ‚Ä¢ Tag each document with doc_type metadata
   ‚Ä¢ Enable filtered retrieval later
   ‚Ä¢ Time: <1 second
   ‚Üì
Step 3: Text Extraction & Chunking
   ‚Ä¢ PDF Parsing:
     Library: PyPDFLoader (LangChain)
     Extract: Text page-by-page
     Handle: Multi-column layouts, tables
   
   ‚Ä¢ Chunking Strategy:
     Splitter: RecursiveCharacterTextSplitter
     Chunk size: 500 characters
     Overlap: 50 characters (preserve context)
     Separators: ["\n\n", "\n", ". ", " "]
     
     Why 500 chars?
       - Balance: Not too small (loses context) or large (noise)
       - Typical: 3-5 sentences per chunk
       - Retrieval: Better precision with focused chunks
   
   ‚Ä¢ Example:
     Input: 5-page resume
     Output: ~30-40 chunks
     Metadata: {source: "resume.pdf", page: 1, doc_type: "RESUME"}
   
   ‚Ä¢ Time: 2-3 seconds
   ‚Üì
Step 4: Generate Embeddings
   ‚Ä¢ Model: sentence-transformers/all-MiniLM-L6-v2
     - Size: 90MB (CPU-friendly)
     - Dimensions: 384
     - Speed: ~1000 sentences/sec (CPU)
     - Normalization: L2 norm
   
   ‚Ä¢ Process:
     1. Tokenize each chunk (max 256 tokens)
     2. Pass through BERT-based encoder
     3. Mean pooling to get 384-dim vector
     4. L2 normalize for cosine similarity
   
   ‚Ä¢ Batch processing: 32 chunks at a time
   ‚Ä¢ Time: 3-5 seconds for 50 chunks
   ‚Üì
Step 5: Build FAISS Vector Index
   ‚Ä¢ Index Type: IndexFlatL2 (exact search, no approximation)
   ‚Ä¢ Distance Metric: L2 Euclidean distance
     - Equivalent to cosine similarity with normalized vectors
     - Formula: distance = sqrt(sum((a - b)^2))
   
   ‚Ä¢ Storage:
     - In-memory: Fast retrieval during session
     - Persistent: Saved to disk (./rag_coach_index/)
     - Reload: Available across restarts
   
   ‚Ä¢ Index size: ~50MB for 1000 chunks
   ‚Ä¢ Build time: <1 second
   ‚Üì
Step 6: Auto-Generated Initial Analysis
   ‚Ä¢ Extract skills from both resume and JD
   ‚Ä¢ Apply 50+ synonym normalizations
   ‚Ä¢ Set operations for gap analysis
   ‚Ä¢ Generate 3-5 resume bullet points using TinyLLama
   
   ‚Ä¢ Bullet Point Generation:
     Prompt: "Based on job requirements, suggest resume improvements"
     Context: Resume + JD chunks
     Output: ATS-optimized action-oriented bullets
   
   ‚Ä¢ Time: 5-8 seconds
   ‚Üì
Step 7: Interactive Q&A System
   ‚Ä¢ User asks question (e.g., "What certifications should I get?")
   ‚Üì
Step 7a: Query Embedding
   ‚Ä¢ Same model: all-MiniLM-L6-v2
   ‚Ä¢ Convert question to 384-dim vector
   ‚Ä¢ Time: <100ms
   ‚Üì
Step 7b: Similarity Search
   ‚Ä¢ FAISS retrieval:
     - Top-K: 4 most relevant chunks
     - Distance threshold: None (take top 4)
     - Filter: Can filter by doc_type if specified
   
   ‚Ä¢ Query Intent Detection:
     If "job description" in query ‚Üí Filter doc_type="JOB_DESCRIPTION"
     If "my resume" in query ‚Üí Filter doc_type="RESUME"
     Else ‚Üí Search all documents
   
   ‚Ä¢ Retrieved chunks include:
     - Text content
     - Source document name
     - Page number
     - Doc type
   
   ‚Ä¢ Time: <50ms
   ‚Üì
Step 7c: Local LLM Generation (TinyLLama)
   ‚Ä¢ Model: TinyLlama-1.1B-Chat-v1.0
     - Parameters: 1.1 billion
     - Quantization: Q4_K_M (4-bit)
     - Memory: ~4GB RAM
     - Inference: ~50 tokens/sec (CPU)
     - Context window: 2048 tokens
   
   ‚Ä¢ Deployment: Ollama (local inference server)
     - No internet connection needed
     - No data leaves your computer
     - Models stored locally (~637MB download)
   
   ‚Ä¢ Prompt Template:
     ```
     Context from your documents:
     {chunk1}
     {chunk2}
     {chunk3}
     {chunk4}
     
     Question: {user_question}
     
     Answer based on the context above:
     ```
   
   ‚Ä¢ Generation Params:
     - temperature: 0.7
     - max_tokens: 512
     - top_p: 0.9
     - repeat_penalty: 1.1
   
   ‚Ä¢ Time: 5-10 seconds (depends on answer length)
   ‚Üì
Step 8: Response with Source Attribution
   ‚Ä¢ Return:
     - Answer text
     - Source chunks used
     - Document names + page numbers
   
   ‚Ä¢ Example:
     Answer: "You should add Docker and Kubernetes to your skills section..."
     Sources: [resume.pdf (page 1), job_description.pdf (page 2)]
   
   ‚Ä¢ Frontend: Display sources below answer
   ‚Üì
Step 9: Conversation Memory
   ‚Ä¢ Store Q&A in database (rag_coach_queries table)
   ‚Ä¢ Enable history tracking
   ‚Ä¢ Reuse context for follow-up questions
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Local LLM** | TinyLlama 1.1B (Q4) | Answer generation | 100% local, CPU-friendly |
| **Inference Server** | Ollama | Model deployment | REST API, background service |
| **Embeddings** | all-MiniLM-L6-v2 | Semantic search | 384-dim, 90MB model |
| **Vector DB** | FAISS (IndexFlatL2) | Similarity search | Exact search, no approximation |
| **PDF Parser** | PyPDFLoader (LangChain) | Text extraction | Page-by-page parsing |
| **Text Splitter** | RecursiveCharacterTextSplitter | Chunking | 500 chars, 50 overlap |
| **Framework** | LangChain 0.3.27 | RAG orchestration | Document loaders, chains |
| **Storage** | Disk persistence | Index saving | Reload without re-embedding |

**System Architecture:**

```
User's Computer (Local)
‚îú‚îÄ Frontend (React)
‚îÇ  ‚îî‚îÄ File upload component
‚îÇ
‚îú‚îÄ Backend (FastAPI)
‚îÇ  ‚îú‚îÄ /rag-coach/upload endpoint
‚îÇ  ‚îú‚îÄ PDF parsing (PyPDFLoader)
‚îÇ  ‚îú‚îÄ Chunking (RecursiveCharacterTextSplitter)
‚îÇ  ‚îî‚îÄ /rag-coach/query endpoint
‚îÇ
‚îú‚îÄ Embedding Model (Local)
‚îÇ  ‚îú‚îÄ Model: all-MiniLM-L6-v2
‚îÇ  ‚îú‚îÄ Location: ~/.cache/huggingface/
‚îÇ  ‚îî‚îÄ Device: CPU (no GPU needed)
‚îÇ
‚îú‚îÄ FAISS Index (Local)
‚îÇ  ‚îú‚îÄ Type: IndexFlatL2
‚îÇ  ‚îú‚îÄ Location: ./rag_coach_index/
‚îÇ  ‚îî‚îÄ Size: ~50MB per 1000 chunks
‚îÇ
‚îî‚îÄ Ollama Server (Local)
   ‚îú‚îÄ TinyLlama model
   ‚îú‚îÄ Port: 11434
   ‚îú‚îÄ Storage: ~/.ollama/models/
   ‚îî‚îÄ Privacy: NO external API calls

External APIs Used: NONE ‚úÖ
```

**Privacy Guarantees:**

| Aspect | Status | Details |
|--------|--------|---------|
| **Document Storage** | ‚úÖ Local | Saved to ./uploads/ then deleted |
| **Embeddings** | ‚úÖ Local | Generated on your CPU |
| **Vector Index** | ‚úÖ Local | FAISS index on disk |
| **LLM Inference** | ‚úÖ Local | TinyLlama via Ollama |
| **Network Calls** | ‚ùå None | Zero external API calls |
| **Data Retention** | ‚úÖ User control | Delete anytime |

**Key Innovation: Skill Normalization**

**Problem Before:**
```
Resume: "React.js", "Node.js", "PostgreSQL"
JD: "react", "nodejs", "postgres"
Result: 0 matches detected (false negative)
User sees: "Add React.js" (but they already have it!)
```

**Solution:**
```python
SKILL_SYNONYMS = {
    'react.js': 'react',
    'reactjs': 'react',
    'react js': 'react',
    'node.js': 'nodejs',
    'node js': 'nodejs',
    'postgresql': 'postgres',
    'postgres sql': 'postgres',
    'sqlite3': 'sqlite',
    'ci/cd': 'cicd',
    'continuous integration': 'cicd',
    # ... 50+ total mappings
}

def normalize_skill(skill):
    skill_lower = skill.lower().strip()
    return SKILL_SYNONYMS.get(skill_lower, skill_lower)
```

**Impact:**
- Before: 24 "missing" skills (false positives)
- After: 4 actual missing skills
- Accuracy: 83% improvement in gap detection

**Real User Example:**

```
Step 1: User uploads 2 files
‚îú‚îÄ resume.pdf (3 pages)
‚îî‚îÄ job_description.pdf (2 pages)

Step 2: Auto-analysis generated
‚îú‚îÄ Skills to add: Docker, Kubernetes, CI/CD
‚îú‚îÄ Resume bullets:
‚îÇ  ‚Ä¢ "Implemented REST APIs using Django framework for microservices"
‚îÇ  ‚Ä¢ "Designed PostgreSQL database schema for user management system"
‚îî‚îÄ ATS keywords: Python, Django, REST API, Docker, Kubernetes

Step 3: User asks: "What projects should I highlight?"

Step 4: System retrieves relevant chunks
‚îú‚îÄ resume.pdf (page 2): "Projects: E-commerce platform, Chat app..."
‚îî‚îÄ job_description.pdf (page 1): "Experience with scalable web applications..."

Step 5: TinyLlama generates answer
"Based on the job requirements, you should highlight:
1. Your e-commerce platform project - emphasize scalability aspects
2. Chat application - mention real-time features and backend architecture
3. Any projects using Docker/containers (add if missing)
Focus on quantifiable results (user load, performance metrics)"

Sources: resume.pdf (page 2), job_description.pdf (page 1)
Time: 7.3 seconds
```

**Use Cases:**

1. **Cover Letter Writing**
   - Q: "What should I mention in my cover letter?"
   - A: Context-aware suggestions from both docs

2. **Interview Prep**
   - Q: "What questions might they ask based on this JD?"
   - A: Gap analysis + likely interview topics

3. **Resume Tailoring**
   - Q: "Which experiences match the role best?"
   - A: Relevant projects/jobs from resume

4. **Skill Prioritization**
   - Q: "Which missing skill should I learn first?"
   - A: Critical vs. nice-to-have skills

**Why This Matters:**
- **Privacy:** Your documents never uploaded to external servers
- **Flexibility:** Ask ANY question about your documents
- **Accuracy:** Context-aware answers (not generic AI)
- **Speed:** 7-10 seconds per Q&A (local inference)
- **Cost:** Free (no API credits, no subscriptions)

---

### 5. üìä Admin Dashboard (New!)

**Comprehensive analytics and user management for administrators**

**Features:**
- **KPI Cards:** Total users, active users (7d/30d), analyses, average match score
- **Retention Metrics:** Overall, 7-day, and 30-day retention rates
- **User Growth Chart:** 30-day time-series visualization
- **Match Score Distribution:** Pie chart categorizing ATS scores
- **Top Jobs & Skills:** Bar charts showing most recommended jobs and missing skills
- **Recent Activity Feed:** Real-time user action tracking
- **User Management:** Suspend, activate, delete users

**Technologies:**
- **Charts:** Recharts (Area, Bar, Pie charts)
- **Data Visualization:** Material-UI Grid with responsive layout
- **Backend:** Comprehensive `/admin/stats` endpoint with 15+ metrics
- **Design:** Wozber-style minimalist cards with gradient accents

**Access:**
- Admin-only route: `/admin`
- Role-based authentication with JWT
- Create admin user: `python create_admin.py`

---

## Technology Stack

### Frontend

**React 18 + Material-UI v5 + Aurora WebGL**

```javascript
Core Libraries:
  - React 18.2.0 (hooks, context API)
  - Material-UI v5.14.19 (components, theming)
  - React Router v6.20.1 (client-side routing)
  - Axios 1.6.2 (HTTP client with JWT)
  - Recharts 2.10.3 (data visualization)
  - OGL 1.0.11 (WebGL library for Aurora)

Aurora Effect Implementation:
  - Technology: WebGL shaders (vertex + fragment)
  - Library: OGL (7KB, GPU-accelerated)
  - Color Scheme: Purple (#8b5cf6) ‚Üí Blue (#3b82f6) ‚Üí Green (#10b981)
  - Performance: 60 FPS at 1080p
  - Configuration:
    * Amplitude: 1.5
    * Blend: 0.8
    * Speed: 0.4
    * Noise: Simplex noise algorithm

UI Features:
  - Wozber-Style Design: Clean white cards with subtle shadows
  - Light Theme: Light gray (#F9FAFB) background with indigo/cyan accents
  - Protected Routes: JWT-based authentication guards
  - Responsive Design: Mobile-first with MUI breakpoints
  - Custom Fonts: Poppins (primary), Inter, Space Grotesk (headings)
  - Premium Formatting: Markdown-style AI responses with typography hierarchy
```

**Aurora Technical Details:**
```javascript
// Vertex Shader (GLSL 3.0)
#version 300 es
in vec2 position;
void main() {
  gl_Position = vec4(position, 0.0, 1.0);
}

// Fragment Shader (Simplex Noise)
- Generates animated gradient using GPU
- Color interpolation between 3 stops
- Time-based animation (uTime uniform)
- Resolution-aware (uResolution uniform)
```

---

### Backend

**FastAPI + Python 3.10 + Async Architecture**

```python
Core Framework:
  - FastAPI 0.116.1 (async ASGI)
  - Uvicorn 0.35.0 (ASGI server)
  - Pydantic 2.11.7 (data validation)

Authentication:
  - python-jose 3.5.0 (JWT tokens)
  - passlib 1.7.4 (bcrypt hashing)
  - Google OAuth 2.0 (SSO)

Database:
  - SQLAlchemy 2.0.43 (ORM)
  - SQLite 3 (file-based, zero configuration)

API Features:
  - RESTful endpoints (/analyze_resume/, /rag-coach/*, /admin/*)
  - Admin dashboard with comprehensive analytics
  - Role-based access control (user/admin)
  - CORS enabled (localhost:3000, localhost:8501)
  - Lazy model loading (background threads)
  - Comprehensive logging

Additional Features:
  - Admin Dashboard: User analytics, retention metrics, engagement tracking
  - History Tracking: Resume analyses, career queries, RAG interactions
  - User Management: Suspend, activate, delete users (admin only)
```

---

### AI & Machine Learning

**LLMs:**
```
1. Google Gemini Pro (API)
   - Purpose: Skill extraction, ATS feedback, RAG fallback
   - Model: gemini-1.5-pro
   - Context Window: 32k tokens
   - Temperature: 0.3 (consistent outputs)
   - Fallback: Regex-based extraction (100+ patterns)

2. GPT-2 Medium (Fine-tuned)
   - Purpose: Career advice generation
   - Parameters: 355 million
   - Size: 1.5GB
   - Training: 749 examples, 15 epochs
   - Format: PyTorch (HuggingFace)
   - Fallback: RAG over career guides

3. TinyLLama 1.1B (RAG)
   - Purpose: Document Q&A
   - Parameters: 1.1 billion
   - Quantization: Q4_K_M (4-bit)
   - Memory: ~4GB RAM
   - Inference: CPU-friendly (~50 tokens/sec)
   - Privacy: 100% local execution via Ollama
```

**ML Models:**
```python
Job Classification:
  - Algorithm: Multinomial Naive Bayes
  - Feature Engineering: TF-IDF Vectorization
  - Training Data: 8000+ job-skill mappings (jobs_cleaned.csv)
  - Accuracy: ~85% on test set
  - Inference Time: <10ms
  - Model Size: 450KB

  Categories (12 groups):
    ‚Ä¢ Data Professional
    ‚Ä¢ Software Developer
    ‚Ä¢ IT Operations & Infrastructure
    ‚Ä¢ Project / Product Manager
    ‚Ä¢ QA / Test Engineer
    ‚Ä¢ Human Resources
    ‚Ä¢ Sales & Business Development
    ‚Ä¢ Marketing
    ‚Ä¢ UI/UX & Design
    ‚Ä¢ Finance & Accounting
    ‚Ä¢ Customer Support
    ‚Ä¢ Other

  Hyperparameters (GridSearchCV):
    ‚Ä¢ Vectorizer: TfidfVectorizer(ngram_range=(1,2))
    ‚Ä¢ Classifier: MultinomialNB(alpha=0.1-1.0) OR LogisticRegression
    ‚Ä¢ Train/Test Split: 80/20
    ‚Ä¢ CV Folds: 5
    ‚Ä¢ Scoring: F1 Weighted
```

**RAG System:**
```python
Vector Database: FAISS
  - Index Type: IndexFlatL2 (exact search)
  - Distance Metric: L2 Euclidean
  - Persistent Storage: ./rag_data/faiss_index

Embeddings: HuggingFace Sentence Transformers
  - Model: all-MiniLM-L6-v2
  - Dimensions: 384
  - Normalization: L2 norm
  - Batch Size: 32

Document Processing:
  - Loader: PyPDFLoader (LangChain)
  - Chunking: RecursiveCharacterTextSplitter
    * chunk_size: 500
    * chunk_overlap: 50
    * separators: ["\n\n", "\n", ". ", " "]
  - Metadata: source, doc_type, page, doc_index

Retrieval:
  - Strategy: Similarity search
  - Top-K: 4 chunks per query
  - Filtering: Document type (resume vs JD)
  - Context Window: 2048 tokens (TinyLLama)
```

**Document Parsing:**
```
PDF: pdfplumber 0.10.3 (complex layouts)
DOCX: python-docx 1.1.0 (paragraph extraction)
Text Splitting: langchain 0.0.335
```

---

## AI Models & Parameters

### Fine-Tuned LLM (Career Advisor)

**Training Configuration:**
```python
# Base Model
model_name = "gpt2-medium"
num_parameters = 355_000_000
architecture = "Transformer decoder (24 layers, 16 attention heads)"

# Training Data
train_dataset = "career_advice_dataset.jsonl + career_advice_ultra_clear_dataset.jsonl"
total_examples = 749
train_val_split = "80/20 (599 train, 150 val)"

# Training Hyperparameters
training_args = {
    "num_train_epochs": 15,
    "learning_rate": 1e-5,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,  # Effective batch = 16
    "max_seq_length": 512,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "fp16": True,  # Mixed precision (GPU only)
    "logging_steps": 50,
    "save_strategy": "epoch",
    "evaluation_strategy": "epoch"
}

# Training Results
total_steps = 1500  # ~100 steps/epoch
training_time_gpu = "15-20 minutes (RTX 2050)"
training_time_cpu = "6+ hours (not recommended)"
final_loss = 0.87
validation_perplexity = 2.39

# Generation Config
generation_config = {
    "max_length": 200,
    "temperature": 0.7,  # 0.5-0.9 for coherence
    "top_p": 0.9,        # Nucleus sampling
    "top_k": 50,         # Top-k sampling
    "repetition_penalty": 1.2,
    "do_sample": True
}
```

**Dataset Structure:**
```json
{
  "prompt": "What skills are required for a Data Scientist in India?",
  "completion": "### Key Skills:\n* Python, R, SQL\n* ML: Regression, Trees, Deep Learning\n\n### Certifications:\n* Google Cloud Professional Data Engineer\n\n### Salary: ‚Çπ8-25 LPA"
}
```

---

### RAG System (Document Q&A)

**TinyLLama Configuration:**
```python
# Model Setup (Ollama)
model_name = "tinyllama"
full_name = "TinyLlama-1.1B-Chat-v1.0"
parameters = 1_100_000_000
quantization = "Q4_K_M (4-bit)"
model_size = "637MB download"
memory_usage = "~4GB RAM"

# Inference Performance
tokens_per_second_cpu = 50
tokens_per_second_gpu = 150
context_window = 2048
temperature = 0.7
max_tokens = 512
top_p = 0.9
repeat_penalty = 1.1

# Privacy
execution = "100% local (no API calls)"
data_retention = "Zero (PDFs can be deleted post-indexing)"
```

**FAISS Vector Store:**
```python
# Index Configuration
index_type = "IndexFlatL2"  # Exact search, no compression
distance_metric = "L2 Euclidean distance"
dimension = 384  # all-MiniLM-L6-v2 embedding size

# Storage
index_path = "./rag_data/faiss_index"
persistence = "Disk (loaded on startup)"
index_size = "~50MB for 1000 chunks"

# Retrieval Parameters
search_type = "similarity"
k = 4  # Top-4 chunks
score_threshold = 0.7  # Minimum similarity
return_metadata = True  # source, doc_type, page
```

**Embedding Model:**
```python
# HuggingFace Sentence Transformers
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_size = "90MB"
embedding_dimension = 384
max_sequence_length = 256
normalization = "L2 norm"
device = "cpu"  # Lightweight, no GPU needed

# Performance
encoding_speed = "~1000 sentences/sec (CPU)"
batch_size = 32
```

---

### ML Model Training (Job Classification)

**Model Training Script:** `model_training.py`

```python
# Data Loading
dataset = pd.read_csv("jobs_cleaned.csv")
total_records = 8000+
columns = ["Job Title", "Skills", "Grouped_Title"]

# Preprocessing
skill_validation = "skills_db.json (10,000+ valid skills)"
job_consolidation = "54 unique titles ‚Üí 10 groups"
normalization = "lowercase, strip whitespace"
train_test_split = "80/20"

# Feature Engineering
vectorizer = TfidfVectorizer(
    max_features=500,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)

# Model Selection (GridSearchCV)
algorithms_tested = [
    "MultinomialNB",
    "LogisticRegression"
]
cv_folds = 5
best_algorithm = "MultinomialNB(alpha=1.0)"

# Training
fit_time = "~2 minutes"
accuracy = 0.85
precision = 0.83
recall = 0.82
f1_score = 0.82

# Output Artifacts
saved_models = [
    "job_recommender_pipeline.joblib",     # TF-IDF + NB
    "job_title_encoder.joblib",            # LabelEncoder
    "prioritized_skills.joblib",           # Job ‚Üí skills mapping
    "master_skill_vocab.joblib"            # Complete vocabulary
]
```

---

## How It Works

### 1. CV Analyzer - Complete Pipeline

**Stage-by-Stage Breakdown:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 1: File Upload & Text Extraction             ‚îÇ
‚îÇ ‚îú‚îÄ PDF: pdfplumber (handles complex layouts)       ‚îÇ
‚îÇ ‚îî‚îÄ DOCX: python-docx (paragraph extraction)        ‚îÇ
‚îÇ Output: Plain text (500-5000 chars)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 2: AI Skill Extraction                       ‚îÇ
‚îÇ ‚îú‚îÄ Primary: Gemini LLM (contextual NER)            ‚îÇ
‚îÇ ‚îÇ   - Prompt: Extract technical skills, tools      ‚îÇ
‚îÇ ‚îÇ   - Temperature: 0.3 (consistency)               ‚îÇ
‚îÇ ‚îÇ   - Time: 2-4 seconds                            ‚îÇ
‚îÇ ‚îî‚îÄ Fallback: 9 RegEx patterns (100+ technologies)  ‚îÇ
‚îÇ Output: ["python", "django", "mysql", "git"]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 3: ML Job Prediction                         ‚îÇ
‚îÇ ‚îú‚îÄ TF-IDF Vectorization (skills ‚Üí numeric vector)  ‚îÇ
‚îÇ ‚îú‚îÄ Naive Bayes Classification (P(Job|Skills))      ‚îÇ
‚îÇ ‚îî‚îÄ LabelEncoder (decode to job title)              ‚îÇ
‚îÇ Output: "Software Developer" (85% accuracy)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 4: Skill Gap Analysis                        ‚îÇ
‚îÇ ‚îú‚îÄ Retrieve required skills from database          ‚îÇ
‚îÇ ‚îú‚îÄ Set operations (required - user_skills)         ‚îÇ
‚îÇ ‚îî‚îÄ Calculate match percentage                      ‚îÇ
‚îÇ Output: Match 42%, Missing ["docker", "k8s"]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 5: AI Layout Feedback                        ‚îÇ
‚îÇ ‚îú‚îÄ Primary: Gemini LLM (ATS optimization)          ‚îÇ
‚îÇ ‚îî‚îÄ Fallback: 7 rule-based checks                   ‚îÇ
‚îÇ Output: "‚úÖ Add professional summary"               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 6: Live Job Scraping                         ‚îÇ
‚îÇ ‚îú‚îÄ LinkedIn job search (India)                     ‚îÇ
‚îÇ ‚îú‚îÄ BeautifulSoup parsing (3 fallback selectors)    ‚îÇ
‚îÇ ‚îî‚îÄ Browser emulation headers                       ‚îÇ
‚îÇ Output: Top 5 jobs with links                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 7: YouTube Tutorial Mapping                  ‚îÇ
‚îÇ ‚îú‚îÄ JSON lookup (youtube_links.json)                ‚îÇ
‚îÇ ‚îî‚îÄ Map each missing skill to tutorial              ‚îÇ
‚îÇ Output: Skill ‚Üí YouTube link                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 8: Database Storage (if logged in)           ‚îÇ
‚îÇ ‚îú‚îÄ SQLAlchemy ORM                                  ‚îÇ
‚îÇ ‚îî‚îÄ Save to ResumeAnalysis table                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 9: Return JSON Response                      ‚îÇ
‚îÇ Total Time: 8-12 seconds end-to-end                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Performance Metrics:**

| Stage | Time | Notes |
|-------|------|-------|
| Text Extraction | <1s | Typical 2-page resume |
| Skill Extraction | 2-4s | Gemini API latency |
| Job Prediction | <0.1s | Pre-trained ML model |
| Gap Analysis | <0.01s | Python set operations |
| Layout Feedback | 2-3s | Or instant (fallback) |
| Job Scraping | 2-4s | Network-dependent |
| YouTube Mapping | <0.01s | In-memory lookup |
| **TOTAL** | **8-12s** | **Full pipeline** |

---

### 2. AI Career Advisor - Dual System

**Workflow:**

```
User Query: "Tell me about DevOps"
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Check Model  ‚îÇ
    ‚îÇ   Status     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Model Loaded?   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       /           \
    YES             NO
     ‚Üì               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇFine-tuned‚îÇ   ‚îÇ   RAG    ‚îÇ
‚îÇ  GPT-2   ‚îÇ   ‚îÇ  System  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì               ‚Üì
   Generate      Retrieve
   Response      Context
     ‚Üì               ‚Üì
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Enhance with    ‚îÇ
    ‚îÇ Live Jobs       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Return JSON     ‚îÇ
    ‚îÇ Response        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fine-Tuned Model Path:**
```python
# Generation Process
input_text = "### Question: Tell me about DevOps\n\n### Answer:"
tokenized = tokenizer(input_text, return_tensors="pt")
output = model.generate(
    tokenized.input_ids,
    max_length=200,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.2
)
response = tokenizer.decode(output[0], skip_special_tokens=True)
# Returns: Structured advice with skills, certs, salary
```

**RAG Fallback Path:**
```python
# Retrieval Process
query_embedding = embeddings.embed_query(user_question)
relevant_docs = faiss_index.similarity_search(query_embedding, k=4)
context = "\n\n".join([doc.page_content for doc in relevant_docs])

# Generation
prompt = f"Context: {context}\n\nQuestion: {user_question}\n\nAnswer:"
response = gemini_llm.invoke(prompt)
# Returns: Context-aware answer from career guides
```

---

### 3. RAG Coach - Document Intelligence

**Complete Workflow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 1: Upload & Detection            ‚îÇ
‚îÇ ‚îú‚îÄ User uploads resume.pdf + jd.pdf    ‚îÇ
‚îÇ ‚îú‚îÄ Content analysis (keywords)          ‚îÇ
‚îÇ ‚îÇ   Resume: "experience", "education"   ‚îÇ
‚îÇ ‚îÇ   JD: "requirements", "qualifications"‚îÇ
‚îÇ ‚îî‚îÄ Tag metadata: doc_type, source      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 2: Chunking & Embedding          ‚îÇ
‚îÇ ‚îú‚îÄ RecursiveCharacterTextSplitter      ‚îÇ
‚îÇ ‚îÇ   - chunk_size: 500                  ‚îÇ
‚îÇ ‚îÇ   - overlap: 50                      ‚îÇ
‚îÇ ‚îú‚îÄ all-MiniLM-L6-v2 embeddings         ‚îÇ
‚îÇ ‚îî‚îÄ 384-dim vectors                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 3: FAISS Indexing                ‚îÇ
‚îÇ ‚îú‚îÄ IndexFlatL2 (exact search)          ‚îÇ
‚îÇ ‚îú‚îÄ Store metadata with each chunk      ‚îÇ
‚îÇ ‚îî‚îÄ Persistent storage (disk)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 4: Auto Skill Analysis           ‚îÇ
‚îÇ ‚îú‚îÄ Extract skills from both docs       ‚îÇ
‚îÇ ‚îú‚îÄ Normalize (React.js ‚Üí react)        ‚îÇ
‚îÇ ‚îú‚îÄ Set operations (JD - Resume)        ‚îÇ
‚îÇ ‚îî‚îÄ Generate bullet points              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 5: Interactive Q&A               ‚îÇ
‚îÇ ‚îú‚îÄ User query embedding                 ‚îÇ
‚îÇ ‚îú‚îÄ FAISS similarity search (top-4)     ‚îÇ
‚îÇ ‚îú‚îÄ Filter by doc_type (if needed)      ‚îÇ
‚îÇ ‚îú‚îÄ TinyLLama generation                ‚îÇ
‚îÇ ‚îî‚îÄ Return answer + sources             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Skill Normalization (Key Innovation):**
```python
# 50+ synonym mappings
synonym_map = {
    'react.js': 'react',
    'reactjs': 'react',
    'node.js': 'nodejs',
    'postgresql': 'postgres',
    'sqlite3': 'sqlite',
    'restful api': 'rest api',
    'ci/cd': 'cicd',
    'oop': 'object-oriented programming',
    # ... 40+ more
}

# Before: 24 "missing" skills (false positives)
# After: 4 actual missing skills
# Accuracy improvement: 83% reduction in false positives
```

**Query Intent Detection:**
```python
# Filter context by question type
if "job description" in query.lower():
    filter_docs(doc_type="JOB_DESCRIPTION")
elif "my resume" in query.lower():
    filter_docs(doc_type="RESUME")
else:
    use_all_docs()
```

---

## Installation

### Quick Start (5 Minutes)

```bash
# 1. Clone repository
git clone https://github.com/arjuntanil/NextStep-AI.git
cd NextStep-AI

# 2. Create virtual environment
python -m venv career_coach
career_coach\Scripts\activate  # Windows
# source career_coach/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env: Add GOOGLE_API_KEY, JWT_SECRET_KEY

# 5. Train models
python model_training.py

# 6. Build RAG indexes
python ingest_guides.py

# 7. Install Ollama (for RAG Coach)
# Download from https://ollama.ai
ollama pull tinyllama

# 8. Create admin user (optional)
python create_admin.py
# Default credentials: admin@gmail.com / admin

# 9. Start backend
python -m uvicorn backend_api:app --reload

# 10. Start frontend (new terminal)
# Option A: React (modern UI with Aurora)
cd frontend && npm install && npm start

# Option B: Streamlit (simple UI - legacy)
streamlit run app.py
```

### React Frontend Setup

```bash
cd frontend

# Install dependencies (~1-2 minutes, 1406 packages)
npm install

# Start development server
npm run dev

# Access at: http://localhost:3000
```

**React Frontend Features:**
- ‚òÄÔ∏è Wozber-style light theme with clean white cards
- ‚ö° Aurora WebGL background (GPU-accelerated, purple/blue/green)
- üîê JWT authentication with protected routes
- üì± Mobile-responsive Material-UI components
- üé® Indigo/Cyan color scheme with Poppins font
- üìä Admin dashboard with comprehensive visualizations
- ‚ú® Premium markdown-style AI response formatting

---

### Environment Variables

**.env Configuration:**
```env
# Required
GOOGLE_API_KEY=AIzaSy...  # Get from https://makersuite.google.com/app/apikey
JWT_SECRET_KEY=<64-char-hex>  # python -c "import secrets; print(secrets.token_hex(32))"

# Optional (for Google OAuth login)
GOOGLE_CLIENT_ID=your_google_oauth_client_id
GOOGLE_CLIENT_SECRET=your_google_oauth_client_secret

# Frontend URL
STREAMLIT_FRONTEND_URL=http://localhost:8501
```

---

### Fine-Tune Career Advisor (Optional)

**Option 1: Local Training (GPU Recommended)**
```bash
python production_finetuning_optimized.py
# Time: 15-20 min (GPU) / 6+ hours (CPU)
# Output: career-advisor-final/ directory (1.5GB)
```

**Option 2: Skip Training (Use RAG Only)**
The system will automatically fall back to the RAG system if the fine-tuned model is not found. This is perfectly functional for most use cases.

---

## Usage

### Access URLs

| Service | URL | Description |
|---------|-----|-------------|
| **React Frontend** | http://localhost:3000 | Modern UI with Aurora |
| **Streamlit Frontend** | http://localhost:8501 | Data-centric UI |
| **Backend API** | http://localhost:8000 | FastAPI server |
| **API Docs** | http://localhost:8000/docs | Swagger UI |

### Using CV Analyzer

1. Navigate to "CV Analyzer" page
2. Upload resume (PDF/DOCX)
3. Wait 8-12 seconds for analysis
4. Review:
   - Recommended job title
   - Match percentage
   - Skills to learn (with YouTube links)
   - Live LinkedIn jobs
   - ATS feedback

### Using Resume Analyzer with JD

1. Navigate to "Resume Analyzer (with Job Description)" page
2. Upload both resume and job description (PDF/DOCX)
3. Wait 10-15 seconds for analysis
4. Review:
   - ATS compatibility score with gradient bar
   - Matching skills (green cards)
   - Missing skills with YouTube links (red cards)
   - ATS optimization tips (blue card)
   - All detected skills (purple card)

### Using AI Career Advisor

1. Navigate to "AI Career Advisor" page
2. Ask question (e.g., "Tell me about Data Science")
3. Adjust temperature (0.1-1.0) and length (50-120)
4. Click "Get AI Advice"
5. Review premium-formatted response with:
   - Headers (###, ####)
   - Bullet points with custom markers
   - Bold/italic text
   - Purple accent colors

### Using RAG Coach

1. Navigate to "RAG Coach" page
2. Upload resume.pdf + job_description.pdf
3. Wait for auto-analysis (~5-10 seconds)
4. Review:
   - Skills to add (normalized)
   - Resume bullet points
   - ATS keywords
5. Ask follow-up questions with context-aware answers

### Using Admin Dashboard (Admin Only)

1. Login with admin credentials (admin@gmail.com / admin)
2. Click "Features" ‚Üí "Admin Panel"
3. View comprehensive analytics:
   - KPI cards (users, analyses, retention)
   - User growth chart (30 days)
   - Top jobs and missing skills bar charts
   - Match score distribution pie chart
   - Recent activity feed
   - Retention metrics display
4. Manage users (view, suspend, activate, delete)

---

## API Reference

### Authentication

```bash
# Initiate Google OAuth
GET /auth/login

# OAuth callback
GET /auth/callback

# Get current user
GET /users/me
Authorization: Bearer <JWT_TOKEN>
```

### Resume Analysis

```bash
POST /analyze_resume/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume/ \
  -F "file=@resume.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "recommended_job_title": "Full Stack Developer",
  "required_skills": ["python", "django", "react", "docker"],
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "match_percentage": 75.0,
  "live_jobs": [{"title": "...", "company": "...", "link": "..."}],
  "layout_feedback": "‚úÖ Add professional summary..."
}
```

### AI Career Advisor

```bash
POST /query-career-path/
Content-Type: application/json

{
  "text": "Tell me about DevOps",
  "max_length": 200,
  "temperature": 0.7
}
```

### RAG Coach

```bash
# Upload PDFs
POST /rag-coach/upload
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/rag-coach/upload \
  -F "files=@resume.pdf" \
  -F "files=@job_description.pdf" \
  -F "process_resume_job=true"

# Query
POST /rag-coach/query
Content-Type: application/json

{
  "question": "What skills should I add?",
  "show_context": true
}
```

**Response:**
```json
{
  "answer": "Based on the job description, you should focus on...",
  "context_chunks": [
    {"content": "...", "source": "jd.pdf", "doc_type": "JOB_DESCRIPTION"}
  ],
  "sources": ["resume.pdf", "job_description.pdf"]
}
```

### Resume + JD Analysis

```bash
POST /analyze_resume_with_jd/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume_with_jd/ \
  -F "resume=@resume.pdf" \
  -F "job_description=@jd.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "jd_skills": ["python", "django", "react", "docker", "kubernetes"],
  "matching_skills": ["python", "django", "react"],
  "missing_skills": ["docker", "kubernetes"],
  "ats_score": 60.0,
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "layout_feedback": "‚úÖ Skills section is ATS-friendly..."
}
```

### History

```bash
# Resume analyses
GET /history/analyses
Authorization: Bearer TOKEN

# Career queries
GET /history/queries
Authorization: Bearer TOKEN

# RAG interactions
GET /history/rag-queries
Authorization: Bearer TOKEN
```

### Admin Dashboard

```bash
# Get comprehensive statistics (admin only)
GET /admin/stats
Authorization: Bearer ADMIN_TOKEN

# Get all users with pagination
GET /admin/users?skip=0&limit=50
Authorization: Bearer ADMIN_TOKEN

# Suspend user
PUT /admin/user/{user_id}/suspend
Authorization: Bearer ADMIN_TOKEN

# Activate user
PUT /admin/user/{user_id}/activate
Authorization: Bearer ADMIN_TOKEN

# Delete user
DELETE /admin/user/{user_id}
Authorization: Bearer ADMIN_TOKEN
```

**Admin Stats Response:**
```json
{
  "total_users": 150,
  "active_users_30days": 85,
  "active_users_7days": 42,
  "new_users_7days": 12,
  "total_analyses": 450,
  "analyses_7days": 67,
  "avg_match_percentage": 72.3,
  "retention_rate": 0.567,
  "retention_7days": 0.714,
  "retention_30days": 0.589,
  "user_growth": [{"date": "2024-11-01", "count": 138}, ...],
  "top_jobs": [{"job": "Software Developer", "count": 89}, ...],
  "top_missing_skills": [{"skill": "docker", "count": 45}, ...],
  "match_distribution": [65.2, 78.5, 82.1, ...],
  "recent_activity": [{"type": "analysis", "user": "user@email.com", ...}],
  "activity_heatmap": [{"day": "Monday", "hour": 14, "count": 23}, ...]
}
```

---

## Deployment

### Local Production Deployment

**Running on Windows Server/VPS:**
```bash
# 1. Install Python 3.10+ and Git
# 2. Clone and setup (same as installation steps)
# 3. Create Windows Service or use Task Scheduler for auto-start

# Run backend as background process
start /B python -m uvicorn backend_api:app --host 0.0.0.0 --port 8000

# Or use a process manager like PM2
npm install -g pm2
pm2 start "uvicorn backend_api:app --host 0.0.0.0 --port 8000" --name nextstepai-backend
```

**Linux/Mac Deployment with systemd:**
```bash
# Create service file: /etc/systemd/system/nextstepai.service
[Unit]
Description=NextStepAI Backend
After=network.target

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/NextStepAI
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/uvicorn backend_api:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target

# Enable and start
sudo systemctl enable nextstepai
sudo systemctl start nextstepai
```

### Database Management

**SQLite Database Location:**
```
nextstepai.db (created automatically in project root)
```

**Backup SQLite Database:**
```bash
# Create backup
copy nextstepai.db nextstepai_backup_2024-10-29.db

# Or use SQLite command
sqlite3 nextstepai.db ".backup nextstepai_backup.db"
```

**Database Schema:**
```python
# Tables created by SQLAlchemy (models.py):
- users
  * id, email, full_name, password_hash
  * role (user/admin), is_active, created_at, last_active
  
- resume_analyses
  * id, owner_id, recommended_job_title, match_percentage
  * skills_to_add, resume_filename, total_skills_count, created_at
  
- career_queries
  * id, owner_id, user_query_text, matched_job_group
  * model_used (finetuned/rag), response_time_seconds, created_at
  
- rag_coach_queries
  * id, owner_id, question, answer, sources
  * query_length, answer_length, created_at
```

### Production Checklist

- ‚úÖ Set strong JWT_SECRET_KEY (64 chars)
- ‚úÖ Backup SQLite database regularly
- ‚úÖ Enable HTTPS with reverse proxy (nginx/Apache)
- ‚úÖ Configure CORS for production domain
- ‚úÖ Set up rate limiting (10 req/min)
- ‚úÖ Enable structured logging
- ‚úÖ Use environment variables (no hardcoded secrets)
- ‚úÖ Monitor disk space (SQLite database growth)
- ‚úÖ Set up automatic backups (daily/weekly)
- ‚úÖ Use process manager (PM2/systemd) for auto-restart

---

## Troubleshooting

### Backend Issues

**Port 8000 in use:**
```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8000 | xargs kill -9
```

**Model loading hangs:**
```bash
# Wait 2-5 minutes or disable fine-tuned model
DISABLE_FINETUNED_MODEL_LOAD=1
```

### RAG Coach Issues

**Ollama model not found:**
```bash
ollama pull tinyllama
ollama list  # Verify
```

**No documents in vector store:**
```bash
curl -X POST http://localhost:8000/rag-coach/build-index
```

### Authentication Issues

**OAuth redirect mismatch:**
- Update Google Console redirect URI
- Must match: `http://localhost:8000/auth/callback`

**JWT token expired:**
- Logout and login again

### Performance Issues

**Slow responses (20+ seconds):**
- Reduce temperature (0.7 ‚Üí 0.3)
- Reduce max_length (200 ‚Üí 80)
- Use GPU instead of CPU

**Out of memory:**
- Close other applications
- Reduce chunk size in RAG
- Use quantized models

---

## Project Structure

```
NextStepAI/
‚îú‚îÄ‚îÄ backend_api.py              # Main FastAPI application (2897 lines)
‚îú‚îÄ‚îÄ models.py                   # SQLAlchemy database models
‚îú‚îÄ‚îÄ model_training.py           # ML model training script (Naive Bayes)
‚îú‚îÄ‚îÄ production_finetuning_optimized.py  # GPT-2 fine-tuning script
‚îú‚îÄ‚îÄ create_admin.py             # Admin user creation utility
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies (191 packages)
‚îú‚îÄ‚îÄ nextstepai.db              # SQLite database (auto-created)
‚îÇ
‚îú‚îÄ‚îÄ Data Files/
‚îÇ   ‚îú‚îÄ‚îÄ jobs_cleaned.csv           # 8000+ job-skill mappings
‚îÇ   ‚îú‚îÄ‚îÄ skills_db.json             # 10,000+ validated skills
‚îÇ   ‚îú‚îÄ‚îÄ career_advice_dataset.jsonl     # 749 career Q&A pairs
‚îÇ   ‚îú‚îÄ‚îÄ youtube_links.json         # Skill ‚Üí YouTube tutorial mapping
‚îÇ   ‚îú‚îÄ‚îÄ job_postings_new.json      # Scraped job postings
‚îÇ   ‚îî‚îÄ‚îÄ career_guides.json         # Career advice guides for RAG
‚îÇ
‚îú‚îÄ‚îÄ Model Artifacts/
‚îÇ   ‚îú‚îÄ‚îÄ job_recommender_pipeline.joblib   # TF-IDF + Naive Bayes
‚îÇ   ‚îú‚îÄ‚îÄ job_title_encoder.joblib          # Label encoder
‚îÇ   ‚îú‚îÄ‚îÄ prioritized_skills.joblib         # Job ‚Üí top skills mapping
‚îÇ   ‚îú‚îÄ‚îÄ master_skill_vocab.joblib         # Complete skill vocabulary
‚îÇ   ‚îî‚îÄ‚îÄ career-advisor-final/             # Fine-tuned GPT-2 (1.5GB)
‚îÇ       ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îî‚îÄ‚îÄ tokenizer files
‚îÇ
‚îú‚îÄ‚îÄ Vector Stores/
‚îÇ   ‚îú‚îÄ‚îÄ rag_coach_index/           # FAISS index for uploaded docs
‚îÇ   ‚îú‚îÄ‚îÄ jobs_index/                # Job postings embeddings
‚îÇ   ‚îî‚îÄ‚îÄ guides_index/              # Career guides embeddings
‚îÇ
‚îú‚îÄ‚îÄ frontend/                      # React 18 application
‚îÇ   ‚îú‚îÄ‚îÄ package.json               # 13 dependencies
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.js                 # Router configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.css              # Global styles (Wozber theme)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ theme.js               # Material-UI theme
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Layout.js          # Header + navigation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Aurora.js          # WebGL background
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProtectedRoute.js  # Auth guard
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AuthContext.js     # JWT state management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.js       # Home page
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CVAnalyzer.js      # Resume analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ResumeAnalyzer.js  # Resume + JD matching
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CareerAdvisor.js   # AI chat
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RAGCoach.js        # Document Q&A
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AdminDashboard.js  # Analytics (600+ lines)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ History.js         # User history
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Login.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Register.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ api.js             # Axios API client
‚îÇ   ‚îî‚îÄ‚îÄ build/                     # Production build
‚îÇ
‚îî‚îÄ‚îÄ uploads/
    ‚îî‚îÄ‚îÄ processed/                 # Temporary uploaded files

Key Files Overview:
- backend_api.py: 22 endpoints (auth, CV analysis, RAG, admin)
- models.py: 4 database tables with relationships
- model_training.py: GridSearchCV, confusion matrix, artifact generation
- AdminDashboard.js: 8+ visualizations with Recharts
```

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

**Code Style:**
- Follow PEP 8 for Python
- Use type hints
- Add docstrings
- Keep lines <100 characters

---

## License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file.

---

## Acknowledgments

**Technologies:**
- [HuggingFace Transformers](https://huggingface.co/) - LLM infrastructure
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [React](https://reactjs.org/) - Frontend library
- [Material-UI](https://mui.com/) - UI components
- [OGL](https://github.com/oframe/ogl) - WebGL library (Aurora)
- [LangChain](https://python.langchain.com/) - RAG orchestration
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [Ollama](https://ollama.ai/) - Local LLM inference

**Data:**
- Google Gemini - Skill extraction
- all-MiniLM-L6-v2 - Sentence embeddings
- GPT-2-Medium - Base model
- Scikit-learn - ML utilities

---

## Contact

**Author:** Arjun T Anil  
**GitHub:** [@arjuntanil](https://github.com/arjuntanil)  
**Repository:** [NextStep-AI](https://github.com/arjuntanil/NextStep-AI)  

**Support:**
- üêõ [Report Bugs](https://github.com/arjuntanil/NextStep-AI/issues)
- üí° [Request Features](https://github.com/arjuntanil/NextStep-AI/discussions)

---

<div align="center">

**‚≠ê Star this project if you find it helpful! ‚≠ê**

Made with ‚ù§Ô∏è by [Arjun T Anil](https://github.com/arjuntanil)

</div>
