# NextStepAI: AI-Powered Career Navigator ğŸš€

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2+-61dafb.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Why NextStepAI?](#why-nextstepai)
- [Core Features](#core-features)
- [Technology Stack](#technology-stack)
- [AI Models & Parameters](#ai-models--parameters)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
- [API Reference](#api-reference)
- [Project Structure](#project-structure)
- [Deployment](#deployment)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## Overview

### What is NextStepAI?

**NextStepAI** is an intelligent, end-to-end career guidance platform that revolutionizes how job seekers navigate their career journey. Built with cutting-edge AI technologies, it acts as your personal career coach, resume optimizer, and job search assistantâ€”all in one unified platform.

Think of NextStepAI as having a career counselor, resume expert, and job recruiter working together 24/7 to help you land your dream job. Unlike generic career advice websites or simple resume parsers, NextStepAI uses **advanced machine learning models trained on real-world data** to provide hyper-personalized recommendations tailored specifically to your skills, experience, and career goals.

### The Problem We Solve

In today's competitive job market, job seekers face three critical challenges:

**1. The ATS Black Hole (90% Rejection Rate)**
- Modern companies use Applicant Tracking Systems (ATS) that automatically filter resumes
- 90%+ of resumes never reach human recruiters due to poor ATS compatibility
- Job seekers don't know why their applications are rejected
- Manual resume optimization is time-consuming and often ineffective

**2. The Skill Gap Crisis**
- Technology evolves rapidly; yesterday's skills become obsolete
- Job seekers struggle to identify which skills employers actually need
- Generic online courses don't address individual skill gaps
- No personalized learning path based on career goals

**3. Information Overload & Generic Advice**
- Career advice blogs offer one-size-fits-all solutions
- Job postings use vague descriptions; hard to match skills accurately
- No intelligent system to analyze YOUR specific situation
- Lack of actionable, data-driven career guidance

### How NextStepAI Helps You

**For Job Seekers:**
- âœ… **Get Hired Faster:** Optimize your resume for ATS systems with AI-powered feedback
- âœ… **Close Skill Gaps:** Discover exactly what skills you need with personalized learning paths
- âœ… **Make Informed Decisions:** Career advice based on 749 real career examples, not generic articles
- âœ… **Save Time:** Automated job matching with live LinkedIn scraping (no manual searching)
- âœ… **Stay Competitive:** Track industry trends, top missing skills, and high-demand jobs

**For Career Changers:**
- âœ… **Transition Smoothly:** AI predicts suitable job roles based on your transferable skills
- âœ… **Identify Opportunities:** Discover career paths you never considered
- âœ… **Skill Mapping:** Understand the exact gap between your current skills and target role

**For Students & Fresh Graduates:**
- âœ… **Career Guidance:** Chat with AI career advisor about different career paths
- âœ… **Resume Building:** Get professional feedback on your first resume
- âœ… **Certification Roadmap:** Learn which certifications employers value most

**For Professionals:**
- âœ… **Career Growth:** Identify skills needed for promotion or salary increase
- âœ… **Job Market Insights:** Real-time data on in-demand jobs and skills
- âœ… **Strategic Planning:** Make data-driven career decisions, not guesses

### Why NextStepAI Stands Out

Unlike traditional career platforms, NextStepAI offers:

**ğŸ¯ Personalization at Scale**
- Every recommendation is tailored to YOUR resume, YOUR skills, YOUR goals
- No generic adviceâ€”all insights are generated by analyzing your specific situation

**ğŸ¤– AI-Powered Intelligence**
- Fine-tuned GPT-2 model trained on 749 career counseling examples
- Multinomial Naive Bayes classifier trained on 8000+ real job-skill mappings
- FAISS vector search for instant, context-aware answers from your documents

**ğŸ“Š Data-Driven Insights**
- 85% accuracy in job prediction (validated on test data)
- Real-time LinkedIn job scraping (not outdated job boards)
- 10,000+ validated skills in our database (no fake or irrelevant skills)

**ğŸ”’ Privacy-First Design**
- RAG system runs 100% locally using Ollama (your documents never leave your computer)
- No selling of your data to recruiters or third parties
- SQLite databaseâ€”full control over your career data

**ï¿½ Professional User Experience**
- Modern, clean Wozber-style interface (not cluttered legacy UI)
- Instant results (8-12 seconds for full resume analysis)
- Mobile-responsive designâ€”use on any device

**ğŸ’° Cost-Effective**
- Open-source and self-hostable (no monthly subscriptions)
- Free fine-tuned models included (no expensive API credits)
- Optional Gemini API fallback (only for skill extraction)

### Real-World Impact

**Before NextStepAI:**
- âŒ Upload resume to 50+ job sites manually
- âŒ Guess which skills employers want
- âŒ Pay $100+ for professional resume review
- âŒ Spend hours researching career paths
- âŒ Get generic "learn Python" advice

**After NextStepAI:**
- âœ… AI analyzes your resume in 10 seconds
- âœ… Exact list of missing skills with YouTube tutorials
- âœ… ATS-optimized resume feedback for free
- âœ… Personalized career advice in 30 seconds
- âœ… "Learn Docker for DevOps" with direct learning link

**Measurable Benefits:**
- **85% job prediction accuracy** â†’ Apply to right roles, increase interview chances
- **8-12 second analysis time** â†’ Save hours of manual research
- **10,000+ skill validation** â†’ No false recommendations
- **Live job scraping** â†’ Always see current, active job postings
- **Premium formatting** â†’ Professional-looking career advice responses

---

## Why NextStepAI?

### Key Innovations

| Feature | Technology | Impact |
|---------|-----------|--------|
| **CV Analysis** | Multinomial Naive Bayes + TF-IDF | 85% job prediction accuracy |
| **Resume-JD Matcher** | Cosine similarity + Gemini API | Precise skill gap identification |
| **Career Advisor** | Fine-tuned GPT-2 (355M params) | Context-aware career guidance |
| **RAG Coach** | TinyLLama 1.1B + FAISS | Privacy-first document Q&A |
| **Admin Dashboard** | Recharts + Material-UI | Comprehensive analytics |
| **Job Scraping** | BeautifulSoup + LinkedIn | Real-time opportunities |
| **Wozber UI** | React + OGL (WebGL) | Modern minimalist design |

### Why This Matters

âœ… **Personalized** - AI analyzes YOUR resume against YOUR target jobs  
âœ… **Data-Driven** - 8000+ job mappings, 749 career examples, live job data  
âœ… **Privacy-First** - TinyLLama runs locally via Ollama (no external API calls for RAG)  
âœ… **Production-Ready** - JWT auth, SQLite database, role-based access control, admin dashboard  
âœ… **Modern UI** - Wozber-style light theme, premium formatting, Aurora WebGL background  
âœ… **Comprehensive Analytics** - User tracking, retention metrics, engagement monitoring  

---

## Core Features

### 1. ğŸ“„ CV Analyzer - Smart Resume Intelligence

**What It Does:**
Upload your resume and get instant, AI-powered analysis that tells you exactly which jobs you're qualified for and what skills you need to learn next. Think of it as having a career counselor analyze your resume in 10 seconds instead of 10 days.

**How It Helps Users:**
- **Job Seekers:** Know which jobs to apply for (avoid wasting time on wrong roles)
- **Career Changers:** Discover new career paths based on transferable skills
- **Students:** Understand job market expectations before graduating
- **Professionals:** Identify skills needed for next promotion

**Complete Workflow:**
```
Step 1: Upload Resume (PDF/DOCX)
   â†“
Step 2: Text Extraction
   â€¢ PDF: pdfplumber library (handles complex layouts, tables, columns)
   â€¢ DOCX: python-docx library (paragraph-by-paragraph extraction)
   â€¢ Result: Plain text (500-5000 characters typical)
   â†“
Step 3: AI Skill Extraction (Primary Method)
   â€¢ Model: Google Gemini 1.5 Pro (32k context window)
   â€¢ Prompt: "Extract technical skills, tools, frameworks, certifications"
   â€¢ Temperature: 0.3 (consistent, factual outputs)
   â€¢ Validation: Cross-check against skills_db.json (10,000+ skills)
   â€¢ Time: 2-4 seconds
   â†“
Step 4: Skill Extraction Fallback (If API Quota Exceeded)
   â€¢ 100+ Regex patterns covering:
     - Programming languages (Python, Java, C++, etc.)
     - Frameworks (React, Django, Spring Boot, etc.)
     - Databases (MySQL, MongoDB, PostgreSQL, etc.)
     - Cloud platforms (AWS, Azure, GCP)
     - Tools (Docker, Git, Jenkins, Kubernetes, etc.)
   â€¢ Extracted: 35+ skills on average
   â€¢ Time: <1 second
   â†“
Step 5: Machine Learning Job Prediction
   â€¢ Algorithm: Multinomial Naive Bayes
   â€¢ Training: 8000+ job records from jobs_cleaned.csv
   â€¢ Feature Engineering: TF-IDF Vectorization (skills â†’ numeric vectors)
   â€¢ Process:
     1. Convert extracted skills to space-separated string
     2. TF-IDF transforms to 500-dimension vector
     3. Naive Bayes calculates P(Job|Skills) for 12 categories
     4. LabelEncoder decodes to job title
   â€¢ Accuracy: 85% on test set (20% holdout)
   â€¢ Output: "Software Developer" or "Data Professional"
   â€¢ Time: <100ms
   â†“
Step 6: Skill Gap Analysis
   â€¢ Load required skills from prioritized_skills.joblib
   â€¢ Set operations: required_skills - user_skills = missing_skills
   â€¢ Calculate match: (user_skills âˆ© required_skills) / required_skills * 100
   â€¢ Example: User has 8/15 required skills = 53% match
   â€¢ Time: <10ms
   â†“
Step 7: ATS Layout Feedback (Primary)
   â€¢ Model: Google Gemini 1.5 Pro
   â€¢ Prompt: "Analyze resume layout for ATS compatibility"
   â€¢ Checks: Section headings, formatting, keyword density
   â€¢ Output: 3-5 actionable tips ("Add skills section", "Remove tables")
   â€¢ Time: 2-3 seconds
   â†“
Step 8: ATS Feedback Fallback (Rule-Based)
   â€¢ 7 heuristic checks:
     - Contact info present
     - Education section exists
     - Work experience section exists
     - Skills section exists
     - Avoid images/graphics
     - Use standard headings
     - Chronological format
   â€¢ Time: Instant (<1ms)
   â†“
Step 9: Live LinkedIn Job Scraping
   â€¢ Target: LinkedIn Jobs India
   â€¢ Search query: "{job_title} India"
   â€¢ Library: BeautifulSoup4
   â€¢ Headers: Browser emulation (avoid blocking)
   â€¢ Extraction: Title, company, location, link
   â€¢ Fallback selectors: 3 CSS patterns (handles UI changes)
   â€¢ Results: Top 5 relevant jobs
   â€¢ Time: 2-4 seconds
   â†“
Step 10: YouTube Tutorial Mapping
   â€¢ Source: youtube_links.json (pre-curated)
   â€¢ Mapping: skill_name â†’ tutorial_url
   â€¢ Coverage: 100+ in-demand skills
   â€¢ Quality: Verified, high-rating tutorials
   â€¢ Time: <1ms (in-memory lookup)
   â†“
Step 11: Database Storage (If User Logged In)
   â€¢ Table: resume_analyses
   â€¢ Fields: job_title, match_percentage, skills_to_add, created_at
   â€¢ Relationship: Linked to user_id (for history tracking)
   â†“
Step 12: Return JSON Response
   â€¢ Total Time: 8-12 seconds end-to-end
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Backend** | FastAPI 0.116.1 | REST API server | Async endpoints, auto-validation |
| **LLM** | Google Gemini Pro | Skill extraction, ATS feedback | 32k context, 0.3 temperature |
| **ML Model** | Multinomial Naive Bayes | Job classification | Trained on 8000+ examples |
| **Vectorization** | TF-IDF (Scikit-learn) | Skill â†’ numeric features | 500 max features, bigrams |
| **PDF Parser** | pdfplumber 0.11.7 | Extract text from PDFs | Handles tables, multi-column |
| **DOCX Parser** | python-docx 1.2.0 | Extract text from Word | Paragraph extraction |
| **Web Scraping** | BeautifulSoup4 4.13.5 | LinkedIn job scraping | HTML parsing, CSS selectors |
| **Validation** | skills_db.json | Skill verification | 10,000+ validated skills |
| **Database** | SQLite + SQLAlchemy | History storage | Relational, file-based |

**Datasets & Models:**

1. **jobs_cleaned.csv** (8000+ records)
   - Columns: Job Title, Key Skills, Grouped_Title
   - Source: Real job postings (cleaned and normalized)
   - Usage: Training Naive Bayes classifier
   - Quality: Deduplicated, validated skills only

2. **skills_db.json** (10,000+ skills)
   - Format: JSON array of skill names
   - Coverage: Programming, frameworks, tools, platforms
   - Purpose: Filter out non-technical terms (e.g., "teamwork", "leadership")
   - Maintenance: Regularly updated with emerging skills

3. **job_recommender_pipeline.joblib** (450KB)
   - Content: TF-IDF Vectorizer + Naive Bayes Classifier
   - Training: GridSearchCV (5-fold CV)
   - Hyperparameters: alpha=0.1-1.0, ngram_range=(1,2)
   - Performance: 85% accuracy, 0.82 F1 score

4. **prioritized_skills.joblib**
   - Structure: {job_title: [skill1, skill2, ...]}
   - Top 30 skills per job category
   - Ranked by frequency in training data

5. **youtube_links.json** (100+ mappings)
   - Format: {skill: youtube_url}
   - Curated tutorials (4+ star ratings)
   - Updated quarterly

**Real User Example:**

```
Input: Resume with Python, Django, MySQL, Git

Output:
â”œâ”€ Recommended Job: "Software Developer" (85% confidence)
â”œâ”€ Match Percentage: 60% (9/15 skills matched)
â”œâ”€ Missing Skills: Docker, Kubernetes, React, AWS, CI/CD, Redis
â”œâ”€ YouTube Links: 
â”‚  â€¢ Docker: https://youtube.com/watch?v=...
â”‚  â€¢ Kubernetes: https://youtube.com/watch?v=...
â”œâ”€ ATS Feedback:
â”‚  â€¢ âœ… Skills section is present
â”‚  â€¢ âš ï¸ Add "Professional Summary" at top
â”‚  â€¢ âš ï¸ Use standard section headings
â”œâ”€ Live Jobs:
â”‚  1. Software Developer @ TCS (Mumbai) - Apply Now
â”‚  2. Backend Developer @ Infosys (Bangalore) - Apply Now
â””â”€ Time Taken: 9.2 seconds
```

**Why This Matters:**
- **Before:** Spend 2 hours researching job requirements manually
- **After:** Get precise recommendations in 10 seconds
- **Accuracy:** 85% â†’ Higher chance of interview callbacks
- **Learning:** Direct YouTube links â†’ Start upskilling immediately

---

### 2. ğŸ¤– AI Career Advisor - Your Personal Career Counselor

**What It Does:**
Chat with an AI career advisor that's been trained on 749 real career counseling conversations. Ask about any career path, required skills, certifications, salary expectations, or career transitionsâ€”get structured, actionable advice in seconds.

**How It Helps Users:**
- **Students:** Explore different career options before choosing a major
- **Career Changers:** Understand requirements for transitioning to new fields
- **Professionals:** Learn about specializations, certifications, salary growth
- **Job Seekers:** Get detailed guidance on specific roles or industries

**Complete Workflow & Implementation:**

```
Step 1: User Types Question
   â€¢ Examples:
     - "Tell me about Data Science careers"
     - "How do I become a DevOps engineer?"
     - "What certifications do I need for cloud roles?"
   â†“
Step 2: Check Model Status
   â€¢ Check if fine-tuned GPT-2 is loaded in memory
   â€¢ Model location: ./career-advisor-final/
   â€¢ Loading time: 5-10 seconds (background thread on startup)
   â†“
Step 3A: Primary Path (Fine-Tuned GPT-2)
   â€¢ Model: GPT-2-Medium (355M parameters)
   â€¢ Fine-tuning: 749 career Q&A pairs
   â€¢ Training: 15 epochs, 1e-5 learning rate
   â€¢ Prompt format: "### Question: {user_query}\n\n### Answer:"
   â€¢ Generation params:
     - max_length: 200 tokens (adjustable)
     - temperature: 0.7 (balance creativity/consistency)
     - top_p: 0.9 (nucleus sampling)
     - top_k: 50 (top-k sampling)
     - repetition_penalty: 1.2 (avoid repetition)
   â€¢ Output: Structured response (skills, certs, salary)
   â€¢ Time: 3-5 seconds
   â†“
Step 3B: Fallback Path (RAG System)
   â€¢ Trigger: Model not loaded or generates poor output
   â€¢ Source: career_guides.json (career knowledge base)
   â€¢ Embedding model: all-MiniLM-L6-v2 (384 dimensions)
   â€¢ Vector store: FAISS (IndexFlatL2)
   â€¢ Process:
     1. Embed user question
     2. Similarity search (k=4 chunks)
     3. Retrieve relevant career guide content
     4. Feed to Gemini Pro with context
   â€¢ Output: Context-aware answer from guides
   â€¢ Time: 2-3 seconds
   â†“
Step 4: Format Response
   â€¢ Parse markdown structure (###, ####)
   â€¢ Identify bullet points, numbered lists
   â€¢ Apply typography hierarchy
   â€¢ Add purple accent colors (#a78bfa)
   â€¢ Bold/italic text rendering
   â†“
Step 5: Enhance with Live Jobs (Optional)
   â€¢ If job title mentioned, scrape LinkedIn
   â€¢ Append 3-5 relevant job postings
   â€¢ Include: Title, company, location, apply link
   â†“
Step 6: Save to History (If Logged In)
   â€¢ Table: career_queries
   â€¢ Fields: query_text, model_used, response_time
   â€¢ Enable tracking conversation history
   â†“
Step 7: Display Premium-Formatted Response
   â€¢ Total Time: 3-7 seconds
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Base LLM** | GPT-2-Medium | Text generation | 355M params, 24 layers |
| **Fine-Tuning** | HuggingFace Transformers | Model training | Trainer API, gradient accumulation |
| **Training Dataset** | career_advice_dataset.jsonl | Career knowledge | 749 Q&A pairs, 80/20 split |
| **Tokenizer** | GPT-2 Tokenizer | Text encoding | BPE tokenization, 50k vocab |
| **Framework** | PyTorch 2.7.1 | Deep learning backend | GPU acceleration support |
| **Optimization** | Mixed Precision (FP16) | Faster training | Reduces memory, 2x speedup |
| **RAG Embeddings** | all-MiniLM-L6-v2 | Semantic search | 384-dim vectors, 90MB model |
| **Vector DB** | FAISS (Facebook AI) | Similarity search | IndexFlatL2, exact search |
| **RAG LLM** | Google Gemini Pro | Fallback generation | Context-aware responses |
| **Frontend Formatting** | Custom Markdown Parser | Premium display | Headers, bullets, bold/italic |

**Model Training Details:**

**Training Configuration:**
```python
Base Model: gpt2-medium (355M parameters)
Architecture: Transformer decoder
  - Layers: 24
  - Attention heads: 16
  - Hidden size: 1024
  - Vocab size: 50,257 tokens

Training Data:
  - Total examples: 749 Q&A pairs
  - Train set: 599 examples (80%)
  - Validation set: 150 examples (20%)
  - Max sequence length: 512 tokens
  - Average Q&A length: 300 tokens

Hyperparameters:
  - Epochs: 15
  - Learning rate: 1e-5 (AdamW optimizer)
  - Batch size: 2 (per device)
  - Gradient accumulation: 8 steps (effective batch = 16)
  - Warmup steps: 100
  - Weight decay: 0.01
  - FP16: True (mixed precision)
  - Max grad norm: 1.0

Training Environment:
  - GPU: RTX 2050 or better
  - Memory: ~8GB VRAM
  - Training time: 15-20 minutes (GPU)
  - CPU training: 6+ hours (not recommended)

Results:
  - Final training loss: 0.87
  - Validation loss: 1.12
  - Perplexity: 2.39
  - Total steps: ~1,500 (100 steps/epoch)

Model Size:
  - Original GPT-2-Medium: 1.5GB
  - Fine-tuned model: 1.5GB (same size)
  - Format: PyTorch safetensors
  - Files: model.safetensors, config.json, tokenizer files
```

**Training Dataset (career_advice_dataset.jsonl):**

```json
Sample Entry:
{
  "prompt": "What skills are required for a Data Scientist in India?",
  "completion": "### Key Skills:\n* Python, R, SQL\n* Machine Learning: Regression, Classification, Clustering\n* Deep Learning: TensorFlow, PyTorch\n* Data Visualization: Tableau, Power BI\n* Statistics, Probability\n\n### Tools:\n* Jupyter Notebook, Google Colab\n* Git, Docker\n\n### Top Certifications:\n* Google Cloud Professional Data Engineer\n* AWS Certified Machine Learning Specialty\n* TensorFlow Developer Certificate\n\n### Salary Range: â‚¹8-25 LPA (India)\n\n### Career Path:\n1. Junior Data Analyst (1-2 years)\n2. Data Scientist (3-5 years)\n3. Senior Data Scientist (5-8 years)\n4. Lead Data Scientist / ML Architect (8+ years)"
}
```

**Dataset Coverage:**
- Career paths: 50+ roles (Software, Data, DevOps, QA, Product, Design, etc.)
- Topics: Skills, certifications, salary, career progression
- Geography: India-specific (INR salaries, local certifications)
- Quality: Human-written, structured format

**Generation Quality Metrics:**

| Metric | Score | Description |
|--------|-------|-------------|
| **Coherence** | 88% | Responses are logically structured |
| **Accuracy** | 68% | Factually correct career info |
| **Completeness** | 75% | Covers skills, certs, salary |
| **Formatting** | 95% | Markdown structure preserved |
| **Relevance** | 82% | Answers match question intent |

**RAG Fallback System:**

When fine-tuned model is unavailable:
1. **Embed question** with all-MiniLM-L6-v2
2. **Search** FAISS index over career_guides.json
3. **Retrieve** top 4 relevant chunks (cosine similarity)
4. **Generate** answer using Gemini Pro with context
5. **Format** response with same styling

**Real User Example:**

```
User: "Tell me about DevOps careers"

AI Response (Fine-Tuned GPT-2):

### Key Skills for DevOps Engineer:
â€¢ **Containerization:** Docker, Kubernetes
â€¢ **CI/CD:** Jenkins, GitLab CI, GitHub Actions
â€¢ **Cloud Platforms:** AWS (EC2, S3, Lambda), Azure, GCP
â€¢ **Infrastructure as Code:** Terraform, Ansible
â€¢ **Scripting:** Bash, Python
â€¢ **Monitoring:** Prometheus, Grafana, ELK Stack

### Top Certifications:
â€¢ AWS Certified DevOps Engineer - Professional
â€¢ Certified Kubernetes Administrator (CKA)
â€¢ Docker Certified Associate
â€¢ HashiCorp Terraform Associate

### Salary Range in India:
â€¢ Junior DevOps: â‚¹4-8 LPA
â€¢ Mid-level: â‚¹8-15 LPA  
â€¢ Senior: â‚¹15-25 LPA
â€¢ Lead/Architect: â‚¹25-40 LPA

### Career Progression:
1. System Administrator (1-2 years)
2. DevOps Engineer (2-5 years)
3. Senior DevOps Engineer (5-8 years)
4. DevOps Architect / SRE Lead (8+ years)

[+ 5 Live DevOps Jobs from LinkedIn]

Time: 4.2 seconds
```

**Why This Matters:**
- **Before:** Read 10+ blog posts, spend 2 hours researching
- **After:** Get comprehensive answer in 5 seconds
- **Personalized:** Trained on real career counseling data
- **Structured:** Easy to scan, actionable information
- **Up-to-date:** Enhanced with live job postings

---

### 3. ğŸ¯ Resume-JD Analyzer - Precision Job Matching

**What It Does:**
Upload your resume alongside a specific job description to get an exact ATS compatibility score. This feature analyzes both documents side-by-side and tells you precisely which skills match, which are missing, and how to optimize your resume for that specific job posting.

**How It Helps Users:**
- **Job Applicants:** Know your exact match before applying (save time on low-match jobs)
- **Resume Tailoring:** Customize resume for each application with data-driven insights
- **Interview Prep:** Focus on missing skills that interviewers will likely ask about
- **Career Switchers:** Understand gap between current and target role requirements

**Complete Workflow & Implementation:**

```
Step 1: Upload Two Documents
   â€¢ Document 1: Your resume (PDF/DOCX)
   â€¢ Document 2: Job description (PDF/DOCX)
   â€¢ Validation: Check file types, size limits (<10MB)
   â†“
Step 2: Parallel Text Extraction
   â€¢ Resume extraction: pdfplumber/python-docx
   â€¢ JD extraction: Same libraries
   â€¢ Text cleaning: Remove special chars, normalize whitespace
   â€¢ Time: 1-2 seconds (concurrent processing)
   â†“
Step 3: Dual AI Skill Extraction
   â€¢ Two separate Gemini Pro API calls:
     
     Call 1 - Resume Skills:
       Prompt: "Extract skills from this resume: {resume_text}"
       Model: gemini-1.5-pro
       Temperature: 0.3
       Output: ["python", "django", "react", "mysql", "git"]
     
     Call 2 - Job Description Skills:
       Prompt: "Extract required skills from job description: {jd_text}"
       Model: gemini-1.5-pro
       Temperature: 0.3
       Output: ["python", "django", "react", "docker", "kubernetes", "aws"]
   
   â€¢ Validation: Cross-check with skills_db.json
   â€¢ Fallback: Regex extraction if API fails
   â€¢ Time: 4-6 seconds (parallel calls)
   â†“
Step 4: Skill Normalization & Matching
   â€¢ Normalize variations:
     - "React.js" â†’ "react"
     - "PostgreSQL" â†’ "postgres"
     - "Node.js" â†’ "nodejs"
     - "CI/CD" â†’ "cicd"
   â€¢ 50+ synonym mappings applied
   
   â€¢ Set operations:
     matching_skills = resume_skills âˆ© jd_skills
     missing_skills = jd_skills - resume_skills
     extra_skills = resume_skills - jd_skills
   
   â€¢ Example:
     Resume: {python, django, mysql, git, react}
     JD Required: {python, django, react, docker, kubernetes, aws}
     Matching: {python, django, react} = 3 skills
     Missing: {docker, kubernetes, aws} = 3 skills
   â†“
Step 5: ATS Score Calculation
   â€¢ Formula: (matching_skills / total_jd_skills) Ã— 100
   â€¢ Example: (3 / 6) Ã— 100 = 50% match
   
   â€¢ Score Categories:
     - 80-100%: Excellent match (high interview chance)
     - 60-79%: Good match (competitive candidate)
     - 40-59%: Fair match (some gaps to fill)
     - 0-39%: Poor match (major skill gaps)
   â†“
Step 6: ATS Layout Feedback
   â€¢ Same as CV Analyzer (Gemini + fallback)
   â€¢ Specific to resume document only
   â€¢ Optimization tips for ATS parsing
   â€¢ Time: 2-3 seconds
   â†“
Step 7: YouTube Learning Path Generation
   â€¢ For each missing skill:
     1. Lookup in youtube_links.json
     2. Return {skill_name, youtube_link}
   â€¢ Curated tutorials (4+ ratings)
   â€¢ Direct learning path
   â†“
Step 8: Professional UI Display
   â€¢ ATS Score Card:
     - Large percentage with gradient progress bar
     - Color-coded (green: 80+, orange: 60-79, red: <60)
   
   â€¢ Matching Skills Card (Green):
     - Skills you have that JD requires
     - Shows your strengths
   
   â€¢ Missing Skills Card (Red):
     - Critical gaps to fill
     - YouTube link for each skill
     - Priority learning list
   
   â€¢ ATS Tips Card (Blue):
     - Layout improvements
     - Keyword suggestions
   
   â€¢ All Skills Card (Purple):
     - Complete skill inventory
     - Extra skills you have
   â†“
Step 9: Return Results
   â€¢ Total Time: 10-15 seconds
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **LLM** | Google Gemini Pro | Dual skill extraction | 2 parallel API calls |
| **Text Extraction** | pdfplumber, python-docx | Document parsing | PDF/DOCX support |
| **Skill Normalization** | Custom synonym mapping | Reduce false negatives | 50+ mappings |
| **Matching** | Python set operations | Fast comparison | O(n) complexity |
| **Scoring** | Cosine similarity concept | ATS compatibility | Percentage match |
| **Learning Paths** | youtube_links.json | Tutorial mapping | 100+ curated links |
| **Frontend** | Material-UI Cards | Professional display | Color-coded sections |
| **Charts** | Custom gradient progress bar | Visual score | Green/orange/red |

**Skill Normalization (Key Innovation):**

**Problem:** False negatives due to variations
- Resume says "React.js" but JD says "react" â†’ Mismatch detected
- Result: Artificially low ATS score

**Solution:** Comprehensive synonym mapping
```python
SKILL_SYNONYMS = {
    'react.js': 'react',
    'reactjs': 'react',
    'react js': 'react',
    'node.js': 'nodejs',
    'node js': 'nodejs',
    'postgresql': 'postgres',
    'postgres sql': 'postgres',
    'sqlite3': 'sqlite',
    'ci/cd': 'cicd',
    'continuous integration': 'cicd',
    'oop': 'object-oriented programming',
    'restful api': 'rest api',
    'amazon web services': 'aws',
    # ... 40+ more mappings
}
```

**Impact:**
- Before normalization: 24 "missing" skills (many false positives)
- After normalization: 4 actual missing skills
- Accuracy improvement: 83% reduction in false positives

**Real User Example:**

```
Input:
â”œâ”€ Resume: Python, Django, PostgreSQL, React.js, Git, HTML/CSS
â””â”€ Job Description: Python, Django, Postgres, React, Docker, Kubernetes, AWS

After Normalization:
â”œâ”€ Resume Skills: {python, django, postgres, react, git, html, css}
â””â”€ JD Skills: {python, django, postgres, react, docker, kubernetes, aws}

Results:
â”œâ”€ ATS Score: 57% (4/7 skills matched)
â”œâ”€ Matching Skills: âœ… Python, Django, Postgres, React
â”œâ”€ Missing Skills: âŒ Docker, Kubernetes, AWS
â”‚  â€¢ Docker: https://youtube.com/watch?v=...
â”‚  â€¢ Kubernetes: https://youtube.com/watch?v=...
â”‚  â€¢ AWS: https://youtube.com/watch?v=...
â”œâ”€ Extra Skills: Git, HTML, CSS (bonus but not required)
â””â”€ ATS Tip: "Add Cloud Computing section highlighting any AWS experience"

Time: 12.3 seconds
```

**Use Cases:**

**1. Job Application Strategy**
- Apply to 80%+ matches immediately
- For 60-79% matches: Learn 1-2 missing skills first
- Skip <40% matches: Too many gaps

**2. Resume Customization**
- Add missing keywords from JD to resume
- Highlight matching skills in summary section
- Reorder experience to emphasize relevant skills

**3. Interview Preparation**
- Focus study on missing critical skills
- Prepare to explain gaps if asked
- Showcase extra skills as differentiators

**4. Career Gap Analysis**
- Understand distance from dream job
- Create learning roadmap (3-6 months)
- Track progress by re-analyzing monthly

**Why This Matters:**
- **Precision:** Exact match percentage (not vague "good fit")
- **Actionable:** Direct YouTube links to learn missing skills
- **Time-Saving:** Know before applying (avoid rejections)
- **Strategic:** Prioritize which jobs to pursue
- **ATS-Friendly:** Optimize resume for specific role

---

### 4. ğŸ“š RAG Coach - Privacy-First Document Intelligence

**What It Does:**
Upload your resume and job descriptions as PDFs, then ask ANY question about them using a completely local AI system. Unlike cloud-based services, your documents never leave your computerâ€”everything runs locally using Ollama. Think of it as having a career coach who's read your entire resume and the job description, ready to answer questions instantly.

**How It Helps Users:**
- **Privacy-Conscious Users:** Documents processed 100% locally (no data sent to external APIs)
- **Interactive Analysis:** Ask follow-up questions, get context-aware answers
- **Document Comparison:** Query across multiple documents simultaneously
- **Custom Guidance:** "What should I emphasize in my cover letter?" type questions

**Complete Workflow & Implementation:**

```
Step 1: Upload Multiple Documents
   â€¢ Supported: PDF files (resume, job descriptions, cover letters)
   â€¢ Multiple file upload (2-10 documents typical)
   â€¢ Size limit: 10MB per file
   â†“
Step 2: Automatic Document Detection
   â€¢ Content analysis using keyword matching:
     
     Resume Keywords:
       - "experience", "education", "skills"
       - "work history", "projects", "certifications"
       - Heuristic: 3+ keywords â†’ classified as RESUME
     
     Job Description Keywords:
       - "requirements", "qualifications", "responsibilities"
       - "about the role", "what you'll do", "must have"
       - Heuristic: 3+ keywords â†’ classified as JOB_DESCRIPTION
   
   â€¢ Tag each document with doc_type metadata
   â€¢ Enable filtered retrieval later
   â€¢ Time: <1 second
   â†“
Step 3: Text Extraction & Chunking
   â€¢ PDF Parsing:
     Library: PyPDFLoader (LangChain)
     Extract: Text page-by-page
     Handle: Multi-column layouts, tables
   
   â€¢ Chunking Strategy:
     Splitter: RecursiveCharacterTextSplitter
     Chunk size: 500 characters
     Overlap: 50 characters (preserve context)
     Separators: ["\n\n", "\n", ". ", " "]
     
     Why 500 chars?
       - Balance: Not too small (loses context) or large (noise)
       - Typical: 3-5 sentences per chunk
       - Retrieval: Better precision with focused chunks
   
   â€¢ Example:
     Input: 5-page resume
     Output: ~30-40 chunks
     Metadata: {source: "resume.pdf", page: 1, doc_type: "RESUME"}
   
   â€¢ Time: 2-3 seconds
   â†“
Step 4: Generate Embeddings
   â€¢ Model: sentence-transformers/all-MiniLM-L6-v2
     - Size: 90MB (CPU-friendly)
     - Dimensions: 384
     - Speed: ~1000 sentences/sec (CPU)
     - Normalization: L2 norm
   
   â€¢ Process:
     1. Tokenize each chunk (max 256 tokens)
     2. Pass through BERT-based encoder
     3. Mean pooling to get 384-dim vector
     4. L2 normalize for cosine similarity
   
   â€¢ Batch processing: 32 chunks at a time
   â€¢ Time: 3-5 seconds for 50 chunks
   â†“
Step 5: Build FAISS Vector Index
   â€¢ Index Type: IndexFlatL2 (exact search, no approximation)
   â€¢ Distance Metric: L2 Euclidean distance
     - Equivalent to cosine similarity with normalized vectors
     - Formula: distance = sqrt(sum((a - b)^2))
   
   â€¢ Storage:
     - In-memory: Fast retrieval during session
     - Persistent: Saved to disk (./rag_coach_index/)
     - Reload: Available across restarts
   
   â€¢ Index size: ~50MB for 1000 chunks
   â€¢ Build time: <1 second
   â†“
Step 6: Auto-Generated Initial Analysis
   â€¢ Extract skills from both resume and JD
   â€¢ Apply 50+ synonym normalizations
   â€¢ Set operations for gap analysis
   â€¢ Generate 3-5 resume bullet points using TinyLLama
   
   â€¢ Bullet Point Generation:
     Prompt: "Based on job requirements, suggest resume improvements"
     Context: Resume + JD chunks
     Output: ATS-optimized action-oriented bullets
   
   â€¢ Time: 5-8 seconds
   â†“
Step 7: Interactive Q&A System
   â€¢ User asks question (e.g., "What certifications should I get?")
   â†“
Step 7a: Query Embedding
   â€¢ Same model: all-MiniLM-L6-v2
   â€¢ Convert question to 384-dim vector
   â€¢ Time: <100ms
   â†“
Step 7b: Similarity Search
   â€¢ FAISS retrieval:
     - Top-K: 4 most relevant chunks
     - Distance threshold: None (take top 4)
     - Filter: Can filter by doc_type if specified
   
   â€¢ Query Intent Detection:
     If "job description" in query â†’ Filter doc_type="JOB_DESCRIPTION"
     If "my resume" in query â†’ Filter doc_type="RESUME"
     Else â†’ Search all documents
   
   â€¢ Retrieved chunks include:
     - Text content
     - Source document name
     - Page number
     - Doc type
   
   â€¢ Time: <50ms
   â†“
Step 7c: Local LLM Generation (TinyLLama)
   â€¢ Model: TinyLlama-1.1B-Chat-v1.0
     - Parameters: 1.1 billion
     - Quantization: Q4_K_M (4-bit)
     - Memory: ~4GB RAM
     - Inference: ~50 tokens/sec (CPU)
     - Context window: 2048 tokens
   
   â€¢ Deployment: Ollama (local inference server)
     - No internet connection needed
     - No data leaves your computer
     - Models stored locally (~637MB download)
   
   â€¢ Prompt Template:
     ```
     Context from your documents:
     {chunk1}
     {chunk2}
     {chunk3}
     {chunk4}
     
     Question: {user_question}
     
     Answer based on the context above:
     ```
   
   â€¢ Generation Params:
     - temperature: 0.7
     - max_tokens: 512
     - top_p: 0.9
     - repeat_penalty: 1.1
   
   â€¢ Time: 5-10 seconds (depends on answer length)
   â†“
Step 8: Response with Source Attribution
   â€¢ Return:
     - Answer text
     - Source chunks used
     - Document names + page numbers
   
   â€¢ Example:
     Answer: "You should add Docker and Kubernetes to your skills section..."
     Sources: [resume.pdf (page 1), job_description.pdf (page 2)]
   
   â€¢ Frontend: Display sources below answer
   â†“
Step 9: Conversation Memory
   â€¢ Store Q&A in database (rag_coach_queries table)
   â€¢ Enable history tracking
   â€¢ Reuse context for follow-up questions
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Local LLM** | TinyLlama 1.1B (Q4) | Answer generation | 100% local, CPU-friendly |
| **Inference Server** | Ollama | Model deployment | REST API, background service |
| **Embeddings** | all-MiniLM-L6-v2 | Semantic search | 384-dim, 90MB model |
| **Vector DB** | FAISS (IndexFlatL2) | Similarity search | Exact search, no approximation |
| **PDF Parser** | PyPDFLoader (LangChain) | Text extraction | Page-by-page parsing |
| **Text Splitter** | RecursiveCharacterTextSplitter | Chunking | 500 chars, 50 overlap |
| **Framework** | LangChain 0.3.27 | RAG orchestration | Document loaders, chains |
| **Storage** | Disk persistence | Index saving | Reload without re-embedding |

**System Architecture:**

```
User's Computer (Local)
â”œâ”€ Frontend (React)
â”‚  â””â”€ File upload component
â”‚
â”œâ”€ Backend (FastAPI)
â”‚  â”œâ”€ /rag-coach/upload endpoint
â”‚  â”œâ”€ PDF parsing (PyPDFLoader)
â”‚  â”œâ”€ Chunking (RecursiveCharacterTextSplitter)
â”‚  â””â”€ /rag-coach/query endpoint
â”‚
â”œâ”€ Embedding Model (Local)
â”‚  â”œâ”€ Model: all-MiniLM-L6-v2
â”‚  â”œâ”€ Location: ~/.cache/huggingface/
â”‚  â””â”€ Device: CPU (no GPU needed)
â”‚
â”œâ”€ FAISS Index (Local)
â”‚  â”œâ”€ Type: IndexFlatL2
â”‚  â”œâ”€ Location: ./rag_coach_index/
â”‚  â””â”€ Size: ~50MB per 1000 chunks
â”‚
â””â”€ Ollama Server (Local)
   â”œâ”€ TinyLlama model
   â”œâ”€ Port: 11434
   â”œâ”€ Storage: ~/.ollama/models/
   â””â”€ Privacy: NO external API calls

External APIs Used: NONE âœ…
```

**Privacy Guarantees:**

| Aspect | Status | Details |
|--------|--------|---------|
| **Document Storage** | âœ… Local | Saved to ./uploads/ then deleted |
| **Embeddings** | âœ… Local | Generated on your CPU |
| **Vector Index** | âœ… Local | FAISS index on disk |
| **LLM Inference** | âœ… Local | TinyLlama via Ollama |
| **Network Calls** | âŒ None | Zero external API calls |
| **Data Retention** | âœ… User control | Delete anytime |

**Key Innovation: Skill Normalization**

**Problem Before:**
```
Resume: "React.js", "Node.js", "PostgreSQL"
JD: "react", "nodejs", "postgres"
Result: 0 matches detected (false negative)
User sees: "Add React.js" (but they already have it!)
```

**Solution:**
```python
SKILL_SYNONYMS = {
    'react.js': 'react',
    'reactjs': 'react',
    'react js': 'react',
    'node.js': 'nodejs',
    'node js': 'nodejs',
    'postgresql': 'postgres',
    'postgres sql': 'postgres',
    'sqlite3': 'sqlite',
    'ci/cd': 'cicd',
    'continuous integration': 'cicd',
    # ... 50+ total mappings
}

def normalize_skill(skill):
    skill_lower = skill.lower().strip()
    return SKILL_SYNONYMS.get(skill_lower, skill_lower)
```

**Impact:**
- Before: 24 "missing" skills (false positives)
- After: 4 actual missing skills
- Accuracy: 83% improvement in gap detection

**Real User Example:**

```
Step 1: User uploads 2 files
â”œâ”€ resume.pdf (3 pages)
â””â”€ job_description.pdf (2 pages)

Step 2: Auto-analysis generated
â”œâ”€ Skills to add: Docker, Kubernetes, CI/CD
â”œâ”€ Resume bullets:
â”‚  â€¢ "Implemented REST APIs using Django framework for microservices"
â”‚  â€¢ "Designed PostgreSQL database schema for user management system"
â””â”€ ATS keywords: Python, Django, REST API, Docker, Kubernetes

Step 3: User asks: "What projects should I highlight?"

Step 4: System retrieves relevant chunks
â”œâ”€ resume.pdf (page 2): "Projects: E-commerce platform, Chat app..."
â””â”€ job_description.pdf (page 1): "Experience with scalable web applications..."

Step 5: TinyLlama generates answer
"Based on the job requirements, you should highlight:
1. Your e-commerce platform project - emphasize scalability aspects
2. Chat application - mention real-time features and backend architecture
3. Any projects using Docker/containers (add if missing)
Focus on quantifiable results (user load, performance metrics)"

Sources: resume.pdf (page 2), job_description.pdf (page 1)
Time: 7.3 seconds
```

**Use Cases:**

1. **Cover Letter Writing**
   - Q: "What should I mention in my cover letter?"
   - A: Context-aware suggestions from both docs

2. **Interview Prep**
   - Q: "What questions might they ask based on this JD?"
   - A: Gap analysis + likely interview topics

3. **Resume Tailoring**
   - Q: "Which experiences match the role best?"
   - A: Relevant projects/jobs from resume

4. **Skill Prioritization**
   - Q: "Which missing skill should I learn first?"
   - A: Critical vs. nice-to-have skills

**Why This Matters:**
- **Privacy:** Your documents never uploaded to external servers
- **Flexibility:** Ask ANY question about your documents
- **Accuracy:** Context-aware answers (not generic AI)
- **Speed:** 7-10 seconds per Q&A (local inference)
- **Cost:** Free (no API credits, no subscriptions)

---

### 5. ğŸ“Š Admin Dashboard - Platform Analytics & Management

**What It Does:**
Provides administrators with real-time insights into platform usage, user engagement, skill trends, and system performance. View comprehensive analytics through 8+ interactive visualizations including user growth, retention metrics, popular jobs, and activity heatmaps. Also enables user management (suspend/activate/delete accounts).

**How It Helps Administrators:**
- **Platform Owners:** Monitor growth, engagement, retention rates
- **HR Teams:** Identify trending skills, popular job roles
- **Business Intelligence:** Data-driven decisions on feature development
- **User Management:** Handle spam accounts, inactive users
- **System Health:** Track API usage, error rates, response times

**Complete Features & Implementation:**

```
Access Control:
â”œâ”€ Route: /admin (protected)
â”œâ”€ Authentication: JWT token required
â”œâ”€ Authorization: role="admin" in database
â””â”€ Setup: python create_admin.py (creates admin@gmail.com / admin)

Dashboard Sections:
```

**1. KPI Cards (4 Metrics)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Total Users         Active 30d     â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆ 150           â–ˆâ–ˆâ–ˆâ–ˆ 85         â”‚
â”‚  +12 this week      +18 this week   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Total Analyses     Avg Match       â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆ 450           â–ˆâ–ˆâ–ˆâ–ˆ 72.3%      â”‚
â”‚  +67 this week      ATS Score Avg   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Implementation:
- Component: Material-UI Card with gradient icons
- Backend: SQL aggregation queries
- Update: Real-time on page load
- Animation: Hover lift effect

SQL Queries:
â€¢ total_users: COUNT(*) FROM users
â€¢ active_users_30days: WHERE last_active >= NOW() - 30 days
â€¢ total_analyses: COUNT(*) FROM resume_analyses
â€¢ avg_match_percentage: AVG(match_percentage)
```

**2. Retention Metrics Display**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Overall Retention    7-Day Retention â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆ 56.7%          â–ˆâ–ˆâ–ˆâ–ˆ 71.4%    â”‚
â”‚                                       â”‚
â”‚  30-Day Retention                     â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆ 58.9%                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Calculation:
â€¢ retention_rate = returning_users / total_users
â€¢ retention_7days = active_last_7 / active_7_days_ago
â€¢ retention_30days = active_last_30 / active_30_days_ago

Key Insights:
- High 7-day retention (71%) = Good first impression
- 30-day retention (59%) = Users return after a month
- Overall (57%) = Long-term engagement health
```

**3. User Growth Chart (30-Day Time Series)**
```
      User Growth (Last 30 Days)
160 â”¤         â•­â”€â”€â”€â”€â”€â”€â”€â”€â•®
150 â”¤    â•­â”€â”€â”€â”€â•¯        â•°â”€â”€â”€
140 â”¤  â•­â”€â•¯                  
130 â”¤â•­â”€â•¯                    
120 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0d   10d   20d   30d

Visualization: Recharts AreaChart
- Type: Time series with gradient fill
- Data: [{date: "2024-11-01", count: 138}, ...]
- X-axis: Dates (last 30 days)
- Y-axis: Cumulative user count
- Color: Indigo gradient (#4F46E5)

Backend Generation:
for i in range(30):
    date = today - timedelta(days=30-i)
    user_count = db.query(User).filter(User.created_at <= date).count()
    user_growth.append({"date": date.strftime("%Y-%m-%d"), "count": user_count})
```

**4. Match Score Distribution (Pie Chart)**
```
    Match Score Categories
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ â—â—â—â— Excellent   â”‚  35%  (80-100%)
     â”‚ â—â—â—  Good        â”‚  28%  (60-79%)
     â”‚ â—â—   Fair        â”‚  23%  (40-59%)
     â”‚ â—    Poor        â”‚  14%  (<40%)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Visualization: Recharts PieChart
- Data: Categorize all match_percentage values
- Colors: Green (Excellent), Blue (Good), Orange (Fair), Red (Poor)
- Labels: Percentage + category name

Insights:
- 63% have Good+ scores â†’ Platform working well
- 14% Poor scores â†’ Need upskilling resources
- Target: Increase Excellent category over time
```

**5. Top Recommended Jobs (Bar Chart)**
```
Top 10 Recommended Job Titles
Software Developer   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 89
Data Professional    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 67
DevOps Engineer      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 54
QA Engineer          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 42
Product Manager      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35
...

Visualization: Recharts BarChart
- X-axis: Job titles (45Â° rotation for readability)
- Y-axis: Number of recommendations
- Color: Indigo (#4F46E5)
- Sorting: Descending by count

SQL Query:
SELECT recommended_job_title, COUNT(*) as count
FROM resume_analyses
GROUP BY recommended_job_title
ORDER BY count DESC
LIMIT 10

Business Value:
- Identify platform demographics
- Content targeting (create guides for top jobs)
- Partnership opportunities (recruit for these roles)
```

**6. Top Missing Skills (Bar Chart)**
```
Top 10 Skills Users Need
Docker               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45
Kubernetes           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 38
AWS                  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35
CI/CD                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 28
React                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 25
...

Visualization: Recharts BarChart
- X-axis: Skill names
- Y-axis: Frequency count
- Color: Red (#EF4444) - highlights gaps
- Sorting: Descending

SQL Query (Complex):
1. Extract skills_to_add JSON field from all analyses
2. Parse JSON arrays
3. Count skill frequencies
4. Order by count DESC
5. LIMIT 10

Strategic Value:
- Course creation priorities
- Partnership with training platforms
- Feature: Add skill learning modules
```

**7. Recent Activity Feed (Table)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Type   â”‚ User            â”‚ Action           â”‚ Time     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Analysisâ”‚ user@email.com â”‚ Analyzed resume  â”‚ 2m ago   â”‚
â”‚ Query  â”‚ john@email.com  â”‚ Asked about AI   â”‚ 5m ago   â”‚
â”‚ Analysisâ”‚ jane@email.com â”‚ Resume+JD match  â”‚ 8m ago   â”‚
â”‚ Query  â”‚ user@email.com  â”‚ DevOps career    â”‚ 12m ago  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Implementation:
- Data: UNION of resume_analyses and career_queries
- Sort: created_at DESC
- Limit: Last 20 activities
- Display: 10 most recent
- Chips: Color-coded by type (blue=analysis, cyan=query)

SQL Query:
(SELECT 'analysis' as type, email, 'Analyzed resume' as action, 
 created_at FROM resume_analyses JOIN users)
UNION ALL
(SELECT 'query' as type, email, user_query_text as action,
 created_at FROM career_queries JOIN users)
ORDER BY created_at DESC LIMIT 20

Monitoring Value:
- Real-time platform usage
- Identify active hours
- Detect unusual patterns
```

**8. User Management Table**
```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID  â”‚ Email           â”‚ Analyses   â”‚ Status   â”‚ Actions â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 142 â”‚ user@email.com  â”‚ 12         â”‚ âœ… Activeâ”‚ [â—â—â—]   â”‚
â”‚ 141 â”‚ spam@email.com  â”‚ 0          â”‚ â›” Bannedâ”‚ [â—â—â—]   â”‚
â”‚ 140 â”‚ john@email.com  â”‚ 5          â”‚ âœ… Activeâ”‚ [â—â—â—]   â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Features:
- Pagination: 50 users per page
- Search: By email/name
- Actions: Suspend, Activate, Delete
- Confirmation: Modal before delete

Endpoints:
â€¢ GET /admin/users?skip=0&limit=50
â€¢ PUT /admin/user/{id}/suspend
â€¢ PUT /admin/user/{id}/activate
â€¢ DELETE /admin/user/{id}
```

**Technologies Used:**

| Component | Technology | Purpose | Details |
|-----------|-----------|---------|---------|
| **Frontend** | React 18.2.0 | Dashboard UI | AdminDashboard.js (600+ lines) |
| **Charts** | Recharts 2.10.3 | Data visualization | Area, Bar, Pie charts |
| **UI Library** | Material-UI 5.14.19 | Components | Cards, Grid, Table, Chips |
| **Layout** | MUI Grid | Responsive design | 3-column adaptive layout |
| **Backend** | FastAPI /admin/stats | Data aggregation | 15+ metrics in one call |
| **Database** | SQLite + SQLAlchemy | Data queries | Complex JOIN and GROUP BY |
| **Authentication** | JWT + role check | Access control | get_current_admin dependency |
| **Styling** | Wozber theme | Minimal design | Clean white cards, subtle shadows |

**Backend Implementation (/admin/stats endpoint):**

```python
@app.get("/admin/stats", tags=["Admin"])
async def get_admin_stats(
    current_admin: User = Depends(get_current_admin),  # Verify admin role
    db: SessionLocal = Depends(get_db)
):
    from sqlalchemy import func
    from datetime import timedelta
    
    now = datetime.utcnow()
    thirty_days_ago = now - timedelta(days=30)
    seven_days_ago = now - timedelta(days=7)
    
    # User Statistics
    total_users = db.query(User).count()
    active_users_30d = db.query(User).filter(
        User.last_active >= thirty_days_ago
    ).count()
    
    # Analysis Statistics
    total_analyses = db.query(ResumeAnalysis).count()
    avg_match = db.query(func.avg(ResumeAnalysis.match_percentage)).scalar()
    
    # User Growth (30-day time series)
    user_growth = []
    for i in range(30):
        date = now - timedelta(days=30-i)
        count = db.query(User).filter(User.created_at <= date).count()
        user_growth.append({"date": date.strftime("%Y-%m-%d"), "count": count})
    
    # Top Jobs
    top_jobs = db.query(
        ResumeAnalysis.recommended_job_title,
        func.count(ResumeAnalysis.id).label('count')
    ).group_by(ResumeAnalysis.recommended_job_title)\
     .order_by(func.count(ResumeAnalysis.id).desc())\
     .limit(10).all()
    
    # ... more aggregations ...
    
    return {
        "total_users": total_users,
        "active_users_30days": active_users_30d,
        # ... 15+ total fields ...
    }
```

**Database Schema (models.py):**

```python
class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True)
    email = Column(String, unique=True)
    full_name = Column(String)
    password_hash = Column(String)
    role = Column(String, default="user")  # "user" or "admin"
    is_active = Column(Boolean, default=True)  # For suspend feature
    created_at = Column(DateTime, default=datetime.utcnow)
    last_active = Column(DateTime, default=datetime.utcnow)
    
    # Relationships for JOIN queries
    analyses = relationship("ResumeAnalysis", back_populates="owner")
    queries = relationship("CareerQuery", back_populates="owner")
```

**Admin User Creation:**

```bash
$ python create_admin.py

====================================================
CREATE ADMIN USER - NextStepAI
====================================================

âš ï¸  Admin user already exists: admin@gmail.com

Update admin password to 'admin'? (y/n): y
âœ… Admin password updated!

====================================================
Admin Credentials:
Email: admin@gmail.com
Password: admin
====================================================
```

**Access Flow:**

```
1. Login with admin credentials
   POST /admin/login
   {"email": "admin@gmail.com", "password": "admin"}
   â†’ Returns JWT token with role="admin"

2. Navigate to admin dashboard
   Click "Features" â†’ "Admin Panel"
   OR visit: http://localhost:3000/admin

3. Backend verifies admin
   JWT decoded â†’ Check role field
   If role != "admin" â†’ 403 Forbidden
   
4. Load dashboard data
   GET /admin/stats
   Authorization: Bearer {token}
   â†’ Returns 15+ metrics in JSON

5. Render visualizations
   Recharts components consume JSON data
   Material-UI Grid arranges cards
   â†’ Display interactive dashboard
```

**Real Admin Dashboard Example:**

```json
Dashboard Data (GET /admin/stats):
{
  "total_users": 150,
  "active_users_30days": 85,
  "active_users_7days": 42,
  "new_users_7days": 12,
  "total_analyses": 450,
  "analyses_7days": 67,
  "total_queries": 328,
  "queries_7days": 54,
  "avg_match_percentage": 72.3,
  "retention_rate": 0.567,
  "retention_7days": 0.714,
  "retention_30days": 0.589,
  "user_growth": [
    {"date": "2024-11-01", "count": 138},
    {"date": "2024-11-02", "count": 140},
    ...30 days
  ],
  "top_jobs": [
    {"job": "Software Developer", "count": 89},
    {"job": "Data Professional", "count": 67},
    ...10 jobs
  ],
  "top_missing_skills": [
    {"skill": "docker", "count": 45},
    {"skill": "kubernetes", "count": 38},
    ...10 skills
  ],
  "match_distribution": [65.2, 78.5, 82.1, ...450 scores],
  "recent_activity": [
    {
      "type": "analysis",
      "user": "user@email.com",
      "action": "Analyzed resume",
      "timestamp": "2024-11-07T14:30:22Z"
    },
    ...20 activities
  ],
  "activity_heatmap": [
    {"day": "Monday", "hour": 14, "count": 23},
    ...168 data points (7 days Ã— 24 hours)
  ]
}

Load Time: 200-300ms (optimized SQL queries)
Visualization Render: <100ms (Recharts)
Total Page Load: ~400ms
```

**Business Insights from Dashboard:**

1. **User Engagement:**
   - 71% 7-day retention â†’ Strong onboarding
   - Peak hours: 10 AM - 4 PM â†’ Server capacity planning

2. **Content Strategy:**
   - Top missing skill: Docker â†’ Create Docker tutorial
   - Top job: Software Developer â†’ Focus content there

3. **Growth Tracking:**
   - 12 new users this week â†’ Marketing ROI
   - 85/150 (57%) active â†’ Engagement campaigns needed

4. **Platform Health:**
   - 72% avg match score â†’ Algorithm performing well
   - 450 analyses â†’ High feature usage

5. **User Quality:**
   - 63% have Good+ matches â†’ Quality user base
   - 14% Poor matches â†’ Need skill development features

**Why This Matters:**
- **Data-Driven Decisions:** No guesswork on platform performance
- **Trend Identification:** Spot skill gaps, popular careers early
- **User Management:** Handle spam, inactive accounts efficiently
- **Growth Tracking:** Visualize user acquisition, retention
- **Strategic Planning:** Content, features, partnerships based on real data

---

## Technology Stack

### Frontend

**React 18 + Material-UI v5 + Aurora WebGL**

```javascript
Core Libraries:
  - React 18.2.0 (hooks, context API)
  - Material-UI v5.14.19 (components, theming)
  - React Router v6.20.1 (client-side routing)
  - Axios 1.6.2 (HTTP client with JWT)
  - Recharts 2.10.3 (data visualization)
  - OGL 1.0.11 (WebGL library for Aurora)

Aurora Effect Implementation:
  - Technology: WebGL shaders (vertex + fragment)
  - Library: OGL (7KB, GPU-accelerated)
  - Color Scheme: Purple (#8b5cf6) â†’ Blue (#3b82f6) â†’ Green (#10b981)
  - Performance: 60 FPS at 1080p
  - Configuration:
    * Amplitude: 1.5
    * Blend: 0.8
    * Speed: 0.4
    * Noise: Simplex noise algorithm

UI Features:
  - Wozber-Style Design: Clean white cards with subtle shadows
  - Light Theme: Light gray (#F9FAFB) background with indigo/cyan accents
  - Protected Routes: JWT-based authentication guards
  - Responsive Design: Mobile-first with MUI breakpoints
  - Custom Fonts: Poppins (primary), Inter, Space Grotesk (headings)
  - Premium Formatting: Markdown-style AI responses with typography hierarchy
```

**Aurora Technical Details:**
```javascript
// Vertex Shader (GLSL 3.0)
#version 300 es
in vec2 position;
void main() {
  gl_Position = vec4(position, 0.0, 1.0);
}

// Fragment Shader (Simplex Noise)
- Generates animated gradient using GPU
- Color interpolation between 3 stops
- Time-based animation (uTime uniform)
- Resolution-aware (uResolution uniform)
```

---

### Backend

**FastAPI + Python 3.10 + Async Architecture**

```python
Core Framework:
  - FastAPI 0.116.1 (async ASGI)
  - Uvicorn 0.35.0 (ASGI server)
  - Pydantic 2.11.7 (data validation)

Authentication:
  - python-jose 3.5.0 (JWT tokens)
  - passlib 1.7.4 (bcrypt hashing)
  - Google OAuth 2.0 (SSO)

Database:
  - SQLAlchemy 2.0.43 (ORM)
  - SQLite 3 (file-based, zero configuration)

API Features:
  - RESTful endpoints (/analyze_resume/, /rag-coach/*, /admin/*)
  - Admin dashboard with comprehensive analytics
  - Role-based access control (user/admin)
  - CORS enabled (localhost:3000, localhost:8501)
  - Lazy model loading (background threads)
  - Comprehensive logging

Additional Features:
  - Admin Dashboard: User analytics, retention metrics, engagement tracking
  - History Tracking: Resume analyses, career queries, RAG interactions
  - User Management: Suspend, activate, delete users (admin only)
```

---

### AI & Machine Learning

**LLMs:**
```
1. Google Gemini Pro (API)
   - Purpose: Skill extraction, ATS feedback, RAG fallback
   - Model: gemini-1.5-pro
   - Context Window: 32k tokens
   - Temperature: 0.3 (consistent outputs)
   - Fallback: Regex-based extraction (100+ patterns)

2. GPT-2 Medium (Fine-tuned)
   - Purpose: Career advice generation
   - Parameters: 355 million
   - Size: 1.5GB
   - Training: 749 examples, 15 epochs
   - Format: PyTorch (HuggingFace)
   - Fallback: RAG over career guides

3. TinyLLama 1.1B (RAG)
   - Purpose: Document Q&A
   - Parameters: 1.1 billion
   - Quantization: Q4_K_M (4-bit)
   - Memory: ~4GB RAM
   - Inference: CPU-friendly (~50 tokens/sec)
   - Privacy: 100% local execution via Ollama
```

**ML Models:**
```python
Job Classification:
  - Algorithm: Multinomial Naive Bayes
  - Feature Engineering: TF-IDF Vectorization
  - Training Data: 8000+ job-skill mappings (jobs_cleaned.csv)
  - Accuracy: ~85% on test set
  - Inference Time: <10ms
  - Model Size: 450KB

  Categories (12 groups):
    â€¢ Data Professional
    â€¢ Software Developer
    â€¢ IT Operations & Infrastructure
    â€¢ Project / Product Manager
    â€¢ QA / Test Engineer
    â€¢ Human Resources
    â€¢ Sales & Business Development
    â€¢ Marketing
    â€¢ UI/UX & Design
    â€¢ Finance & Accounting
    â€¢ Customer Support
    â€¢ Other

  Hyperparameters (GridSearchCV):
    â€¢ Vectorizer: TfidfVectorizer(ngram_range=(1,2))
    â€¢ Classifier: MultinomialNB(alpha=0.1-1.0) OR LogisticRegression
    â€¢ Train/Test Split: 80/20
    â€¢ CV Folds: 5
    â€¢ Scoring: F1 Weighted
```

**RAG System:**
```python
Vector Database: FAISS
  - Index Type: IndexFlatL2 (exact search)
  - Distance Metric: L2 Euclidean
  - Persistent Storage: ./rag_data/faiss_index

Embeddings: HuggingFace Sentence Transformers
  - Model: all-MiniLM-L6-v2
  - Dimensions: 384
  - Normalization: L2 norm
  - Batch Size: 32

Document Processing:
  - Loader: PyPDFLoader (LangChain)
  - Chunking: RecursiveCharacterTextSplitter
    * chunk_size: 500
    * chunk_overlap: 50
    * separators: ["\n\n", "\n", ". ", " "]
  - Metadata: source, doc_type, page, doc_index

Retrieval:
  - Strategy: Similarity search
  - Top-K: 4 chunks per query
  - Filtering: Document type (resume vs JD)
  - Context Window: 2048 tokens (TinyLLama)
```

**Document Parsing:**
```
PDF: pdfplumber 0.10.3 (complex layouts)
DOCX: python-docx 1.1.0 (paragraph extraction)
Text Splitting: langchain 0.0.335
```

---

## AI Models & Parameters

### Fine-Tuned LLM (Career Advisor)

**Training Configuration:**
```python
# Base Model
model_name = "gpt2-medium"
num_parameters = 355_000_000
architecture = "Transformer decoder (24 layers, 16 attention heads)"

# Training Data
train_dataset = "career_advice_dataset.jsonl + career_advice_ultra_clear_dataset.jsonl"
total_examples = 749
train_val_split = "80/20 (599 train, 150 val)"

# Training Hyperparameters
training_args = {
    "num_train_epochs": 15,
    "learning_rate": 1e-5,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,  # Effective batch = 16
    "max_seq_length": 512,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "fp16": True,  # Mixed precision (GPU only)
    "logging_steps": 50,
    "save_strategy": "epoch",
    "evaluation_strategy": "epoch"
}

# Training Results
total_steps = 1500  # ~100 steps/epoch
training_time_gpu = "15-20 minutes (RTX 2050)"
training_time_cpu = "6+ hours (not recommended)"
final_loss = 0.87
validation_perplexity = 2.39

# Generation Config
generation_config = {
    "max_length": 200,
    "temperature": 0.7,  # 0.5-0.9 for coherence
    "top_p": 0.9,        # Nucleus sampling
    "top_k": 50,         # Top-k sampling
    "repetition_penalty": 1.2,
    "do_sample": True
}
```

**Dataset Structure:**
```json
{
  "prompt": "What skills are required for a Data Scientist in India?",
  "completion": "### Key Skills:\n* Python, R, SQL\n* ML: Regression, Trees, Deep Learning\n\n### Certifications:\n* Google Cloud Professional Data Engineer\n\n### Salary: â‚¹8-25 LPA"
}
```

---

### RAG System (Document Q&A)

**TinyLLama Configuration:**
```python
# Model Setup (Ollama)
model_name = "tinyllama"
full_name = "TinyLlama-1.1B-Chat-v1.0"
parameters = 1_100_000_000
quantization = "Q4_K_M (4-bit)"
model_size = "637MB download"
memory_usage = "~4GB RAM"

# Inference Performance
tokens_per_second_cpu = 50
tokens_per_second_gpu = 150
context_window = 2048
temperature = 0.7
max_tokens = 512
top_p = 0.9
repeat_penalty = 1.1

# Privacy
execution = "100% local (no API calls)"
data_retention = "Zero (PDFs can be deleted post-indexing)"
```

**FAISS Vector Store:**
```python
# Index Configuration
index_type = "IndexFlatL2"  # Exact search, no compression
distance_metric = "L2 Euclidean distance"
dimension = 384  # all-MiniLM-L6-v2 embedding size

# Storage
index_path = "./rag_data/faiss_index"
persistence = "Disk (loaded on startup)"
index_size = "~50MB for 1000 chunks"

# Retrieval Parameters
search_type = "similarity"
k = 4  # Top-4 chunks
score_threshold = 0.7  # Minimum similarity
return_metadata = True  # source, doc_type, page
```

**Embedding Model:**
```python
# HuggingFace Sentence Transformers
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_size = "90MB"
embedding_dimension = 384
max_sequence_length = 256
normalization = "L2 norm"
device = "cpu"  # Lightweight, no GPU needed

# Performance
encoding_speed = "~1000 sentences/sec (CPU)"
batch_size = 32
```

---

### ML Model Training (Job Classification)

**Model Training Script:** `model_training.py`

```python
# Data Loading
dataset = pd.read_csv("jobs_cleaned.csv")
total_records = 8000+
columns = ["Job Title", "Skills", "Grouped_Title"]

# Preprocessing
skill_validation = "skills_db.json (10,000+ valid skills)"
job_consolidation = "54 unique titles â†’ 10 groups"
normalization = "lowercase, strip whitespace"
train_test_split = "80/20"

# Feature Engineering
vectorizer = TfidfVectorizer(
    max_features=500,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)

# Model Selection (GridSearchCV)
algorithms_tested = [
    "MultinomialNB",
    "LogisticRegression"
]
cv_folds = 5
best_algorithm = "MultinomialNB(alpha=1.0)"

# Training
fit_time = "~2 minutes"
accuracy = 0.85
precision = 0.83
recall = 0.82
f1_score = 0.82

# Output Artifacts
saved_models = [
    "job_recommender_pipeline.joblib",     # TF-IDF + NB
    "job_title_encoder.joblib",            # LabelEncoder
    "prioritized_skills.joblib",           # Job â†’ skills mapping
    "master_skill_vocab.joblib"            # Complete vocabulary
]
```

---

## How It Works

### 1. CV Analyzer - Complete Pipeline

**Stage-by-Stage Breakdown:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: File Upload & Text Extraction             â”‚
â”‚ â”œâ”€ PDF: pdfplumber (handles complex layouts)       â”‚
â”‚ â””â”€ DOCX: python-docx (paragraph extraction)        â”‚
â”‚ Output: Plain text (500-5000 chars)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: AI Skill Extraction                       â”‚
â”‚ â”œâ”€ Primary: Gemini LLM (contextual NER)            â”‚
â”‚ â”‚   - Prompt: Extract technical skills, tools      â”‚
â”‚ â”‚   - Temperature: 0.3 (consistency)               â”‚
â”‚ â”‚   - Time: 2-4 seconds                            â”‚
â”‚ â””â”€ Fallback: 9 RegEx patterns (100+ technologies)  â”‚
â”‚ Output: ["python", "django", "mysql", "git"]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: ML Job Prediction                         â”‚
â”‚ â”œâ”€ TF-IDF Vectorization (skills â†’ numeric vector)  â”‚
â”‚ â”œâ”€ Naive Bayes Classification (P(Job|Skills))      â”‚
â”‚ â””â”€ LabelEncoder (decode to job title)              â”‚
â”‚ Output: "Software Developer" (85% accuracy)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: Skill Gap Analysis                        â”‚
â”‚ â”œâ”€ Retrieve required skills from database          â”‚
â”‚ â”œâ”€ Set operations (required - user_skills)         â”‚
â”‚ â””â”€ Calculate match percentage                      â”‚
â”‚ Output: Match 42%, Missing ["docker", "k8s"]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 5: AI Layout Feedback                        â”‚
â”‚ â”œâ”€ Primary: Gemini LLM (ATS optimization)          â”‚
â”‚ â””â”€ Fallback: 7 rule-based checks                   â”‚
â”‚ Output: "âœ… Add professional summary"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 6: Live Job Scraping                         â”‚
â”‚ â”œâ”€ LinkedIn job search (India)                     â”‚
â”‚ â”œâ”€ BeautifulSoup parsing (3 fallback selectors)    â”‚
â”‚ â””â”€ Browser emulation headers                       â”‚
â”‚ Output: Top 5 jobs with links                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 7: YouTube Tutorial Mapping                  â”‚
â”‚ â”œâ”€ JSON lookup (youtube_links.json)                â”‚
â”‚ â””â”€ Map each missing skill to tutorial              â”‚
â”‚ Output: Skill â†’ YouTube link                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 8: Database Storage (if logged in)           â”‚
â”‚ â”œâ”€ SQLAlchemy ORM                                  â”‚
â”‚ â””â”€ Save to ResumeAnalysis table                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 9: Return JSON Response                      â”‚
â”‚ Total Time: 8-12 seconds end-to-end                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Metrics:**

| Stage | Time | Notes |
|-------|------|-------|
| Text Extraction | <1s | Typical 2-page resume |
| Skill Extraction | 2-4s | Gemini API latency |
| Job Prediction | <0.1s | Pre-trained ML model |
| Gap Analysis | <0.01s | Python set operations |
| Layout Feedback | 2-3s | Or instant (fallback) |
| Job Scraping | 2-4s | Network-dependent |
| YouTube Mapping | <0.01s | In-memory lookup |
| **TOTAL** | **8-12s** | **Full pipeline** |

---

### 2. AI Career Advisor - Dual System

**Workflow:**

```
User Query: "Tell me about DevOps"
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Check Model  â”‚
    â”‚   Status     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Model Loaded?   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       /           \
    YES             NO
     â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Fine-tunedâ”‚   â”‚   RAG    â”‚
â”‚  GPT-2   â”‚   â”‚  System  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“               â†“
   Generate      Retrieve
   Response      Context
     â†“               â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Enhance with    â”‚
    â”‚ Live Jobs       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Return JSON     â”‚
    â”‚ Response        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fine-Tuned Model Path:**
```python
# Generation Process
input_text = "### Question: Tell me about DevOps\n\n### Answer:"
tokenized = tokenizer(input_text, return_tensors="pt")
output = model.generate(
    tokenized.input_ids,
    max_length=200,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.2
)
response = tokenizer.decode(output[0], skip_special_tokens=True)
# Returns: Structured advice with skills, certs, salary
```

**RAG Fallback Path:**
```python
# Retrieval Process
query_embedding = embeddings.embed_query(user_question)
relevant_docs = faiss_index.similarity_search(query_embedding, k=4)
context = "\n\n".join([doc.page_content for doc in relevant_docs])

# Generation
prompt = f"Context: {context}\n\nQuestion: {user_question}\n\nAnswer:"
response = gemini_llm.invoke(prompt)
# Returns: Context-aware answer from career guides
```

---

### 3. RAG Coach - Document Intelligence

**Complete Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Upload & Detection            â”‚
â”‚ â”œâ”€ User uploads resume.pdf + jd.pdf    â”‚
â”‚ â”œâ”€ Content analysis (keywords)          â”‚
â”‚ â”‚   Resume: "experience", "education"   â”‚
â”‚ â”‚   JD: "requirements", "qualifications"â”‚
â”‚ â””â”€ Tag metadata: doc_type, source      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Chunking & Embedding          â”‚
â”‚ â”œâ”€ RecursiveCharacterTextSplitter      â”‚
â”‚ â”‚   - chunk_size: 500                  â”‚
â”‚ â”‚   - overlap: 50                      â”‚
â”‚ â”œâ”€ all-MiniLM-L6-v2 embeddings         â”‚
â”‚ â””â”€ 384-dim vectors                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: FAISS Indexing                â”‚
â”‚ â”œâ”€ IndexFlatL2 (exact search)          â”‚
â”‚ â”œâ”€ Store metadata with each chunk      â”‚
â”‚ â””â”€ Persistent storage (disk)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: Auto Skill Analysis           â”‚
â”‚ â”œâ”€ Extract skills from both docs       â”‚
â”‚ â”œâ”€ Normalize (React.js â†’ react)        â”‚
â”‚ â”œâ”€ Set operations (JD - Resume)        â”‚
â”‚ â””â”€ Generate bullet points              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: Interactive Q&A               â”‚
â”‚ â”œâ”€ User query embedding                 â”‚
â”‚ â”œâ”€ FAISS similarity search (top-4)     â”‚
â”‚ â”œâ”€ Filter by doc_type (if needed)      â”‚
â”‚ â”œâ”€ TinyLLama generation                â”‚
â”‚ â””â”€ Return answer + sources             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Skill Normalization (Key Innovation):**
```python
# 50+ synonym mappings
synonym_map = {
    'react.js': 'react',
    'reactjs': 'react',
    'node.js': 'nodejs',
    'postgresql': 'postgres',
    'sqlite3': 'sqlite',
    'restful api': 'rest api',
    'ci/cd': 'cicd',
    'oop': 'object-oriented programming',
    # ... 40+ more
}

# Before: 24 "missing" skills (false positives)
# After: 4 actual missing skills
# Accuracy improvement: 83% reduction in false positives
```

**Query Intent Detection:**
```python
# Filter context by question type
if "job description" in query.lower():
    filter_docs(doc_type="JOB_DESCRIPTION")
elif "my resume" in query.lower():
    filter_docs(doc_type="RESUME")
else:
    use_all_docs()
```

---

## Installation

### Quick Start (5 Minutes)

```bash
# 1. Clone repository
git clone https://github.com/arjuntanil/NextStep-AI.git
cd NextStep-AI

# 2. Create virtual environment
python -m venv career_coach
career_coach\Scripts\activate  # Windows
# source career_coach/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env: Add GOOGLE_API_KEY, JWT_SECRET_KEY

# 5. Train models
python model_training.py

# 6. Build RAG indexes
python ingest_guides.py

# 7. Install Ollama (for RAG Coach)
# Download from https://ollama.ai
ollama pull tinyllama

# 8. Create admin user (optional)
python create_admin.py
# Default credentials: admin@gmail.com / admin

# 9. Start backend
python -m uvicorn backend_api:app --reload

# 10. Start frontend (new terminal)
# Option A: React (modern UI with Aurora)
cd frontend && npm install && npm start

# Option B: Streamlit (simple UI - legacy)
streamlit run app.py
```

### React Frontend Setup

```bash
cd frontend

# Install dependencies (~1-2 minutes, 1406 packages)
npm install

# Start development server
npm run dev

# Access at: http://localhost:3000
```

**React Frontend Features:**
- â˜€ï¸ Wozber-style light theme with clean white cards
- âš¡ Aurora WebGL background (GPU-accelerated, purple/blue/green)
- ğŸ” JWT authentication with protected routes
- ğŸ“± Mobile-responsive Material-UI components
- ğŸ¨ Indigo/Cyan color scheme with Poppins font
- ğŸ“Š Admin dashboard with comprehensive visualizations
- âœ¨ Premium markdown-style AI response formatting

---

### Environment Variables

**.env Configuration:**
```env
# Required
GOOGLE_API_KEY=AIzaSy...  # Get from https://makersuite.google.com/app/apikey
JWT_SECRET_KEY=<64-char-hex>  # python -c "import secrets; print(secrets.token_hex(32))"

# Optional (for Google OAuth login)
GOOGLE_CLIENT_ID=your_google_oauth_client_id
GOOGLE_CLIENT_SECRET=your_google_oauth_client_secret

# Frontend URL
STREAMLIT_FRONTEND_URL=http://localhost:8501
```

---

### Fine-Tune Career Advisor (Optional)

**Option 1: Local Training (GPU Recommended)**
```bash
python production_finetuning_optimized.py
# Time: 15-20 min (GPU) / 6+ hours (CPU)
# Output: career-advisor-final/ directory (1.5GB)
```

**Option 2: Skip Training (Use RAG Only)**
The system will automatically fall back to the RAG system if the fine-tuned model is not found. This is perfectly functional for most use cases.

---

## Usage

### Access URLs

| Service | URL | Description |
|---------|-----|-------------|
| **React Frontend** | http://localhost:3000 | Modern UI with Aurora |
| **Streamlit Frontend** | http://localhost:8501 | Data-centric UI |
| **Backend API** | http://localhost:8000 | FastAPI server |
| **API Docs** | http://localhost:8000/docs | Swagger UI |

### Using CV Analyzer

1. Navigate to "CV Analyzer" page
2. Upload resume (PDF/DOCX)
3. Wait 8-12 seconds for analysis
4. Review:
   - Recommended job title
   - Match percentage
   - Skills to learn (with YouTube links)
   - Live LinkedIn jobs
   - ATS feedback

### Using Resume Analyzer with JD

1. Navigate to "Resume Analyzer (with Job Description)" page
2. Upload both resume and job description (PDF/DOCX)
3. Wait 10-15 seconds for analysis
4. Review:
   - ATS compatibility score with gradient bar
   - Matching skills (green cards)
   - Missing skills with YouTube links (red cards)
   - ATS optimization tips (blue card)
   - All detected skills (purple card)

### Using AI Career Advisor

1. Navigate to "AI Career Advisor" page
2. Ask question (e.g., "Tell me about Data Science")
3. Adjust temperature (0.1-1.0) and length (50-120)
4. Click "Get AI Advice"
5. Review premium-formatted response with:
   - Headers (###, ####)
   - Bullet points with custom markers
   - Bold/italic text
   - Purple accent colors

### Using RAG Coach

1. Navigate to "RAG Coach" page
2. Upload resume.pdf + job_description.pdf
3. Wait for auto-analysis (~5-10 seconds)
4. Review:
   - Skills to add (normalized)
   - Resume bullet points
   - ATS keywords
5. Ask follow-up questions with context-aware answers

### Using Admin Dashboard (Admin Only)

1. Login with admin credentials (admin@gmail.com / admin)
2. Click "Features" â†’ "Admin Panel"
3. View comprehensive analytics:
   - KPI cards (users, analyses, retention)
   - User growth chart (30 days)
   - Top jobs and missing skills bar charts
   - Match score distribution pie chart
   - Recent activity feed
   - Retention metrics display
4. Manage users (view, suspend, activate, delete)

---

## API Reference

### Authentication

```bash
# Initiate Google OAuth
GET /auth/login

# OAuth callback
GET /auth/callback

# Get current user
GET /users/me
Authorization: Bearer <JWT_TOKEN>
```

### Resume Analysis

```bash
POST /analyze_resume/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume/ \
  -F "file=@resume.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "recommended_job_title": "Full Stack Developer",
  "required_skills": ["python", "django", "react", "docker"],
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "match_percentage": 75.0,
  "live_jobs": [{"title": "...", "company": "...", "link": "..."}],
  "layout_feedback": "âœ… Add professional summary..."
}
```

### AI Career Advisor

```bash
POST /query-career-path/
Content-Type: application/json

{
  "text": "Tell me about DevOps",
  "max_length": 200,
  "temperature": 0.7
}
```

### RAG Coach

```bash
# Upload PDFs
POST /rag-coach/upload
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/rag-coach/upload \
  -F "files=@resume.pdf" \
  -F "files=@job_description.pdf" \
  -F "process_resume_job=true"

# Query
POST /rag-coach/query
Content-Type: application/json

{
  "question": "What skills should I add?",
  "show_context": true
}
```

**Response:**
```json
{
  "answer": "Based on the job description, you should focus on...",
  "context_chunks": [
    {"content": "...", "source": "jd.pdf", "doc_type": "JOB_DESCRIPTION"}
  ],
  "sources": ["resume.pdf", "job_description.pdf"]
}
```

### Resume + JD Analysis

```bash
POST /analyze_resume_with_jd/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume_with_jd/ \
  -F "resume=@resume.pdf" \
  -F "job_description=@jd.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "jd_skills": ["python", "django", "react", "docker", "kubernetes"],
  "matching_skills": ["python", "django", "react"],
  "missing_skills": ["docker", "kubernetes"],
  "ats_score": 60.0,
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "layout_feedback": "âœ… Skills section is ATS-friendly..."
}
```

### History

```bash
# Resume analyses
GET /history/analyses
Authorization: Bearer TOKEN

# Career queries
GET /history/queries
Authorization: Bearer TOKEN

# RAG interactions
GET /history/rag-queries
Authorization: Bearer TOKEN
```

### Admin Dashboard

```bash
# Get comprehensive statistics (admin only)
GET /admin/stats
Authorization: Bearer ADMIN_TOKEN

# Get all users with pagination
GET /admin/users?skip=0&limit=50
Authorization: Bearer ADMIN_TOKEN

# Suspend user
PUT /admin/user/{user_id}/suspend
Authorization: Bearer ADMIN_TOKEN

# Activate user
PUT /admin/user/{user_id}/activate
Authorization: Bearer ADMIN_TOKEN

# Delete user
DELETE /admin/user/{user_id}
Authorization: Bearer ADMIN_TOKEN
```

**Admin Stats Response:**
```json
{
  "total_users": 150,
  "active_users_30days": 85,
  "active_users_7days": 42,
  "new_users_7days": 12,
  "total_analyses": 450,
  "analyses_7days": 67,
  "avg_match_percentage": 72.3,
  "retention_rate": 0.567,
  "retention_7days": 0.714,
  "retention_30days": 0.589,
  "user_growth": [{"date": "2024-11-01", "count": 138}, ...],
  "top_jobs": [{"job": "Software Developer", "count": 89}, ...],
  "top_missing_skills": [{"skill": "docker", "count": 45}, ...],
  "match_distribution": [65.2, 78.5, 82.1, ...],
  "recent_activity": [{"type": "analysis", "user": "user@email.com", ...}],
  "activity_heatmap": [{"day": "Monday", "hour": 14, "count": 23}, ...]
}
```

---

## Deployment

### Local Production Deployment

**Running on Windows Server/VPS:**
```bash
# 1. Install Python 3.10+ and Git
# 2. Clone and setup (same as installation steps)
# 3. Create Windows Service or use Task Scheduler for auto-start

# Run backend as background process
start /B python -m uvicorn backend_api:app --host 0.0.0.0 --port 8000

# Or use a process manager like PM2
npm install -g pm2
pm2 start "uvicorn backend_api:app --host 0.0.0.0 --port 8000" --name nextstepai-backend
```

**Linux/Mac Deployment with systemd:**
```bash
# Create service file: /etc/systemd/system/nextstepai.service
[Unit]
Description=NextStepAI Backend
After=network.target

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/NextStepAI
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/uvicorn backend_api:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target

# Enable and start
sudo systemctl enable nextstepai
sudo systemctl start nextstepai
```

### Database Management

**SQLite Database Location:**
```
nextstepai.db (created automatically in project root)
```

**Backup SQLite Database:**
```bash
# Create backup
copy nextstepai.db nextstepai_backup_2024-10-29.db

# Or use SQLite command
sqlite3 nextstepai.db ".backup nextstepai_backup.db"
```

**Database Schema:**
```python
# Tables created by SQLAlchemy (models.py):
- users
  * id, email, full_name, password_hash
  * role (user/admin), is_active, created_at, last_active
  
- resume_analyses
  * id, owner_id, recommended_job_title, match_percentage
  * skills_to_add, resume_filename, total_skills_count, created_at
  
- career_queries
  * id, owner_id, user_query_text, matched_job_group
  * model_used (finetuned/rag), response_time_seconds, created_at
  
- rag_coach_queries
  * id, owner_id, question, answer, sources
  * query_length, answer_length, created_at
```

### Production Checklist

- âœ… Set strong JWT_SECRET_KEY (64 chars)
- âœ… Backup SQLite database regularly
- âœ… Enable HTTPS with reverse proxy (nginx/Apache)
- âœ… Configure CORS for production domain
- âœ… Set up rate limiting (10 req/min)
- âœ… Enable structured logging
- âœ… Use environment variables (no hardcoded secrets)
- âœ… Monitor disk space (SQLite database growth)
- âœ… Set up automatic backups (daily/weekly)
- âœ… Use process manager (PM2/systemd) for auto-restart

---

## Troubleshooting

### Backend Issues

**Port 8000 in use:**
```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8000 | xargs kill -9
```

**Model loading hangs:**
```bash
# Wait 2-5 minutes or disable fine-tuned model
DISABLE_FINETUNED_MODEL_LOAD=1
```

### RAG Coach Issues

**Ollama model not found:**
```bash
ollama pull tinyllama
ollama list  # Verify
```

**No documents in vector store:**
```bash
curl -X POST http://localhost:8000/rag-coach/build-index
```

### Authentication Issues

**OAuth redirect mismatch:**
- Update Google Console redirect URI
- Must match: `http://localhost:8000/auth/callback`

**JWT token expired:**
- Logout and login again

### Performance Issues

**Slow responses (20+ seconds):**
- Reduce temperature (0.7 â†’ 0.3)
- Reduce max_length (200 â†’ 80)
- Use GPU instead of CPU

**Out of memory:**
- Close other applications
- Reduce chunk size in RAG
- Use quantized models

---

## Project Structure

```
NextStepAI/
â”œâ”€â”€ backend_api.py              # Main FastAPI application (2897 lines)
â”œâ”€â”€ models.py                   # SQLAlchemy database models
â”œâ”€â”€ model_training.py           # ML model training script (Naive Bayes)
â”œâ”€â”€ production_finetuning_optimized.py  # GPT-2 fine-tuning script
â”œâ”€â”€ create_admin.py             # Admin user creation utility
â”œâ”€â”€ requirements.txt            # Python dependencies (191 packages)
â”œâ”€â”€ nextstepai.db              # SQLite database (auto-created)
â”‚
â”œâ”€â”€ Data Files/
â”‚   â”œâ”€â”€ jobs_cleaned.csv           # 8000+ job-skill mappings
â”‚   â”œâ”€â”€ skills_db.json             # 10,000+ validated skills
â”‚   â”œâ”€â”€ career_advice_dataset.jsonl     # 749 career Q&A pairs
â”‚   â”œâ”€â”€ youtube_links.json         # Skill â†’ YouTube tutorial mapping
â”‚   â”œâ”€â”€ job_postings_new.json      # Scraped job postings
â”‚   â””â”€â”€ career_guides.json         # Career advice guides for RAG
â”‚
â”œâ”€â”€ Model Artifacts/
â”‚   â”œâ”€â”€ job_recommender_pipeline.joblib   # TF-IDF + Naive Bayes
â”‚   â”œâ”€â”€ job_title_encoder.joblib          # Label encoder
â”‚   â”œâ”€â”€ prioritized_skills.joblib         # Job â†’ top skills mapping
â”‚   â”œâ”€â”€ master_skill_vocab.joblib         # Complete skill vocabulary
â”‚   â””â”€â”€ career-advisor-final/             # Fine-tuned GPT-2 (1.5GB)
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ tokenizer files
â”‚
â”œâ”€â”€ Vector Stores/
â”‚   â”œâ”€â”€ rag_coach_index/           # FAISS index for uploaded docs
â”‚   â”œâ”€â”€ jobs_index/                # Job postings embeddings
â”‚   â””â”€â”€ guides_index/              # Career guides embeddings
â”‚
â”œâ”€â”€ frontend/                      # React 18 application
â”‚   â”œâ”€â”€ package.json               # 13 dependencies
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                 # Router configuration
â”‚   â”‚   â”œâ”€â”€ index.css              # Global styles (Wozber theme)
â”‚   â”‚   â”œâ”€â”€ theme.js               # Material-UI theme
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Layout.js          # Header + navigation
â”‚   â”‚   â”‚   â”œâ”€â”€ Aurora.js          # WebGL background
â”‚   â”‚   â”‚   â””â”€â”€ ProtectedRoute.js  # Auth guard
â”‚   â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â”‚   â””â”€â”€ AuthContext.js     # JWT state management
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.js       # Home page
â”‚   â”‚   â”‚   â”œâ”€â”€ CVAnalyzer.js      # Resume analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ ResumeAnalyzer.js  # Resume + JD matching
â”‚   â”‚   â”‚   â”œâ”€â”€ CareerAdvisor.js   # AI chat
â”‚   â”‚   â”‚   â”œâ”€â”€ RAGCoach.js        # Document Q&A
â”‚   â”‚   â”‚   â”œâ”€â”€ AdminDashboard.js  # Analytics (600+ lines)
â”‚   â”‚   â”‚   â”œâ”€â”€ History.js         # User history
â”‚   â”‚   â”‚   â”œâ”€â”€ Login.js
â”‚   â”‚   â”‚   â””â”€â”€ Register.js
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ api.js             # Axios API client
â”‚   â””â”€â”€ build/                     # Production build
â”‚
â””â”€â”€ uploads/
    â””â”€â”€ processed/                 # Temporary uploaded files

Key Files Overview:
- backend_api.py: 22 endpoints (auth, CV analysis, RAG, admin)
- models.py: 4 database tables with relationships
- model_training.py: GridSearchCV, confusion matrix, artifact generation
- AdminDashboard.js: 8+ visualizations with Recharts
```

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

**Code Style:**
- Follow PEP 8 for Python
- Use type hints
- Add docstrings
- Keep lines <100 characters

---

## License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file.

---

## Acknowledgments

**Technologies:**
- [HuggingFace Transformers](https://huggingface.co/) - LLM infrastructure
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [React](https://reactjs.org/) - Frontend library
- [Material-UI](https://mui.com/) - UI components
- [OGL](https://github.com/oframe/ogl) - WebGL library (Aurora)
- [LangChain](https://python.langchain.com/) - RAG orchestration
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [Ollama](https://ollama.ai/) - Local LLM inference

**Data:**
- Google Gemini - Skill extraction
- all-MiniLM-L6-v2 - Sentence embeddings
- GPT-2-Medium - Base model
- Scikit-learn - ML utilities

---

## Contact

**Author:** Arjun T Anil  
**GitHub:** [@arjuntanil](https://github.com/arjuntanil)  
**Repository:** [NextStep-AI](https://github.com/arjuntanil/NextStep-AI)  

**Support:**
- ğŸ› [Report Bugs](https://github.com/arjuntanil/NextStep-AI/issues)
- ğŸ’¡ [Request Features](https://github.com/arjuntanil/NextStep-AI/discussions)

---

<div align="center">

**â­ Star this project if you find it helpful! â­**

Made with â¤ï¸ by [Arjun T Anil](https://github.com/arjuntanil)

</div>
