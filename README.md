# NextStepAI: AI-Powered Career Navigator üöÄ

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2+-61dafb.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üìã Table of Contents
- [Overview](#overview)
- [Why NextStepAI?](#why-nextstepai)
- [Core Features](#core-features)
- [Technology Stack](#technology-stack)
- [AI Models & Parameters](#ai-models--parameters)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
- [API Reference](#api-reference)
- [Deployment](#deployment)

---

## Overview

**NextStepAI** is a production-ready AI career coaching platform that combines **Machine Learning**, **Fine-tuned LLMs**, and **Retrieval-Augmented Generation (RAG)** to deliver personalized career guidance. The platform features a modern **React frontend with Aurora WebGL animation** and a high-performance **FastAPI backend**.

### What Problem Does This Solve?

**The Challenge:**
- 90%+ of resumes are filtered by ATS before reaching recruiters
- Job seekers struggle to identify skill gaps for career advancement
- Generic career advice lacks personalization and data-driven insights
- Information overload without intelligent matching

**The Solution:**
- üîç **AI Resume Analysis** - Extract skills, predict jobs, identify gaps
- ü§ñ **Fine-Tuned Career Advisor** - GPT-2 trained on 749 career examples
- üìö **RAG Coach** - Upload PDFs for personalized Q&A
- üéØ **Live Job Matching** - Real-time LinkedIn job scraping
- üé® **Modern UI** - React with Aurora WebGL effects

---

## Why NextStepAI?

### Key Innovations

| Feature | Technology | Impact |
|---------|-----------|--------|
| **CV Analysis** | Multinomial Naive Bayes + Scikit-learn | 85% job prediction accuracy |
| **Career Advisor** | Fine-tuned GPT-2 (355M params) | Context-aware career guidance |
| **RAG Coach** | TinyLLama 1.1B + FAISS | Privacy-first document Q&A |
| **Job Scraping** | BeautifulSoup + LinkedIn | Real-time opportunities |
| **Aurora UI** | React + OGL (WebGL) | GPU-accelerated animations |

### Why This Matters

‚úÖ **Personalized** - AI analyzes YOUR resume against YOUR target jobs  
‚úÖ **Data-Driven** - 8000+ job mappings, 749 career examples, live job data  
‚úÖ **Privacy-First** - TinyLLama runs locally (no API calls for RAG)  
‚úÖ **Production-Ready** - JWT auth, SQLite/PostgreSQL, comprehensive logging  
‚úÖ **Modern UI** - Dark theme, glassmorphism, Aurora WebGL background  

---

## Core Features

### 1. üìÑ CV Analyzer

**Upload your resume and receive comprehensive AI-powered analysis**

**Workflow:**
```
PDF/DOCX Upload ‚Üí Skill Extraction ‚Üí ML Job Prediction ‚Üí 
Gap Analysis ‚Üí ATS Feedback ‚Üí LinkedIn Job Scraping ‚Üí YouTube Tutorials
```

**Output:**
- ‚úÖ Recommended job title with 85% accuracy
- ‚úÖ Match percentage (your skills vs. required)
- ‚úÖ Skills to learn with YouTube tutorials
- ‚úÖ Live LinkedIn job postings
- ‚úÖ ATS optimization feedback

**Technologies:**
- **Skill Extraction:** Google Gemini Pro (with RegEx fallback)
- **Job Classification:** TF-IDF + Naive Bayes (8000+ training examples)
- **Job Scraping:** BeautifulSoup4 (LinkedIn India)
- **File Parsing:** pdfplumber, python-docx

---

### 2. ü§ñ AI Career Advisor

**Chat with a fine-tuned GPT-2 model trained on career counseling**

**How It Works:**
1. **Primary:** Fine-tuned GPT-2-Medium generates personalized advice
2. **Fallback:** RAG system over curated career guides
3. **Enhancement:** Live job postings for recommended roles

**Model Training Details:**
```
Base Model: GPT-2-Medium (355M parameters)
Training Data: 749 career examples (train/val: 80/20)
Epochs: 15
Learning Rate: 1e-5
Batch Size: 2 (gradient accumulation: 8)
Max Length: 512 tokens
Training Time: 15-20 min (GPU) / 6+ hours (CPU)
Final Loss: 0.87
```

**Example Output:**
```
Question: "Tell me about DevOps careers"

Answer: 
### Key Skills:
‚Ä¢ Docker, Kubernetes, CI/CD
‚Ä¢ AWS/Azure/GCP cloud platforms
‚Ä¢ Linux, Bash scripting

### Top Certifications:
‚Ä¢ AWS Certified DevOps Engineer
‚Ä¢ Certified Kubernetes Administrator (CKA)

### Salary Range: ‚Çπ8-20 LPA (India)
[+ Live job postings]
```

---

### 3. üìö RAG Coach

**Upload resume + job description PDFs for personalized guidance**

**Workflow:**
```
Upload PDFs ‚Üí Auto Document Detection ‚Üí FAISS Indexing ‚Üí 
Auto Analysis ‚Üí Interactive Q&A
```

**RAG System Parameters:**
```python
LLM: TinyLLama 1.1B (Q4 quantized)
  - Memory: ~4GB RAM
  - Inference: ~50 tokens/sec (CPU)
  - Privacy: 100% local execution

Embeddings: all-MiniLM-L6-v2
  - Dimensions: 384
  - Model Size: 90MB

Vector Store: FAISS
  - Index Type: IndexFlatL2
  - Distance Metric: L2 Euclidean
  - Top-K Retrieval: 4 chunks

Chunking Strategy:
  - Chunk Size: 500 characters
  - Overlap: 50 characters
  - Splitter: RecursiveCharacterTextSplitter
```

**Auto-Generated Analysis:**
- ‚úÖ Skills to add (with synonym normalization)
- ‚úÖ Resume enhancement bullet points
- ‚úÖ ATS-friendly keywords
- ‚úÖ Interactive Q&A with source attribution

**Key Innovation:**
- **Skill Normalization:** 50+ synonym mappings (React.js ‚Üí react, PostgreSQL ‚Üí postgres)
- **Document Detection:** Content-based classification (resume vs JD)
- **Query Intent:** Filters context based on question type

---

## Technology Stack

### Frontend

**React 18 + Material-UI v5 + Aurora WebGL**

```javascript
Core Libraries:
  - React 18.2.0 (hooks, context API)
  - Material-UI v5 (components, theming)
  - React Router v6 (client-side routing)
  - Axios 1.4.0 (HTTP client with JWT)
  - Recharts 2.5.0 (data visualization)
  - OGL (WebGL library for Aurora)

Aurora Effect Implementation:
  - Technology: WebGL shaders (vertex + fragment)
  - Library: OGL (7KB, GPU-accelerated)
  - Color Scheme: Red (#dc2626) ‚Üí Orange (#f59e0b) ‚Üí Black (#000000)
  - Performance: 60 FPS at 1080p
  - Configuration:
    * Amplitude: 1.5
    * Blend: 0.8
    * Speed: 0.4
    * Noise: Simplex noise algorithm

UI Features:
  - Glassmorphism: rgba backgrounds + backdrop-filter blur
  - Dark Theme: Pure black (#000000) with red/orange accents
  - Protected Routes: JWT-based authentication guards
  - Responsive Design: Mobile-first with MUI breakpoints
```

**Aurora Technical Details:**
```javascript
// Vertex Shader (GLSL 3.0)
#version 300 es
in vec2 position;
void main() {
  gl_Position = vec4(position, 0.0, 1.0);
}

// Fragment Shader (Simplex Noise)
- Generates animated gradient using GPU
- Color interpolation between 3 stops
- Time-based animation (uTime uniform)
- Resolution-aware (uResolution uniform)
```

---

### Backend

**FastAPI + Python 3.10 + Async Architecture**

```python
Core Framework:
  - FastAPI 0.104.1 (async ASGI)
  - Uvicorn 0.24.0 (ASGI server)
  - Pydantic 2.5.0 (data validation)

Authentication:
  - python-jose 3.3.0 (JWT tokens)
  - passlib 1.7.4 (bcrypt hashing)
  - Google OAuth 2.0 (SSO)

Database:
  - SQLAlchemy 2.0.23 (ORM)
  - SQLite 3 (file-based, zero configuration)

API Features:
  - RESTful endpoints (/api/cv/*, /api/rag/*)
  - WebSocket support (/ws/chat)
  - CORS enabled (localhost:3000, localhost:8501)
  - Lazy model loading (background threads)
  - Comprehensive logging
```

---

### AI & Machine Learning

**LLMs:**
```
1. Google Gemini Pro (API)
   - Purpose: Skill extraction, ATS feedback
   - Model: gemini-1.5-pro
   - Context Window: 32k tokens
   - Temperature: 0.3 (consistent outputs)

2. GPT-2 Medium (Fine-tuned)
   - Purpose: Career advice generation
   - Parameters: 355 million
   - Size: 1.5GB
   - Training: 749 examples, 15 epochs
   - Format: PyTorch (HuggingFace)

3. TinyLLama 1.1B (RAG)
   - Purpose: Document Q&A
   - Parameters: 1.1 billion
   - Quantization: Q4_K_M (4-bit)
   - Memory: ~4GB RAM
   - Inference: CPU-friendly (~50 tokens/sec)
   - Privacy: 100% local execution
```

**ML Models:**
```python
Job Classification:
  - Algorithm: Multinomial Naive Bayes
  - Feature Engineering: TF-IDF Vectorization
  - Training Data: 8000+ job-skill mappings (jobs_cleaned.csv)
  - Accuracy: ~85% on test set
  - Inference Time: <10ms
  - Model Size: 450KB

  Categories (10 groups):
    ‚Ä¢ Data Professional
    ‚Ä¢ Software Developer
    ‚Ä¢ IT Operations & Infrastructure
    ‚Ä¢ Project/Product Manager
    ‚Ä¢ QA/Test Engineer
    ‚Ä¢ Human Resources
    ‚Ä¢ Sales & Business Development
    ‚Ä¢ Administrative & Support
    ‚Ä¢ Technical Writer
    ‚Ä¢ Other

  Hyperparameters (GridSearchCV):
    ‚Ä¢ Vectorizer: TfidfVectorizer(max_features=500)
    ‚Ä¢ Classifier: MultinomialNB(alpha=1.0)
    ‚Ä¢ Train/Test Split: 80/20
```

**RAG System:**
```python
Vector Database: FAISS
  - Index Type: IndexFlatL2 (exact search)
  - Distance Metric: L2 Euclidean
  - Persistent Storage: ./rag_data/faiss_index

Embeddings: HuggingFace Sentence Transformers
  - Model: all-MiniLM-L6-v2
  - Dimensions: 384
  - Normalization: L2 norm
  - Batch Size: 32

Document Processing:
  - Loader: PyPDFLoader (LangChain)
  - Chunking: RecursiveCharacterTextSplitter
    * chunk_size: 500
    * chunk_overlap: 50
    * separators: ["\n\n", "\n", ". ", " "]
  - Metadata: source, doc_type, page, doc_index

Retrieval:
  - Strategy: Similarity search
  - Top-K: 4 chunks per query
  - Filtering: Document type (resume vs JD)
  - Context Window: 2048 tokens (TinyLLama)
```

**Document Parsing:**
```
PDF: pdfplumber 0.10.3 (complex layouts)
DOCX: python-docx 1.1.0 (paragraph extraction)
Text Splitting: langchain 0.0.335
```

---

## AI Models & Parameters

### Fine-Tuned LLM (Career Advisor)

**Training Configuration:**
```python
# Base Model
model_name = "gpt2-medium"
num_parameters = 355_000_000
architecture = "Transformer decoder (24 layers, 16 attention heads)"

# Training Data
train_dataset = "career_advice_dataset.jsonl + career_advice_ultra_clear_dataset.jsonl"
total_examples = 749
train_val_split = "80/20 (599 train, 150 val)"

# Training Hyperparameters
training_args = {
    "num_train_epochs": 15,
    "learning_rate": 1e-5,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,  # Effective batch = 16
    "max_seq_length": 512,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "fp16": True,  # Mixed precision (GPU only)
    "logging_steps": 50,
    "save_strategy": "epoch",
    "evaluation_strategy": "epoch"
}

# Training Results
total_steps = 1500  # ~100 steps/epoch
training_time_gpu = "15-20 minutes (RTX 2050)"
training_time_cpu = "6+ hours (not recommended)"
final_loss = 0.87
validation_perplexity = 2.39

# Generation Config
generation_config = {
    "max_length": 200,
    "temperature": 0.7,  # 0.5-0.9 for coherence
    "top_p": 0.9,        # Nucleus sampling
    "top_k": 50,         # Top-k sampling
    "repetition_penalty": 1.2,
    "do_sample": True
}
```

**Dataset Structure:**
```json
{
  "prompt": "What skills are required for a Data Scientist in India?",
  "completion": "### Key Skills:\n* Python, R, SQL\n* ML: Regression, Trees, Deep Learning\n\n### Certifications:\n* Google Cloud Professional Data Engineer\n\n### Salary: ‚Çπ8-25 LPA"
}
```

---

### RAG System (Document Q&A)

**TinyLLama Configuration:**
```python
# Model Setup (Ollama)
model_name = "tinyllama"
full_name = "TinyLlama-1.1B-Chat-v1.0"
parameters = 1_100_000_000
quantization = "Q4_K_M (4-bit)"
model_size = "637MB download"
memory_usage = "~4GB RAM"

# Inference Performance
tokens_per_second_cpu = 50
tokens_per_second_gpu = 150
context_window = 2048
temperature = 0.7
max_tokens = 512
top_p = 0.9
repeat_penalty = 1.1

# Privacy
execution = "100% local (no API calls)"
data_retention = "Zero (PDFs can be deleted post-indexing)"
```

**FAISS Vector Store:**
```python
# Index Configuration
index_type = "IndexFlatL2"  # Exact search, no compression
distance_metric = "L2 Euclidean distance"
dimension = 384  # all-MiniLM-L6-v2 embedding size

# Storage
index_path = "./rag_data/faiss_index"
persistence = "Disk (loaded on startup)"
index_size = "~50MB for 1000 chunks"

# Retrieval Parameters
search_type = "similarity"
k = 4  # Top-4 chunks
score_threshold = 0.7  # Minimum similarity
return_metadata = True  # source, doc_type, page
```

**Embedding Model:**
```python
# HuggingFace Sentence Transformers
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_size = "90MB"
embedding_dimension = 384
max_sequence_length = 256
normalization = "L2 norm"
device = "cpu"  # Lightweight, no GPU needed

# Performance
encoding_speed = "~1000 sentences/sec (CPU)"
batch_size = 32
```

---

### ML Model Training (Job Classification)

**Model Training Script:** `model_training.py`

```python
# Data Loading
dataset = pd.read_csv("jobs_cleaned.csv")
total_records = 8000+
columns = ["Job Title", "Skills", "Grouped_Title"]

# Preprocessing
skill_validation = "skills_db.json (10,000+ valid skills)"
job_consolidation = "54 unique titles ‚Üí 10 groups"
normalization = "lowercase, strip whitespace"
train_test_split = "80/20"

# Feature Engineering
vectorizer = TfidfVectorizer(
    max_features=500,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)

# Model Selection (GridSearchCV)
algorithms_tested = [
    "MultinomialNB",
    "LogisticRegression"
]
cv_folds = 5
best_algorithm = "MultinomialNB(alpha=1.0)"

# Training
fit_time = "~2 minutes"
accuracy = 0.85
precision = 0.83
recall = 0.82
f1_score = 0.82

# Output Artifacts
saved_models = [
    "job_recommender_pipeline.joblib",     # TF-IDF + NB
    "job_title_encoder.joblib",            # LabelEncoder
    "prioritized_skills.joblib",           # Job ‚Üí skills mapping
    "master_skill_vocab.joblib"            # Complete vocabulary
]
```

---

## How It Works

### 1. CV Analyzer - Complete Pipeline

**Stage-by-Stage Breakdown:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 1: File Upload & Text Extraction             ‚îÇ
‚îÇ ‚îú‚îÄ PDF: pdfplumber (handles complex layouts)       ‚îÇ
‚îÇ ‚îî‚îÄ DOCX: python-docx (paragraph extraction)        ‚îÇ
‚îÇ Output: Plain text (500-5000 chars)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 2: AI Skill Extraction                       ‚îÇ
‚îÇ ‚îú‚îÄ Primary: Gemini LLM (contextual NER)            ‚îÇ
‚îÇ ‚îÇ   - Prompt: Extract technical skills, tools      ‚îÇ
‚îÇ ‚îÇ   - Temperature: 0.3 (consistency)               ‚îÇ
‚îÇ ‚îÇ   - Time: 2-4 seconds                            ‚îÇ
‚îÇ ‚îî‚îÄ Fallback: 9 RegEx patterns (100+ technologies)  ‚îÇ
‚îÇ Output: ["python", "django", "mysql", "git"]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 3: ML Job Prediction                         ‚îÇ
‚îÇ ‚îú‚îÄ TF-IDF Vectorization (skills ‚Üí numeric vector)  ‚îÇ
‚îÇ ‚îú‚îÄ Naive Bayes Classification (P(Job|Skills))      ‚îÇ
‚îÇ ‚îî‚îÄ LabelEncoder (decode to job title)              ‚îÇ
‚îÇ Output: "Software Developer" (85% accuracy)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 4: Skill Gap Analysis                        ‚îÇ
‚îÇ ‚îú‚îÄ Retrieve required skills from database          ‚îÇ
‚îÇ ‚îú‚îÄ Set operations (required - user_skills)         ‚îÇ
‚îÇ ‚îî‚îÄ Calculate match percentage                      ‚îÇ
‚îÇ Output: Match 42%, Missing ["docker", "k8s"]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 5: AI Layout Feedback                        ‚îÇ
‚îÇ ‚îú‚îÄ Primary: Gemini LLM (ATS optimization)          ‚îÇ
‚îÇ ‚îî‚îÄ Fallback: 7 rule-based checks                   ‚îÇ
‚îÇ Output: "‚úÖ Add professional summary"               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 6: Live Job Scraping                         ‚îÇ
‚îÇ ‚îú‚îÄ LinkedIn job search (India)                     ‚îÇ
‚îÇ ‚îú‚îÄ BeautifulSoup parsing (3 fallback selectors)    ‚îÇ
‚îÇ ‚îî‚îÄ Browser emulation headers                       ‚îÇ
‚îÇ Output: Top 5 jobs with links                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 7: YouTube Tutorial Mapping                  ‚îÇ
‚îÇ ‚îú‚îÄ JSON lookup (youtube_links.json)                ‚îÇ
‚îÇ ‚îî‚îÄ Map each missing skill to tutorial              ‚îÇ
‚îÇ Output: Skill ‚Üí YouTube link                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 8: Database Storage (if logged in)           ‚îÇ
‚îÇ ‚îú‚îÄ SQLAlchemy ORM                                  ‚îÇ
‚îÇ ‚îî‚îÄ Save to ResumeAnalysis table                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 9: Return JSON Response                      ‚îÇ
‚îÇ Total Time: 8-12 seconds end-to-end                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Performance Metrics:**

| Stage | Time | Notes |
|-------|------|-------|
| Text Extraction | <1s | Typical 2-page resume |
| Skill Extraction | 2-4s | Gemini API latency |
| Job Prediction | <0.1s | Pre-trained ML model |
| Gap Analysis | <0.01s | Python set operations |
| Layout Feedback | 2-3s | Or instant (fallback) |
| Job Scraping | 2-4s | Network-dependent |
| YouTube Mapping | <0.01s | In-memory lookup |
| **TOTAL** | **8-12s** | **Full pipeline** |

---

### 2. AI Career Advisor - Dual System

**Workflow:**

```
User Query: "Tell me about DevOps"
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Check Model  ‚îÇ
    ‚îÇ   Status     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Model Loaded?   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       /           \
    YES             NO
     ‚Üì               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇFine-tuned‚îÇ   ‚îÇ   RAG    ‚îÇ
‚îÇ  GPT-2   ‚îÇ   ‚îÇ  System  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì               ‚Üì
   Generate      Retrieve
   Response      Context
     ‚Üì               ‚Üì
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Enhance with    ‚îÇ
    ‚îÇ Live Jobs       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Return JSON     ‚îÇ
    ‚îÇ Response        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fine-Tuned Model Path:**
```python
# Generation Process
input_text = "### Question: Tell me about DevOps\n\n### Answer:"
tokenized = tokenizer(input_text, return_tensors="pt")
output = model.generate(
    tokenized.input_ids,
    max_length=200,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.2
)
response = tokenizer.decode(output[0], skip_special_tokens=True)
# Returns: Structured advice with skills, certs, salary
```

**RAG Fallback Path:**
```python
# Retrieval Process
query_embedding = embeddings.embed_query(user_question)
relevant_docs = faiss_index.similarity_search(query_embedding, k=4)
context = "\n\n".join([doc.page_content for doc in relevant_docs])

# Generation
prompt = f"Context: {context}\n\nQuestion: {user_question}\n\nAnswer:"
response = gemini_llm.invoke(prompt)
# Returns: Context-aware answer from career guides
```

---

### 3. RAG Coach - Document Intelligence

**Complete Workflow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 1: Upload & Detection            ‚îÇ
‚îÇ ‚îú‚îÄ User uploads resume.pdf + jd.pdf    ‚îÇ
‚îÇ ‚îú‚îÄ Content analysis (keywords)          ‚îÇ
‚îÇ ‚îÇ   Resume: "experience", "education"   ‚îÇ
‚îÇ ‚îÇ   JD: "requirements", "qualifications"‚îÇ
‚îÇ ‚îî‚îÄ Tag metadata: doc_type, source      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 2: Chunking & Embedding          ‚îÇ
‚îÇ ‚îú‚îÄ RecursiveCharacterTextSplitter      ‚îÇ
‚îÇ ‚îÇ   - chunk_size: 500                  ‚îÇ
‚îÇ ‚îÇ   - overlap: 50                      ‚îÇ
‚îÇ ‚îú‚îÄ all-MiniLM-L6-v2 embeddings         ‚îÇ
‚îÇ ‚îî‚îÄ 384-dim vectors                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 3: FAISS Indexing                ‚îÇ
‚îÇ ‚îú‚îÄ IndexFlatL2 (exact search)          ‚îÇ
‚îÇ ‚îú‚îÄ Store metadata with each chunk      ‚îÇ
‚îÇ ‚îî‚îÄ Persistent storage (disk)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 4: Auto Skill Analysis           ‚îÇ
‚îÇ ‚îú‚îÄ Extract skills from both docs       ‚îÇ
‚îÇ ‚îú‚îÄ Normalize (React.js ‚Üí react)        ‚îÇ
‚îÇ ‚îú‚îÄ Set operations (JD - Resume)        ‚îÇ
‚îÇ ‚îî‚îÄ Generate bullet points              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 5: Interactive Q&A               ‚îÇ
‚îÇ ‚îú‚îÄ User query embedding                 ‚îÇ
‚îÇ ‚îú‚îÄ FAISS similarity search (top-4)     ‚îÇ
‚îÇ ‚îú‚îÄ Filter by doc_type (if needed)      ‚îÇ
‚îÇ ‚îú‚îÄ TinyLLama generation                ‚îÇ
‚îÇ ‚îî‚îÄ Return answer + sources             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Skill Normalization (Key Innovation):**
```python
# 50+ synonym mappings
synonym_map = {
    'react.js': 'react',
    'reactjs': 'react',
    'node.js': 'nodejs',
    'postgresql': 'postgres',
    'sqlite3': 'sqlite',
    'restful api': 'rest api',
    'ci/cd': 'cicd',
    'oop': 'object-oriented programming',
    # ... 40+ more
}

# Before: 24 "missing" skills (false positives)
# After: 4 actual missing skills
# Accuracy improvement: 83% reduction in false positives
```

**Query Intent Detection:**
```python
# Filter context by question type
if "job description" in query.lower():
    filter_docs(doc_type="JOB_DESCRIPTION")
elif "my resume" in query.lower():
    filter_docs(doc_type="RESUME")
else:
    use_all_docs()
```

---

## Installation

### Quick Start (5 Minutes)

```bash
# 1. Clone repository
git clone https://github.com/arjuntanil/NextStep-AI.git
cd NextStep-AI

# 2. Create virtual environment
python -m venv career_coach
career_coach\Scripts\activate  # Windows
# source career_coach/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env: Add GOOGLE_API_KEY, JWT_SECRET_KEY

# 5. Train models
python model_training.py

# 6. Build RAG indexes
python ingest_guides.py

# 7. Install Ollama (for RAG Coach)
# Download from https://ollama.ai
ollama pull tinyllama

# 8. Start backend
python -m uvicorn backend_api:app --reload

# 9. Start frontend (new terminal)
# Option A: React (modern UI with Aurora)
cd frontend && npm install && npm run dev

# Option B: Streamlit (simple UI)
streamlit run app.py
```

### React Frontend Setup

```bash
cd frontend

# Install dependencies (~1-2 minutes, 1406 packages)
npm install

# Start development server
npm run dev

# Access at: http://localhost:3000
```

**React Frontend Features:**
- üåô Dark theme with glassmorphism
- ‚ö° Aurora WebGL background (GPU-accelerated)
- üîê JWT authentication with protected routes
- üì± Mobile-responsive Material-UI components
- üé® Red/Orange color scheme

---

### Environment Variables

**.env Configuration:**
```env
# Required
GOOGLE_API_KEY=AIzaSy...  # Get from https://makersuite.google.com/app/apikey
JWT_SECRET_KEY=<64-char-hex>  # python -c "import secrets; print(secrets.token_hex(32))"

# Optional (for Google OAuth login)
GOOGLE_CLIENT_ID=your_google_oauth_client_id
GOOGLE_CLIENT_SECRET=your_google_oauth_client_secret

# Frontend URL
STREAMLIT_FRONTEND_URL=http://localhost:8501
```

---

### Fine-Tune Career Advisor (Optional)

**Option 1: Local Training (GPU Recommended)**
```bash
python production_finetuning_optimized.py
# Time: 15-20 min (GPU) / 6+ hours (CPU)
# Output: career-advisor-final/ directory (1.5GB)
```

**Option 2: Skip Training (Use RAG Only)**
The system will automatically fall back to the RAG system if the fine-tuned model is not found. This is perfectly functional for most use cases.

---

## Usage

### Access URLs

| Service | URL | Description |
|---------|-----|-------------|
| **React Frontend** | http://localhost:3000 | Modern UI with Aurora |
| **Streamlit Frontend** | http://localhost:8501 | Data-centric UI |
| **Backend API** | http://localhost:8000 | FastAPI server |
| **API Docs** | http://localhost:8000/docs | Swagger UI |

### Using CV Analyzer

1. Navigate to "Resume Analyzer" tab
2. Upload resume (PDF/DOCX)
3. Wait 8-12 seconds for analysis
4. Review:
   - Recommended job title
   - Match percentage
   - Skills to learn (with YouTube links)
   - Live LinkedIn jobs
   - ATS feedback

### Using AI Career Advisor

1. Navigate to "AI Career Advisor" tab
2. Ask question (e.g., "Tell me about Data Science")
3. Adjust temperature (0.1-1.0) and length (50-120)
4. Click "Get AI Advice"
5. Review structured response + live jobs

### Using RAG Coach

1. Navigate to "RAG Coach" tab
2. Upload resume.pdf + job_description.pdf
3. Wait for auto-analysis (~5-10 seconds)
4. Review:
   - Skills to add
   - Resume bullet points
   - ATS keywords
5. Ask follow-up questions

---

## API Reference

### Authentication

```bash
# Initiate Google OAuth
GET /auth/login

# OAuth callback
GET /auth/callback

# Get current user
GET /users/me
Authorization: Bearer <JWT_TOKEN>
```

### Resume Analysis

```bash
POST /analyze_resume/
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/analyze_resume/ \
  -F "file=@resume.pdf" \
  -H "Authorization: Bearer TOKEN"
```

**Response:**
```json
{
  "resume_skills": ["python", "django", "react"],
  "recommended_job_title": "Full Stack Developer",
  "required_skills": ["python", "django", "react", "docker"],
  "missing_skills_with_links": [
    {"skill_name": "docker", "youtube_link": "https://..."}
  ],
  "match_percentage": 75.0,
  "live_jobs": [{"title": "...", "company": "...", "link": "..."}],
  "layout_feedback": "‚úÖ Add professional summary..."
}
```

### AI Career Advisor

```bash
POST /query-career-path/
Content-Type: application/json

{
  "text": "Tell me about DevOps",
  "max_length": 200,
  "temperature": 0.7
}
```

### RAG Coach

```bash
# Upload PDFs
POST /rag-coach/upload
Content-Type: multipart/form-data

curl -X POST http://localhost:8000/rag-coach/upload \
  -F "files=@resume.pdf" \
  -F "files=@job_description.pdf" \
  -F "process_resume_job=true"

# Query
POST /rag-coach/query
Content-Type: application/json

{
  "question": "What skills should I add?",
  "show_context": true
}
```

**Response:**
```json
{
  "answer": "Based on the job description, you should focus on...",
  "context_chunks": [
    {"content": "...", "source": "jd.pdf", "doc_type": "JOB_DESCRIPTION"}
  ],
  "sources": ["resume.pdf", "job_description.pdf"]
}
```

### History

```bash
# Resume analyses
GET /history/analyses
Authorization: Bearer TOKEN

# Career queries
GET /history/queries
Authorization: Bearer TOKEN

# RAG interactions
GET /history/rag-queries
Authorization: Bearer TOKEN
```

---

## Deployment

### Local Production Deployment

**Running on Windows Server/VPS:**
```bash
# 1. Install Python 3.10+ and Git
# 2. Clone and setup (same as installation steps)
# 3. Create Windows Service or use Task Scheduler for auto-start

# Run backend as background process
start /B python -m uvicorn backend_api:app --host 0.0.0.0 --port 8000

# Or use a process manager like PM2
npm install -g pm2
pm2 start "uvicorn backend_api:app --host 0.0.0.0 --port 8000" --name nextstepai-backend
```

**Linux/Mac Deployment with systemd:**
```bash
# Create service file: /etc/systemd/system/nextstepai.service
[Unit]
Description=NextStepAI Backend
After=network.target

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/NextStepAI
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/uvicorn backend_api:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target

# Enable and start
sudo systemctl enable nextstepai
sudo systemctl start nextstepai
```

### Database Management

**SQLite Database Location:**
```
nextstepai.db (created automatically in project root)
```

**Backup SQLite Database:**
```bash
# Create backup
copy nextstepai.db nextstepai_backup_2024-10-29.db

# Or use SQLite command
sqlite3 nextstepai.db ".backup nextstepai_backup.db"
```

**Database Schema:**
```python
# Tables created by SQLAlchemy:
- users (id, email, full_name, created_at)
- resume_analyses (id, owner_id, job_title, match_percentage, skills_to_add, created_at)
- career_queries (id, owner_id, query_text, matched_job_group, created_at)
- rag_coach_queries (id, owner_id, question, answer, sources, created_at)
```

### Production Checklist

- ‚úÖ Set strong JWT_SECRET_KEY (64 chars)
- ‚úÖ Backup SQLite database regularly
- ‚úÖ Enable HTTPS with reverse proxy (nginx/Apache)
- ‚úÖ Configure CORS for production domain
- ‚úÖ Set up rate limiting (10 req/min)
- ‚úÖ Enable structured logging
- ‚úÖ Use environment variables (no hardcoded secrets)
- ‚úÖ Monitor disk space (SQLite database growth)
- ‚úÖ Set up automatic backups (daily/weekly)
- ‚úÖ Use process manager (PM2/systemd) for auto-restart

---

## Troubleshooting

### Backend Issues

**Port 8000 in use:**
```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8000 | xargs kill -9
```

**Model loading hangs:**
```bash
# Wait 2-5 minutes or disable fine-tuned model
DISABLE_FINETUNED_MODEL_LOAD=1
```

### RAG Coach Issues

**Ollama model not found:**
```bash
ollama pull tinyllama
ollama list  # Verify
```

**No documents in vector store:**
```bash
curl -X POST http://localhost:8000/rag-coach/build-index
```

### Authentication Issues

**OAuth redirect mismatch:**
- Update Google Console redirect URI
- Must match: `http://localhost:8000/auth/callback`

**JWT token expired:**
- Logout and login again

### Performance Issues

**Slow responses (20+ seconds):**
- Reduce temperature (0.7 ‚Üí 0.3)
- Reduce max_length (200 ‚Üí 80)
- Use GPU instead of CPU

**Out of memory:**
- Close other applications
- Reduce chunk size in RAG
- Use quantized models

---

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

**Code Style:**
- Follow PEP 8 for Python
- Use type hints
- Add docstrings
- Keep lines <100 characters

---

## License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file.

---

## Acknowledgments

**Technologies:**
- [HuggingFace Transformers](https://huggingface.co/) - LLM infrastructure
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [React](https://reactjs.org/) - Frontend library
- [Material-UI](https://mui.com/) - UI components
- [OGL](https://github.com/oframe/ogl) - WebGL library (Aurora)
- [LangChain](https://python.langchain.com/) - RAG orchestration
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [Ollama](https://ollama.ai/) - Local LLM inference

**Data:**
- Google Gemini - Skill extraction
- all-MiniLM-L6-v2 - Sentence embeddings
- GPT-2-Medium - Base model
- Scikit-learn - ML utilities

---

## Contact

**Author:** Arjun T Anil  
**GitHub:** [@arjuntanil](https://github.com/arjuntanil)  
**Repository:** [NextStep-AI](https://github.com/arjuntanil/NextStep-AI)  

**Support:**
- üêõ [Report Bugs](https://github.com/arjuntanil/NextStep-AI/issues)
- üí° [Request Features](https://github.com/arjuntanil/NextStep-AI/discussions)

---

<div align="center">

**‚≠ê Star this project if you find it helpful! ‚≠ê**

Made with ‚ù§Ô∏è by [Arjun T Anil](https://github.com/arjuntanil)

</div>
