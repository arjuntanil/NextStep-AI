"""
Production-Grade Career Advisor LLM Fine-Tuning
Optimized for FAST and ACCURATE results

Configuration:
- Epochs: 3 (faster learning)
- Learning Rate: 5e-5 (optimal convergence)
- Model: GPT-2 Medium (355M params) for better accuracy
- Smart training with early stopping
"""

import json
import torch
from transformers import (
    GPT2LMHeadModel, 
    GPT2Tokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from datasets import Dataset
from pathlib import Path
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class ProductionCareerAdvisorTrainer:
    """Fast, accurate fine-tuning for career advice"""
    
    def __init__(self):
        # Use GPT-2 medium for better quality (still fast on CPU)
        self.model_name = "gpt2-medium"  # 355M params - much better than gpt2
        self.output_dir = Path("./career-advisor-production-v2")
        self.tokenizer = None
        self.model = None
        
    def load_training_data(self):
        """Load high-quality career advice data"""
        logger.info("üìö Loading training data...")
        
        data_files = [
            "career_advice_dataset.jsonl",
            "career_advice_ultra_clear_dataset.jsonl"
        ]
        
        all_examples = []
        for file_path in data_files:
            if Path(file_path).exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            example = json.loads(line.strip())
                            if example.get('prompt') and example.get('completion'):
                                all_examples.append(example)
                        except:
                            continue
        
        logger.info(f"‚úÖ Loaded {len(all_examples)} training examples")
        return all_examples
    
    def format_for_training(self, prompt, completion):
        """Format with clear structure for learning"""
        # Clean and concise format
        return f"Question: {prompt}\n\nAnswer: {completion}<|endoftext|>"
    
    def prepare_dataset(self, examples):
        """Prepare tokenized dataset"""
        logger.info("üîß Preparing dataset for training...")
        
        # Initialize tokenizer
        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Format all examples
        formatted_texts = [
            self.format_for_training(ex['prompt'], ex['completion']) 
            for ex in examples
        ]
        
        # Tokenize efficiently
        logger.info("   Tokenizing...")
        tokenized = self.tokenizer(
            formatted_texts,
            truncation=True,
            padding='max_length',
            max_length=384,  # Shorter for faster training
            return_tensors='pt'
        )
        
        # Create dataset
        dataset = Dataset.from_dict({
            'input_ids': tokenized['input_ids'],
            'attention_mask': tokenized['attention_mask']
        })
        
        logger.info(f"‚úÖ Dataset ready: {len(dataset)} examples")
        return dataset
    
    def train_model(self, dataset):
        """Fast production training with optimal parameters"""
        logger.info("\n" + "="*70)
        logger.info("üöÄ STARTING PRODUCTION TRAINING")
        logger.info("="*70)
        
        # Load model
        logger.info(f"üì¶ Loading {self.model_name} model...")
        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        # Check device
        use_cuda = torch.cuda.is_available()
        device = "GPU (CUDA)" if use_cuda else "CPU"
        logger.info(f"‚úÖ Model loaded on {device}")
        
        # Production-optimized training arguments
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            overwrite_output_dir=True,
            
            # FAST LEARNING - Your suggestion
            num_train_epochs=3,  # Fast learning as you suggested
            
            # Batch configuration
            per_device_train_batch_size=1 if not use_cuda else 2,  # CPU-optimized
            gradient_accumulation_steps=16 if not use_cuda else 8,  # Effective batch = 16
            
            # Learning rate - Your suggestion
            learning_rate=5e-5,  # Optimal convergence rate
            warmup_ratio=0.1,  # 10% warmup
            weight_decay=0.01,
            
            # Optimization for speed
            fp16=use_cuda,
            gradient_checkpointing=True,  # Save memory
            
            # Logging
            logging_steps=10,
            logging_first_step=True,
            
            # Save best model
            save_strategy="steps",
            save_steps=50,
            save_total_limit=2,
            
            # Evaluation
            eval_strategy="steps",
            eval_steps=50,
            load_best_model_at_end=True,
            metric_for_best_model="loss",
            
            # Early stopping for efficiency
            greater_is_better=False,
            
            # Performance
            dataloader_num_workers=0,  # CPU-friendly
            dataloader_pin_memory=use_cuda,
            
            # Other
            remove_unused_columns=False,
            report_to="none",
            seed=42
        )
        
        # Split for validation
        split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # Print configuration
        logger.info("\nüìä Training Configuration:")
        logger.info(f"   Model: {self.model_name} (355M parameters)")
        logger.info(f"   Epochs: {training_args.num_train_epochs}")
        logger.info(f"   Batch Size: {training_args.per_device_train_batch_size}")
        logger.info(f"   Gradient Accumulation: {training_args.gradient_accumulation_steps}")
        logger.info(f"   Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
        logger.info(f"   Learning Rate: {training_args.learning_rate}")
        logger.info(f"   Training Examples: {len(split_dataset['train'])}")
        logger.info(f"   Validation Examples: {len(split_dataset['test'])}")
        logger.info(f"   Device: {device}")
        
        # Calculate steps
        total_steps = len(split_dataset['train']) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs
        logger.info(f"   Total Training Steps: ~{total_steps}")
        
        # Initialize trainer with early stopping
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=split_dataset['train'],
            eval_dataset=split_dataset['test'],
            data_collator=data_collator,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        # Train
        logger.info("\n‚è≥ Training started...")
        logger.info("   This will take approximately 10-20 minutes on CPU")
        logger.info("   Progress will be shown below:\n")
        
        start_time = time.time()
        
        try:
            trainer.train()
            
            elapsed = time.time() - start_time
            logger.info(f"\n‚úÖ Training completed in {elapsed/60:.1f} minutes!")
            
        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è Training interrupted by user")
            return None
        except Exception as e:
            logger.error(f"\n‚ùå Training failed: {e}")
            return None
        
        return trainer
    
    def save_model(self):
        """Save the production model"""
        final_path = self.output_dir / "final_model"
        final_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"\nüíæ Saving production model to {final_path}...")
        self.model.save_pretrained(final_path)
        self.tokenizer.save_pretrained(final_path)
        
        # Save info
        info = {
            "model": "gpt2-medium (355M parameters)",
            "training_examples": 498,
            "epochs": 3,
            "learning_rate": "5e-5",
            "optimization": "Fast learning with early stopping",
            "capabilities": [
                "Accurate skills for all job roles",
                "Interview questions and answers",
                "Career guidance and certifications",
                "Structured, professional responses"
            ],
            "date_trained": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(final_path / "model_info.json", 'w') as f:
            json.dump(info, f, indent=2)
        
        logger.info("‚úÖ Model saved successfully!")
        return final_path
    
    def quick_test(self, test_prompts):
        """Quick validation test"""
        logger.info("\n" + "="*70)
        logger.info("üß™ QUICK VALIDATION TEST")
        logger.info("="*70)
        
        self.model.eval()
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)
        
        for i, prompt in enumerate(test_prompts, 1):
            logger.info(f"\n{'‚îÄ'*70}")
            logger.info(f"Test {i}: {prompt}")
            logger.info('‚îÄ'*70)
            
            input_text = f"Question: {prompt}\n\nAnswer:"
            inputs = self.tokenizer(input_text, return_tensors="pt").to(device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=300,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    do_sample=True,
                    repetition_penalty=1.15,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if "Answer:" in response:
                answer = response.split("Answer:")[1].strip()
            else:
                answer = response
            
            logger.info(f"\nüí° Response:\n{answer[:400]}{'...' if len(answer) > 400 else ''}")
            
            # Quick check
            has_skills = 'skill' in answer.lower()
            has_questions = 'question' in answer.lower() or 'interview' in answer.lower()
            
            logger.info(f"\n   {'‚úÖ' if has_skills else '‚ö†Ô∏è'} Contains skills")
            logger.info(f"   {'‚úÖ' if has_questions else '‚ö†Ô∏è'} Contains questions")


def main():
    """Main training pipeline"""
    print("\n" + "="*70)
    print("üéØ PRODUCTION CAREER ADVISOR LLM TRAINING")
    print("   Fast Learning with Accurate Results")
    print("="*70 + "\n")
    
    # Initialize
    trainer = ProductionCareerAdvisorTrainer()
    
    # Load data
    examples = trainer.load_training_data()
    if len(examples) == 0:
        print("‚ùå No training data found!")
        return
    
    # Prepare
    dataset = trainer.prepare_dataset(examples)
    
    # Train
    result = trainer.train_model(dataset)
    if result is None:
        print("\n‚ùå Training failed or was interrupted")
        return
    
    # Save
    model_path = trainer.save_model()
    
    # Quick test
    test_prompts = [
        "I love DevOps",
        "Tell me about Software Development",
        "I love networking",
    ]
    trainer.quick_test(test_prompts)
    
    # Success
    print("\n" + "="*70)
    print("üéâ TRAINING COMPLETE!")
    print("="*70)
    print(f"\nüìÅ Model saved to: {model_path}")
    print("\n‚úÖ Your AI Career Advisor is now trained with:")
    print("   ‚Ä¢ GPT-2 Medium (355M parameters)")
    print("   ‚Ä¢ 3 epochs for fast learning")
    print("   ‚Ä¢ 498 high-quality career examples")
    print("   ‚Ä¢ Optimized for accurate skills & interview questions")
    print("\nüöÄ Next steps:")
    print("   1. Run: python test_accurate_model.py")
    print("   2. Start backend: python -m uvicorn backend_api:app --port 8000")
    print("   3. Test with your queries!")


if __name__ == "__main__":
    main()
